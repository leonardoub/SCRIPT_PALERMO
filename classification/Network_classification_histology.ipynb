{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "ff6d45f6-d361-4abd-8d61-982092eadc92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "dce62b2b-5a14-4623-c253-42bc38362749",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "01baa543-00b7-462d-eeba-0862917cdfad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRJJ4WxMgyIt"
      },
      "source": [
        "##Z score dei dati dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNbcqlgbgyI5",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_pca.mean(axis=0)\n",
        "std = train_data_stand_pca.std(axis=0)\n",
        "train_data_stand_pca = train_data_stand_pca - mean\n",
        "train_data_stand_pca /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZA9GJO6gyJO",
        "colab": {}
      },
      "source": [
        "test_data_stand_pca = test_data_stand_pca - mean\n",
        "test_data_stand_pca /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(7,)))\n",
        "  model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.005, momentum=0.9)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "b2d5e797-a0a2-4415-9694-ebed2063409a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data, train_labels_dec)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "b308cd1c-8613-475f-cad4-9f88411a5965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "0c908bc0-aaf1-4be7-ccbc-602fe3892734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVSoMnogHVqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "00b2e27c-0f7e-4c04-d7d3-2eadbf93ad67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['accuracy']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_accuracy']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/300\n",
            "87/87 [==============================] - 0s 941us/step - loss: 1.1931 - accuracy: 0.3218 - val_loss: 1.0887 - val_accuracy: 0.3409\n",
            "Epoch 2/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 1.0674 - accuracy: 0.3793 - val_loss: 1.0456 - val_accuracy: 0.3409\n",
            "Epoch 3/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9963 - accuracy: 0.4598 - val_loss: 1.0363 - val_accuracy: 0.3409\n",
            "Epoch 4/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.9689 - accuracy: 0.4713 - val_loss: 1.0357 - val_accuracy: 0.4318\n",
            "Epoch 5/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9479 - accuracy: 0.4943 - val_loss: 1.0330 - val_accuracy: 0.4091\n",
            "Epoch 6/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.9388 - accuracy: 0.5402 - val_loss: 1.0308 - val_accuracy: 0.3636\n",
            "Epoch 7/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.9271 - accuracy: 0.5517 - val_loss: 1.0315 - val_accuracy: 0.3636\n",
            "Epoch 8/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.9188 - accuracy: 0.5517 - val_loss: 1.0310 - val_accuracy: 0.4091\n",
            "Epoch 9/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9124 - accuracy: 0.5747 - val_loss: 1.0292 - val_accuracy: 0.3864\n",
            "Epoch 10/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9047 - accuracy: 0.5862 - val_loss: 1.0302 - val_accuracy: 0.4545\n",
            "Epoch 11/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8988 - accuracy: 0.5862 - val_loss: 1.0309 - val_accuracy: 0.4318\n",
            "Epoch 12/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8891 - accuracy: 0.6092 - val_loss: 1.0330 - val_accuracy: 0.5000\n",
            "Epoch 13/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8833 - accuracy: 0.6092 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
            "Epoch 14/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8729 - accuracy: 0.6092 - val_loss: 1.0362 - val_accuracy: 0.4773\n",
            "Epoch 15/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8676 - accuracy: 0.6092 - val_loss: 1.0379 - val_accuracy: 0.4773\n",
            "Epoch 16/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8575 - accuracy: 0.6207 - val_loss: 1.0426 - val_accuracy: 0.4773\n",
            "Epoch 17/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8517 - accuracy: 0.6207 - val_loss: 1.0496 - val_accuracy: 0.4545\n",
            "Epoch 18/300\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8454 - accuracy: 0.6092 - val_loss: 1.0533 - val_accuracy: 0.4773\n",
            "Epoch 19/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8330 - accuracy: 0.6437 - val_loss: 1.0587 - val_accuracy: 0.4545\n",
            "Epoch 20/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.8310 - accuracy: 0.6437 - val_loss: 1.0674 - val_accuracy: 0.4545\n",
            "Epoch 21/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8194 - accuracy: 0.6437 - val_loss: 1.0715 - val_accuracy: 0.4545\n",
            "Epoch 22/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8124 - accuracy: 0.6437 - val_loss: 1.0793 - val_accuracy: 0.4773\n",
            "Epoch 23/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8044 - accuracy: 0.6437 - val_loss: 1.0858 - val_accuracy: 0.4773\n",
            "Epoch 24/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.7959 - accuracy: 0.6437 - val_loss: 1.0943 - val_accuracy: 0.4773\n",
            "Epoch 25/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.7909 - accuracy: 0.6437 - val_loss: 1.1005 - val_accuracy: 0.4773\n",
            "Epoch 26/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7856 - accuracy: 0.6552 - val_loss: 1.1124 - val_accuracy: 0.4773\n",
            "Epoch 27/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.7818 - accuracy: 0.6437 - val_loss: 1.1238 - val_accuracy: 0.4773\n",
            "Epoch 28/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7692 - accuracy: 0.6552 - val_loss: 1.1312 - val_accuracy: 0.4773\n",
            "Epoch 29/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.7636 - accuracy: 0.6782 - val_loss: 1.1459 - val_accuracy: 0.4545\n",
            "Epoch 30/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.7559 - accuracy: 0.6667 - val_loss: 1.1536 - val_accuracy: 0.4773\n",
            "Epoch 31/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7538 - accuracy: 0.6552 - val_loss: 1.1625 - val_accuracy: 0.4773\n",
            "Epoch 32/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.7456 - accuracy: 0.6782 - val_loss: 1.1722 - val_accuracy: 0.4545\n",
            "Epoch 33/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7380 - accuracy: 0.6782 - val_loss: 1.1867 - val_accuracy: 0.4545\n",
            "Epoch 34/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7315 - accuracy: 0.6897 - val_loss: 1.1941 - val_accuracy: 0.4545\n",
            "Epoch 35/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.7279 - accuracy: 0.6782 - val_loss: 1.2025 - val_accuracy: 0.4545\n",
            "Epoch 36/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7222 - accuracy: 0.6782 - val_loss: 1.2138 - val_accuracy: 0.4545\n",
            "Epoch 37/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7172 - accuracy: 0.7011 - val_loss: 1.2281 - val_accuracy: 0.4545\n",
            "Epoch 38/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.7171 - accuracy: 0.7011 - val_loss: 1.2343 - val_accuracy: 0.4318\n",
            "Epoch 39/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.7100 - accuracy: 0.7011 - val_loss: 1.2519 - val_accuracy: 0.4773\n",
            "Epoch 40/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.7016 - accuracy: 0.7011 - val_loss: 1.2602 - val_accuracy: 0.4545\n",
            "Epoch 41/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6930 - accuracy: 0.7011 - val_loss: 1.2640 - val_accuracy: 0.4545\n",
            "Epoch 42/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6896 - accuracy: 0.7126 - val_loss: 1.2799 - val_accuracy: 0.4773\n",
            "Epoch 43/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6867 - accuracy: 0.7011 - val_loss: 1.2963 - val_accuracy: 0.4773\n",
            "Epoch 44/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6786 - accuracy: 0.7126 - val_loss: 1.3022 - val_accuracy: 0.4318\n",
            "Epoch 45/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.6718 - accuracy: 0.7241 - val_loss: 1.3095 - val_accuracy: 0.4318\n",
            "Epoch 46/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6709 - accuracy: 0.7241 - val_loss: 1.3208 - val_accuracy: 0.4091\n",
            "Epoch 47/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.6616 - accuracy: 0.7126 - val_loss: 1.3340 - val_accuracy: 0.4545\n",
            "Epoch 48/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.6652 - accuracy: 0.7126 - val_loss: 1.3324 - val_accuracy: 0.4318\n",
            "Epoch 49/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6531 - accuracy: 0.7241 - val_loss: 1.3513 - val_accuracy: 0.4318\n",
            "Epoch 50/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6515 - accuracy: 0.7241 - val_loss: 1.3635 - val_accuracy: 0.4318\n",
            "Epoch 51/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.6456 - accuracy: 0.7126 - val_loss: 1.3711 - val_accuracy: 0.4091\n",
            "Epoch 52/300\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.6361 - accuracy: 0.7241 - val_loss: 1.3830 - val_accuracy: 0.4318\n",
            "Epoch 53/300\n",
            "87/87 [==============================] - 0s 335us/step - loss: 0.6352 - accuracy: 0.7241 - val_loss: 1.4017 - val_accuracy: 0.4318\n",
            "Epoch 54/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.6279 - accuracy: 0.7241 - val_loss: 1.4000 - val_accuracy: 0.4545\n",
            "Epoch 55/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6244 - accuracy: 0.7241 - val_loss: 1.4176 - val_accuracy: 0.4091\n",
            "Epoch 56/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6190 - accuracy: 0.7356 - val_loss: 1.4282 - val_accuracy: 0.4545\n",
            "Epoch 57/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6249 - accuracy: 0.7241 - val_loss: 1.4403 - val_accuracy: 0.4318\n",
            "Epoch 58/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.6133 - accuracy: 0.7241 - val_loss: 1.4469 - val_accuracy: 0.4091\n",
            "Epoch 59/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.6184 - accuracy: 0.7471 - val_loss: 1.4715 - val_accuracy: 0.4318\n",
            "Epoch 60/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.6018 - accuracy: 0.7241 - val_loss: 1.4671 - val_accuracy: 0.4318\n",
            "Epoch 61/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.6107 - accuracy: 0.7126 - val_loss: 1.4947 - val_accuracy: 0.4091\n",
            "Epoch 62/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.5919 - accuracy: 0.7471 - val_loss: 1.4879 - val_accuracy: 0.4318\n",
            "Epoch 63/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.6065 - accuracy: 0.7126 - val_loss: 1.4975 - val_accuracy: 0.4318\n",
            "Epoch 64/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.6029 - accuracy: 0.7241 - val_loss: 1.5005 - val_accuracy: 0.4091\n",
            "Epoch 65/300\n",
            "87/87 [==============================] - 0s 308us/step - loss: 0.5833 - accuracy: 0.7356 - val_loss: 1.5104 - val_accuracy: 0.4545\n",
            "Epoch 66/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.5738 - accuracy: 0.7356 - val_loss: 1.5183 - val_accuracy: 0.4318\n",
            "Epoch 67/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.5651 - accuracy: 0.7356 - val_loss: 1.5378 - val_accuracy: 0.4545\n",
            "Epoch 68/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5640 - accuracy: 0.7471 - val_loss: 1.5441 - val_accuracy: 0.4545\n",
            "Epoch 69/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5529 - accuracy: 0.7701 - val_loss: 1.5689 - val_accuracy: 0.4318\n",
            "Epoch 70/300\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.5508 - accuracy: 0.7586 - val_loss: 1.5548 - val_accuracy: 0.4545\n",
            "Epoch 71/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.5454 - accuracy: 0.7356 - val_loss: 1.5821 - val_accuracy: 0.4773\n",
            "Epoch 72/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5480 - accuracy: 0.7586 - val_loss: 1.5821 - val_accuracy: 0.4773\n",
            "Epoch 73/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5422 - accuracy: 0.7701 - val_loss: 1.6016 - val_accuracy: 0.4545\n",
            "Epoch 74/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.5320 - accuracy: 0.7586 - val_loss: 1.5933 - val_accuracy: 0.4773\n",
            "Epoch 75/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.5193 - accuracy: 0.7701 - val_loss: 1.6115 - val_accuracy: 0.4773\n",
            "Epoch 76/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.5137 - accuracy: 0.7701 - val_loss: 1.6188 - val_accuracy: 0.4773\n",
            "Epoch 77/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5051 - accuracy: 0.8046 - val_loss: 1.6357 - val_accuracy: 0.5000\n",
            "Epoch 78/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4994 - accuracy: 0.7931 - val_loss: 1.6642 - val_accuracy: 0.4773\n",
            "Epoch 79/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.5044 - accuracy: 0.7816 - val_loss: 1.6723 - val_accuracy: 0.5000\n",
            "Epoch 80/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4819 - accuracy: 0.7816 - val_loss: 1.6739 - val_accuracy: 0.5000\n",
            "Epoch 81/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4886 - accuracy: 0.8046 - val_loss: 1.6700 - val_accuracy: 0.5227\n",
            "Epoch 82/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.4751 - accuracy: 0.8161 - val_loss: 1.6775 - val_accuracy: 0.5000\n",
            "Epoch 83/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4763 - accuracy: 0.8276 - val_loss: 1.7027 - val_accuracy: 0.5455\n",
            "Epoch 84/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.4526 - accuracy: 0.8276 - val_loss: 1.7250 - val_accuracy: 0.5000\n",
            "Epoch 85/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.4712 - accuracy: 0.8046 - val_loss: 1.7559 - val_accuracy: 0.5455\n",
            "Epoch 86/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4616 - accuracy: 0.8161 - val_loss: 1.7661 - val_accuracy: 0.4545\n",
            "Epoch 87/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.4493 - accuracy: 0.8276 - val_loss: 1.7515 - val_accuracy: 0.5000\n",
            "Epoch 88/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4468 - accuracy: 0.8391 - val_loss: 1.7778 - val_accuracy: 0.5000\n",
            "Epoch 89/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.4330 - accuracy: 0.8506 - val_loss: 1.7835 - val_accuracy: 0.5227\n",
            "Epoch 90/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4291 - accuracy: 0.8391 - val_loss: 1.7865 - val_accuracy: 0.5227\n",
            "Epoch 91/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4215 - accuracy: 0.8621 - val_loss: 1.8119 - val_accuracy: 0.5000\n",
            "Epoch 92/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4223 - accuracy: 0.8621 - val_loss: 1.8451 - val_accuracy: 0.4773\n",
            "Epoch 93/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4039 - accuracy: 0.8621 - val_loss: 1.8638 - val_accuracy: 0.5227\n",
            "Epoch 94/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.4071 - accuracy: 0.8621 - val_loss: 1.8728 - val_accuracy: 0.5227\n",
            "Epoch 95/300\n",
            "87/87 [==============================] - 0s 304us/step - loss: 0.3942 - accuracy: 0.8966 - val_loss: 1.8934 - val_accuracy: 0.5000\n",
            "Epoch 96/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.4012 - accuracy: 0.8851 - val_loss: 1.9223 - val_accuracy: 0.5000\n",
            "Epoch 97/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3886 - accuracy: 0.8851 - val_loss: 1.9147 - val_accuracy: 0.5000\n",
            "Epoch 98/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3836 - accuracy: 0.8736 - val_loss: 1.9970 - val_accuracy: 0.4773\n",
            "Epoch 99/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3729 - accuracy: 0.8966 - val_loss: 2.0033 - val_accuracy: 0.5000\n",
            "Epoch 100/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.3753 - accuracy: 0.8736 - val_loss: 2.0055 - val_accuracy: 0.5000\n",
            "Epoch 101/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.3725 - accuracy: 0.8851 - val_loss: 2.0154 - val_accuracy: 0.5000\n",
            "Epoch 102/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.3648 - accuracy: 0.8851 - val_loss: 2.0473 - val_accuracy: 0.5000\n",
            "Epoch 103/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3558 - accuracy: 0.9080 - val_loss: 2.1016 - val_accuracy: 0.5000\n",
            "Epoch 104/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3462 - accuracy: 0.9080 - val_loss: 2.1011 - val_accuracy: 0.5000\n",
            "Epoch 105/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3473 - accuracy: 0.9310 - val_loss: 2.1354 - val_accuracy: 0.5227\n",
            "Epoch 106/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3346 - accuracy: 0.9080 - val_loss: 2.1750 - val_accuracy: 0.5000\n",
            "Epoch 107/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3365 - accuracy: 0.9195 - val_loss: 2.2014 - val_accuracy: 0.5000\n",
            "Epoch 108/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3297 - accuracy: 0.9195 - val_loss: 2.2066 - val_accuracy: 0.5000\n",
            "Epoch 109/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.3342 - accuracy: 0.9310 - val_loss: 2.2089 - val_accuracy: 0.5000\n",
            "Epoch 110/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3303 - accuracy: 0.8851 - val_loss: 2.2660 - val_accuracy: 0.5227\n",
            "Epoch 111/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.3257 - accuracy: 0.9080 - val_loss: 2.2431 - val_accuracy: 0.5227\n",
            "Epoch 112/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3161 - accuracy: 0.9080 - val_loss: 2.2828 - val_accuracy: 0.5000\n",
            "Epoch 113/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3219 - accuracy: 0.9195 - val_loss: 2.3599 - val_accuracy: 0.5000\n",
            "Epoch 114/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3144 - accuracy: 0.9195 - val_loss: 2.3153 - val_accuracy: 0.5227\n",
            "Epoch 115/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.2998 - accuracy: 0.9425 - val_loss: 2.3549 - val_accuracy: 0.4773\n",
            "Epoch 116/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.2944 - accuracy: 0.9310 - val_loss: 2.3762 - val_accuracy: 0.5000\n",
            "Epoch 117/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.3033 - accuracy: 0.9080 - val_loss: 2.3945 - val_accuracy: 0.5227\n",
            "Epoch 118/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.2850 - accuracy: 0.9310 - val_loss: 2.3775 - val_accuracy: 0.5227\n",
            "Epoch 119/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2791 - accuracy: 0.9195 - val_loss: 2.4605 - val_accuracy: 0.5227\n",
            "Epoch 120/300\n",
            "87/87 [==============================] - 0s 322us/step - loss: 0.3014 - accuracy: 0.9195 - val_loss: 2.5307 - val_accuracy: 0.5000\n",
            "Epoch 121/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2719 - accuracy: 0.9195 - val_loss: 2.4780 - val_accuracy: 0.5227\n",
            "Epoch 122/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2860 - accuracy: 0.9080 - val_loss: 2.5106 - val_accuracy: 0.5227\n",
            "Epoch 123/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2780 - accuracy: 0.9425 - val_loss: 2.5125 - val_accuracy: 0.5227\n",
            "Epoch 124/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2644 - accuracy: 0.9310 - val_loss: 2.5753 - val_accuracy: 0.5000\n",
            "Epoch 125/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2581 - accuracy: 0.9310 - val_loss: 2.5929 - val_accuracy: 0.5227\n",
            "Epoch 126/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2531 - accuracy: 0.9425 - val_loss: 2.5855 - val_accuracy: 0.5227\n",
            "Epoch 127/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.2517 - accuracy: 0.9425 - val_loss: 2.5671 - val_accuracy: 0.4773\n",
            "Epoch 128/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.2534 - accuracy: 0.9425 - val_loss: 2.6557 - val_accuracy: 0.4773\n",
            "Epoch 129/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.2491 - accuracy: 0.9195 - val_loss: 2.6947 - val_accuracy: 0.5227\n",
            "Epoch 130/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2663 - accuracy: 0.9195 - val_loss: 2.6698 - val_accuracy: 0.5227\n",
            "Epoch 131/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2349 - accuracy: 0.9425 - val_loss: 2.7285 - val_accuracy: 0.5000\n",
            "Epoch 132/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.2371 - accuracy: 0.9425 - val_loss: 2.7162 - val_accuracy: 0.5227\n",
            "Epoch 133/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2315 - accuracy: 0.9310 - val_loss: 2.7659 - val_accuracy: 0.5000\n",
            "Epoch 134/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2317 - accuracy: 0.9310 - val_loss: 2.8001 - val_accuracy: 0.5000\n",
            "Epoch 135/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2216 - accuracy: 0.9540 - val_loss: 2.7829 - val_accuracy: 0.5000\n",
            "Epoch 136/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2235 - accuracy: 0.9540 - val_loss: 2.8459 - val_accuracy: 0.5000\n",
            "Epoch 137/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2180 - accuracy: 0.9425 - val_loss: 2.8445 - val_accuracy: 0.5227\n",
            "Epoch 138/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2160 - accuracy: 0.9540 - val_loss: 2.8627 - val_accuracy: 0.4773\n",
            "Epoch 139/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.2124 - accuracy: 0.9540 - val_loss: 2.9234 - val_accuracy: 0.5000\n",
            "Epoch 140/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.2247 - accuracy: 0.9310 - val_loss: 2.9527 - val_accuracy: 0.5000\n",
            "Epoch 141/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2247 - accuracy: 0.9540 - val_loss: 2.9050 - val_accuracy: 0.4773\n",
            "Epoch 142/300\n",
            "87/87 [==============================] - 0s 286us/step - loss: 0.2055 - accuracy: 0.9540 - val_loss: 2.9721 - val_accuracy: 0.5000\n",
            "Epoch 143/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2006 - accuracy: 0.9540 - val_loss: 2.9985 - val_accuracy: 0.5000\n",
            "Epoch 144/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2045 - accuracy: 0.9540 - val_loss: 3.0627 - val_accuracy: 0.5000\n",
            "Epoch 145/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1997 - accuracy: 0.9655 - val_loss: 3.0735 - val_accuracy: 0.5000\n",
            "Epoch 146/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.1896 - accuracy: 0.9770 - val_loss: 3.1166 - val_accuracy: 0.5000\n",
            "Epoch 147/300\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.1900 - accuracy: 0.9770 - val_loss: 3.0886 - val_accuracy: 0.5000\n",
            "Epoch 148/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.1927 - accuracy: 0.9655 - val_loss: 3.1109 - val_accuracy: 0.5000\n",
            "Epoch 149/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1893 - accuracy: 0.9540 - val_loss: 3.1666 - val_accuracy: 0.4773\n",
            "Epoch 150/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1875 - accuracy: 0.9540 - val_loss: 3.1823 - val_accuracy: 0.5000\n",
            "Epoch 151/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.1918 - accuracy: 0.9655 - val_loss: 3.2233 - val_accuracy: 0.5000\n",
            "Epoch 152/300\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.1866 - accuracy: 0.9655 - val_loss: 3.2061 - val_accuracy: 0.5000\n",
            "Epoch 153/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1786 - accuracy: 0.9770 - val_loss: 3.2433 - val_accuracy: 0.5000\n",
            "Epoch 154/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.1809 - accuracy: 0.9770 - val_loss: 3.2641 - val_accuracy: 0.5000\n",
            "Epoch 155/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1714 - accuracy: 0.9770 - val_loss: 3.2761 - val_accuracy: 0.5000\n",
            "Epoch 156/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1723 - accuracy: 0.9770 - val_loss: 3.3298 - val_accuracy: 0.5000\n",
            "Epoch 157/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1614 - accuracy: 0.9770 - val_loss: 3.2988 - val_accuracy: 0.5000\n",
            "Epoch 158/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.1634 - accuracy: 0.9655 - val_loss: 3.3210 - val_accuracy: 0.5000\n",
            "Epoch 159/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1615 - accuracy: 0.9770 - val_loss: 3.3698 - val_accuracy: 0.5000\n",
            "Epoch 160/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1652 - accuracy: 0.9655 - val_loss: 3.3762 - val_accuracy: 0.4773\n",
            "Epoch 161/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1553 - accuracy: 0.9770 - val_loss: 3.4538 - val_accuracy: 0.5000\n",
            "Epoch 162/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1625 - accuracy: 0.9540 - val_loss: 3.4443 - val_accuracy: 0.4773\n",
            "Epoch 163/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.1627 - accuracy: 0.9655 - val_loss: 3.4623 - val_accuracy: 0.5000\n",
            "Epoch 164/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1472 - accuracy: 0.9770 - val_loss: 3.4626 - val_accuracy: 0.4773\n",
            "Epoch 165/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1583 - accuracy: 0.9655 - val_loss: 3.4973 - val_accuracy: 0.4773\n",
            "Epoch 166/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1494 - accuracy: 0.9770 - val_loss: 3.5359 - val_accuracy: 0.4773\n",
            "Epoch 167/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1544 - accuracy: 0.9655 - val_loss: 3.5303 - val_accuracy: 0.5000\n",
            "Epoch 168/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1477 - accuracy: 0.9770 - val_loss: 3.5622 - val_accuracy: 0.4545\n",
            "Epoch 169/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1453 - accuracy: 0.9770 - val_loss: 3.6132 - val_accuracy: 0.4773\n",
            "Epoch 170/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.1451 - accuracy: 0.9770 - val_loss: 3.5824 - val_accuracy: 0.4545\n",
            "Epoch 171/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.1429 - accuracy: 0.9655 - val_loss: 3.6400 - val_accuracy: 0.4773\n",
            "Epoch 172/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1351 - accuracy: 0.9770 - val_loss: 3.6410 - val_accuracy: 0.4545\n",
            "Epoch 173/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1523 - accuracy: 0.9655 - val_loss: 3.6629 - val_accuracy: 0.4773\n",
            "Epoch 174/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.1510 - accuracy: 0.9655 - val_loss: 3.6813 - val_accuracy: 0.4545\n",
            "Epoch 175/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1481 - accuracy: 0.9655 - val_loss: 3.7335 - val_accuracy: 0.4545\n",
            "Epoch 176/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1300 - accuracy: 0.9655 - val_loss: 3.7304 - val_accuracy: 0.4773\n",
            "Epoch 177/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1363 - accuracy: 0.9655 - val_loss: 3.7959 - val_accuracy: 0.4773\n",
            "Epoch 178/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.1371 - accuracy: 0.9770 - val_loss: 3.8522 - val_accuracy: 0.4773\n",
            "Epoch 179/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.1228 - accuracy: 0.9770 - val_loss: 3.8073 - val_accuracy: 0.4545\n",
            "Epoch 180/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1335 - accuracy: 0.9655 - val_loss: 3.8369 - val_accuracy: 0.4545\n",
            "Epoch 181/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.1343 - accuracy: 0.9655 - val_loss: 3.8573 - val_accuracy: 0.4545\n",
            "Epoch 182/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1287 - accuracy: 0.9655 - val_loss: 3.8660 - val_accuracy: 0.4545\n",
            "Epoch 183/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1125 - accuracy: 0.9770 - val_loss: 3.8795 - val_accuracy: 0.4773\n",
            "Epoch 184/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1136 - accuracy: 0.9770 - val_loss: 3.9053 - val_accuracy: 0.4545\n",
            "Epoch 185/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1097 - accuracy: 0.9770 - val_loss: 3.9416 - val_accuracy: 0.4545\n",
            "Epoch 186/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1080 - accuracy: 0.9770 - val_loss: 3.9553 - val_accuracy: 0.4545\n",
            "Epoch 187/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1102 - accuracy: 0.9770 - val_loss: 3.9429 - val_accuracy: 0.4545\n",
            "Epoch 188/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.1083 - accuracy: 0.9770 - val_loss: 3.9984 - val_accuracy: 0.4545\n",
            "Epoch 189/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.1061 - accuracy: 0.9770 - val_loss: 4.0038 - val_accuracy: 0.4545\n",
            "Epoch 190/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1044 - accuracy: 0.9770 - val_loss: 4.0036 - val_accuracy: 0.4545\n",
            "Epoch 191/300\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.1019 - accuracy: 0.9770 - val_loss: 4.0654 - val_accuracy: 0.4545\n",
            "Epoch 192/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1062 - accuracy: 0.9770 - val_loss: 4.0905 - val_accuracy: 0.4545\n",
            "Epoch 193/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0992 - accuracy: 0.9770 - val_loss: 4.0718 - val_accuracy: 0.4545\n",
            "Epoch 194/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0985 - accuracy: 0.9770 - val_loss: 4.1097 - val_accuracy: 0.4545\n",
            "Epoch 195/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0956 - accuracy: 0.9770 - val_loss: 4.1076 - val_accuracy: 0.4545\n",
            "Epoch 196/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0956 - accuracy: 0.9770 - val_loss: 4.1366 - val_accuracy: 0.4545\n",
            "Epoch 197/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.0950 - accuracy: 0.9770 - val_loss: 4.1769 - val_accuracy: 0.4773\n",
            "Epoch 198/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0960 - accuracy: 0.9770 - val_loss: 4.1993 - val_accuracy: 0.4545\n",
            "Epoch 199/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0899 - accuracy: 0.9770 - val_loss: 4.1894 - val_accuracy: 0.4545\n",
            "Epoch 200/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0919 - accuracy: 0.9770 - val_loss: 4.2331 - val_accuracy: 0.4545\n",
            "Epoch 201/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0887 - accuracy: 0.9770 - val_loss: 4.2551 - val_accuracy: 0.4545\n",
            "Epoch 202/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0882 - accuracy: 0.9770 - val_loss: 4.2676 - val_accuracy: 0.4545\n",
            "Epoch 203/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0898 - accuracy: 0.9770 - val_loss: 4.2898 - val_accuracy: 0.4545\n",
            "Epoch 204/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0999 - accuracy: 0.9655 - val_loss: 4.3442 - val_accuracy: 0.4545\n",
            "Epoch 205/300\n",
            "87/87 [==============================] - 0s 322us/step - loss: 0.0924 - accuracy: 0.9885 - val_loss: 4.4018 - val_accuracy: 0.4545\n",
            "Epoch 206/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.0865 - accuracy: 0.9770 - val_loss: 4.3920 - val_accuracy: 0.4545\n",
            "Epoch 207/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0860 - accuracy: 0.9770 - val_loss: 4.3629 - val_accuracy: 0.4545\n",
            "Epoch 208/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.0808 - accuracy: 0.9770 - val_loss: 4.4108 - val_accuracy: 0.4545\n",
            "Epoch 209/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0813 - accuracy: 0.9770 - val_loss: 4.4528 - val_accuracy: 0.4545\n",
            "Epoch 210/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0825 - accuracy: 0.9770 - val_loss: 4.4401 - val_accuracy: 0.4545\n",
            "Epoch 211/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.0794 - accuracy: 0.9885 - val_loss: 4.4169 - val_accuracy: 0.4545\n",
            "Epoch 212/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.0769 - accuracy: 0.9770 - val_loss: 4.4720 - val_accuracy: 0.4545\n",
            "Epoch 213/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.0774 - accuracy: 0.9770 - val_loss: 4.4984 - val_accuracy: 0.4545\n",
            "Epoch 214/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.0750 - accuracy: 0.9770 - val_loss: 4.4952 - val_accuracy: 0.4545\n",
            "Epoch 215/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0736 - accuracy: 0.9770 - val_loss: 4.5299 - val_accuracy: 0.4773\n",
            "Epoch 216/300\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 4.5554 - val_accuracy: 0.4773\n",
            "Epoch 217/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0730 - accuracy: 0.9885 - val_loss: 4.5648 - val_accuracy: 0.4545\n",
            "Epoch 218/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0705 - accuracy: 0.9770 - val_loss: 4.5640 - val_accuracy: 0.4545\n",
            "Epoch 219/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.0699 - accuracy: 0.9885 - val_loss: 4.5923 - val_accuracy: 0.4545\n",
            "Epoch 220/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0698 - accuracy: 0.9885 - val_loss: 4.6150 - val_accuracy: 0.4773\n",
            "Epoch 221/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0668 - accuracy: 0.9885 - val_loss: 4.6324 - val_accuracy: 0.4545\n",
            "Epoch 222/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 4.6650 - val_accuracy: 0.4545\n",
            "Epoch 223/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 4.6655 - val_accuracy: 0.4545\n",
            "Epoch 224/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 4.6799 - val_accuracy: 0.4545\n",
            "Epoch 225/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0652 - accuracy: 0.9885 - val_loss: 4.6945 - val_accuracy: 0.4545\n",
            "Epoch 226/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0634 - accuracy: 0.9885 - val_loss: 4.7117 - val_accuracy: 0.4545\n",
            "Epoch 227/300\n",
            "87/87 [==============================] - 0s 300us/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 4.7219 - val_accuracy: 0.4545\n",
            "Epoch 228/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0615 - accuracy: 0.9885 - val_loss: 4.7472 - val_accuracy: 0.4545\n",
            "Epoch 229/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0616 - accuracy: 0.9885 - val_loss: 4.7740 - val_accuracy: 0.4545\n",
            "Epoch 230/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.0607 - accuracy: 0.9885 - val_loss: 4.8062 - val_accuracy: 0.4545\n",
            "Epoch 231/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0608 - accuracy: 0.9885 - val_loss: 4.7935 - val_accuracy: 0.4545\n",
            "Epoch 232/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0595 - accuracy: 0.9885 - val_loss: 4.7981 - val_accuracy: 0.4545\n",
            "Epoch 233/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.0588 - accuracy: 0.9885 - val_loss: 4.8197 - val_accuracy: 0.4545\n",
            "Epoch 234/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0593 - accuracy: 0.9885 - val_loss: 4.8508 - val_accuracy: 0.4545\n",
            "Epoch 235/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0585 - accuracy: 0.9770 - val_loss: 4.8719 - val_accuracy: 0.4545\n",
            "Epoch 236/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0578 - accuracy: 0.9885 - val_loss: 4.8843 - val_accuracy: 0.4545\n",
            "Epoch 237/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.0560 - accuracy: 0.9885 - val_loss: 4.8969 - val_accuracy: 0.4545\n",
            "Epoch 238/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0564 - accuracy: 0.9885 - val_loss: 4.9206 - val_accuracy: 0.4545\n",
            "Epoch 239/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.0553 - accuracy: 0.9885 - val_loss: 4.9383 - val_accuracy: 0.4545\n",
            "Epoch 240/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0572 - accuracy: 0.9885 - val_loss: 4.9508 - val_accuracy: 0.4545\n",
            "Epoch 241/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0566 - accuracy: 0.9885 - val_loss: 4.9572 - val_accuracy: 0.4545\n",
            "Epoch 242/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0535 - accuracy: 0.9885 - val_loss: 4.9801 - val_accuracy: 0.4545\n",
            "Epoch 243/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0526 - accuracy: 0.9885 - val_loss: 5.0024 - val_accuracy: 0.4545\n",
            "Epoch 244/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0525 - accuracy: 0.9885 - val_loss: 5.0121 - val_accuracy: 0.4545\n",
            "Epoch 245/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.0537 - accuracy: 0.9885 - val_loss: 5.0498 - val_accuracy: 0.4545\n",
            "Epoch 246/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.0519 - accuracy: 0.9885 - val_loss: 5.0424 - val_accuracy: 0.4545\n",
            "Epoch 247/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0510 - accuracy: 0.9885 - val_loss: 5.0403 - val_accuracy: 0.4545\n",
            "Epoch 248/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0501 - accuracy: 0.9885 - val_loss: 5.0545 - val_accuracy: 0.4545\n",
            "Epoch 249/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0505 - accuracy: 0.9885 - val_loss: 5.0746 - val_accuracy: 0.4545\n",
            "Epoch 250/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.0498 - accuracy: 0.9885 - val_loss: 5.1155 - val_accuracy: 0.4545\n",
            "Epoch 251/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.0505 - accuracy: 0.9885 - val_loss: 5.1211 - val_accuracy: 0.4545\n",
            "Epoch 252/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0478 - accuracy: 0.9885 - val_loss: 5.1181 - val_accuracy: 0.4545\n",
            "Epoch 253/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.0476 - accuracy: 0.9885 - val_loss: 5.1486 - val_accuracy: 0.4545\n",
            "Epoch 254/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0473 - accuracy: 0.9885 - val_loss: 5.1530 - val_accuracy: 0.4545\n",
            "Epoch 255/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0469 - accuracy: 0.9885 - val_loss: 5.1823 - val_accuracy: 0.4545\n",
            "Epoch 256/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0481 - accuracy: 0.9885 - val_loss: 5.1944 - val_accuracy: 0.4545\n",
            "Epoch 257/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0453 - accuracy: 0.9885 - val_loss: 5.1964 - val_accuracy: 0.4545\n",
            "Epoch 258/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0475 - accuracy: 0.9885 - val_loss: 5.2044 - val_accuracy: 0.4545\n",
            "Epoch 259/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.0452 - accuracy: 0.9885 - val_loss: 5.2151 - val_accuracy: 0.4545\n",
            "Epoch 260/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0450 - accuracy: 0.9885 - val_loss: 5.2504 - val_accuracy: 0.4545\n",
            "Epoch 261/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0439 - accuracy: 0.9885 - val_loss: 5.2651 - val_accuracy: 0.4545\n",
            "Epoch 262/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.0439 - accuracy: 0.9885 - val_loss: 5.2795 - val_accuracy: 0.4545\n",
            "Epoch 263/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0437 - accuracy: 0.9885 - val_loss: 5.2988 - val_accuracy: 0.4545\n",
            "Epoch 264/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0430 - accuracy: 0.9885 - val_loss: 5.3197 - val_accuracy: 0.4545\n",
            "Epoch 265/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.0422 - accuracy: 0.9885 - val_loss: 5.3222 - val_accuracy: 0.4545\n",
            "Epoch 266/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.0421 - accuracy: 0.9885 - val_loss: 5.3434 - val_accuracy: 0.4545\n",
            "Epoch 267/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0423 - accuracy: 0.9885 - val_loss: 5.3465 - val_accuracy: 0.4545\n",
            "Epoch 268/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.0426 - accuracy: 0.9885 - val_loss: 5.3621 - val_accuracy: 0.4545\n",
            "Epoch 269/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0427 - accuracy: 0.9885 - val_loss: 5.3773 - val_accuracy: 0.4545\n",
            "Epoch 270/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0402 - accuracy: 0.9885 - val_loss: 5.3954 - val_accuracy: 0.4545\n",
            "Epoch 271/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0411 - accuracy: 0.9885 - val_loss: 5.4221 - val_accuracy: 0.4545\n",
            "Epoch 272/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 5.4304 - val_accuracy: 0.4545\n",
            "Epoch 273/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0399 - accuracy: 0.9885 - val_loss: 5.4255 - val_accuracy: 0.4545\n",
            "Epoch 274/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0394 - accuracy: 0.9885 - val_loss: 5.4508 - val_accuracy: 0.4545\n",
            "Epoch 275/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0388 - accuracy: 0.9885 - val_loss: 5.4618 - val_accuracy: 0.4545\n",
            "Epoch 276/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 5.4679 - val_accuracy: 0.4545\n",
            "Epoch 277/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 5.4870 - val_accuracy: 0.4545\n",
            "Epoch 278/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 5.5006 - val_accuracy: 0.4545\n",
            "Epoch 279/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 5.5158 - val_accuracy: 0.4545\n",
            "Epoch 280/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 5.5202 - val_accuracy: 0.4545\n",
            "Epoch 281/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 5.5369 - val_accuracy: 0.4545\n",
            "Epoch 282/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 5.5468 - val_accuracy: 0.4545\n",
            "Epoch 283/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 5.5676 - val_accuracy: 0.4545\n",
            "Epoch 284/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 5.5766 - val_accuracy: 0.4545\n",
            "Epoch 285/300\n",
            "87/87 [==============================] - 0s 336us/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 5.5983 - val_accuracy: 0.4545\n",
            "Epoch 286/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 5.6035 - val_accuracy: 0.4545\n",
            "Epoch 287/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 5.6167 - val_accuracy: 0.4545\n",
            "Epoch 288/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 5.6254 - val_accuracy: 0.4545\n",
            "Epoch 289/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 5.6378 - val_accuracy: 0.4545\n",
            "Epoch 290/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 5.6586 - val_accuracy: 0.4545\n",
            "Epoch 291/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 5.6650 - val_accuracy: 0.4545\n",
            "Epoch 292/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 5.6843 - val_accuracy: 0.4545\n",
            "Epoch 293/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 5.7086 - val_accuracy: 0.4545\n",
            "Epoch 294/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 5.7046 - val_accuracy: 0.4545\n",
            "Epoch 295/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 5.7204 - val_accuracy: 0.4545\n",
            "Epoch 296/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 5.7216 - val_accuracy: 0.4545\n",
            "Epoch 297/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 5.7490 - val_accuracy: 0.4545\n",
            "Epoch 298/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 5.7560 - val_accuracy: 0.4545\n",
            "Epoch 299/300\n",
            "87/87 [==============================] - 0s 352us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 5.7684 - val_accuracy: 0.4545\n",
            "Epoch 300/300\n",
            "87/87 [==============================] - 0s 358us/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 5.7760 - val_accuracy: 0.4545\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/300\n",
            "87/87 [==============================] - 0s 964us/step - loss: 1.1505 - accuracy: 0.3103 - val_loss: 1.0668 - val_accuracy: 0.4091\n",
            "Epoch 2/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.0547 - accuracy: 0.4023 - val_loss: 1.0277 - val_accuracy: 0.4091\n",
            "Epoch 3/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.9962 - accuracy: 0.4598 - val_loss: 1.0017 - val_accuracy: 0.4091\n",
            "Epoch 4/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.9717 - accuracy: 0.5057 - val_loss: 0.9794 - val_accuracy: 0.4545\n",
            "Epoch 5/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.9431 - accuracy: 0.5517 - val_loss: 0.9700 - val_accuracy: 0.4091\n",
            "Epoch 6/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9307 - accuracy: 0.5287 - val_loss: 0.9618 - val_accuracy: 0.5000\n",
            "Epoch 7/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9137 - accuracy: 0.5747 - val_loss: 0.9520 - val_accuracy: 0.5227\n",
            "Epoch 8/300\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.9061 - accuracy: 0.5632 - val_loss: 0.9444 - val_accuracy: 0.5227\n",
            "Epoch 9/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8953 - accuracy: 0.5517 - val_loss: 0.9531 - val_accuracy: 0.5227\n",
            "Epoch 10/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8861 - accuracy: 0.5517 - val_loss: 0.9467 - val_accuracy: 0.5455\n",
            "Epoch 11/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8781 - accuracy: 0.5517 - val_loss: 0.9467 - val_accuracy: 0.5455\n",
            "Epoch 12/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8692 - accuracy: 0.5402 - val_loss: 0.9442 - val_accuracy: 0.5455\n",
            "Epoch 13/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8617 - accuracy: 0.5632 - val_loss: 0.9473 - val_accuracy: 0.5000\n",
            "Epoch 14/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8537 - accuracy: 0.5862 - val_loss: 0.9413 - val_accuracy: 0.5455\n",
            "Epoch 15/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8426 - accuracy: 0.5862 - val_loss: 0.9511 - val_accuracy: 0.4773\n",
            "Epoch 16/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8416 - accuracy: 0.5747 - val_loss: 0.9610 - val_accuracy: 0.5000\n",
            "Epoch 17/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8254 - accuracy: 0.5862 - val_loss: 0.9501 - val_accuracy: 0.5000\n",
            "Epoch 18/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8215 - accuracy: 0.6092 - val_loss: 0.9535 - val_accuracy: 0.5000\n",
            "Epoch 19/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8133 - accuracy: 0.6322 - val_loss: 0.9688 - val_accuracy: 0.4773\n",
            "Epoch 20/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8107 - accuracy: 0.6322 - val_loss: 0.9505 - val_accuracy: 0.5000\n",
            "Epoch 21/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7995 - accuracy: 0.6552 - val_loss: 0.9681 - val_accuracy: 0.5000\n",
            "Epoch 22/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7927 - accuracy: 0.6322 - val_loss: 0.9852 - val_accuracy: 0.4545\n",
            "Epoch 23/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7837 - accuracy: 0.6437 - val_loss: 0.9680 - val_accuracy: 0.5227\n",
            "Epoch 24/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.7809 - accuracy: 0.6552 - val_loss: 0.9646 - val_accuracy: 0.5000\n",
            "Epoch 25/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7703 - accuracy: 0.6667 - val_loss: 0.9738 - val_accuracy: 0.5000\n",
            "Epoch 26/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.7594 - accuracy: 0.6552 - val_loss: 0.9818 - val_accuracy: 0.4773\n",
            "Epoch 27/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.7514 - accuracy: 0.6552 - val_loss: 0.9868 - val_accuracy: 0.4773\n",
            "Epoch 28/300\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.7456 - accuracy: 0.6897 - val_loss: 0.9759 - val_accuracy: 0.4773\n",
            "Epoch 29/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.7340 - accuracy: 0.6897 - val_loss: 0.9858 - val_accuracy: 0.5000\n",
            "Epoch 30/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7254 - accuracy: 0.6897 - val_loss: 0.9868 - val_accuracy: 0.5000\n",
            "Epoch 31/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7276 - accuracy: 0.6897 - val_loss: 0.9810 - val_accuracy: 0.5000\n",
            "Epoch 32/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.7140 - accuracy: 0.6782 - val_loss: 1.0142 - val_accuracy: 0.5000\n",
            "Epoch 33/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.7089 - accuracy: 0.7126 - val_loss: 0.9943 - val_accuracy: 0.5000\n",
            "Epoch 34/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.6986 - accuracy: 0.7011 - val_loss: 0.9796 - val_accuracy: 0.5000\n",
            "Epoch 35/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.6837 - accuracy: 0.7241 - val_loss: 0.9983 - val_accuracy: 0.5000\n",
            "Epoch 36/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.6766 - accuracy: 0.7126 - val_loss: 1.0080 - val_accuracy: 0.5000\n",
            "Epoch 37/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.6695 - accuracy: 0.7126 - val_loss: 1.0019 - val_accuracy: 0.5000\n",
            "Epoch 38/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.6594 - accuracy: 0.7356 - val_loss: 0.9869 - val_accuracy: 0.5000\n",
            "Epoch 39/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.6615 - accuracy: 0.7126 - val_loss: 1.0210 - val_accuracy: 0.5000\n",
            "Epoch 40/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6443 - accuracy: 0.7126 - val_loss: 0.9842 - val_accuracy: 0.5227\n",
            "Epoch 41/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6355 - accuracy: 0.7471 - val_loss: 1.0002 - val_accuracy: 0.5000\n",
            "Epoch 42/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.6253 - accuracy: 0.7356 - val_loss: 1.0232 - val_accuracy: 0.5227\n",
            "Epoch 43/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.6187 - accuracy: 0.7471 - val_loss: 0.9980 - val_accuracy: 0.5000\n",
            "Epoch 44/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.6105 - accuracy: 0.7471 - val_loss: 1.0147 - val_accuracy: 0.5000\n",
            "Epoch 45/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6030 - accuracy: 0.7701 - val_loss: 0.9878 - val_accuracy: 0.5227\n",
            "Epoch 46/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5950 - accuracy: 0.7816 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
            "Epoch 47/300\n",
            "87/87 [==============================] - 0s 328us/step - loss: 0.5902 - accuracy: 0.7471 - val_loss: 1.0598 - val_accuracy: 0.5227\n",
            "Epoch 48/300\n",
            "87/87 [==============================] - 0s 345us/step - loss: 0.5737 - accuracy: 0.7931 - val_loss: 1.0069 - val_accuracy: 0.5227\n",
            "Epoch 49/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.5814 - accuracy: 0.7931 - val_loss: 1.0048 - val_accuracy: 0.5227\n",
            "Epoch 50/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.5595 - accuracy: 0.7931 - val_loss: 1.0390 - val_accuracy: 0.5227\n",
            "Epoch 51/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5550 - accuracy: 0.7701 - val_loss: 1.0124 - val_accuracy: 0.5227\n",
            "Epoch 52/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.5400 - accuracy: 0.8161 - val_loss: 1.0603 - val_accuracy: 0.5000\n",
            "Epoch 53/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.5467 - accuracy: 0.7931 - val_loss: 1.0269 - val_accuracy: 0.5227\n",
            "Epoch 54/300\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5295 - accuracy: 0.8161 - val_loss: 1.0740 - val_accuracy: 0.5000\n",
            "Epoch 55/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.5344 - accuracy: 0.8276 - val_loss: 1.0077 - val_accuracy: 0.5000\n",
            "Epoch 56/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.5263 - accuracy: 0.8046 - val_loss: 1.0979 - val_accuracy: 0.5000\n",
            "Epoch 57/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4973 - accuracy: 0.8276 - val_loss: 1.0633 - val_accuracy: 0.5227\n",
            "Epoch 58/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4959 - accuracy: 0.8276 - val_loss: 1.0398 - val_accuracy: 0.5000\n",
            "Epoch 59/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4870 - accuracy: 0.8391 - val_loss: 1.0877 - val_accuracy: 0.5000\n",
            "Epoch 60/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.4792 - accuracy: 0.8276 - val_loss: 1.0734 - val_accuracy: 0.5227\n",
            "Epoch 61/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4711 - accuracy: 0.8391 - val_loss: 1.0913 - val_accuracy: 0.5000\n",
            "Epoch 62/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.4660 - accuracy: 0.8506 - val_loss: 1.0776 - val_accuracy: 0.4773\n",
            "Epoch 63/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4549 - accuracy: 0.8506 - val_loss: 1.0931 - val_accuracy: 0.5000\n",
            "Epoch 64/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.4482 - accuracy: 0.8391 - val_loss: 1.1079 - val_accuracy: 0.5000\n",
            "Epoch 65/300\n",
            "87/87 [==============================] - 0s 332us/step - loss: 0.4426 - accuracy: 0.8391 - val_loss: 1.1101 - val_accuracy: 0.4773\n",
            "Epoch 66/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4399 - accuracy: 0.8966 - val_loss: 1.1432 - val_accuracy: 0.4773\n",
            "Epoch 67/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4280 - accuracy: 0.8736 - val_loss: 1.1197 - val_accuracy: 0.5000\n",
            "Epoch 68/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4341 - accuracy: 0.8391 - val_loss: 1.0941 - val_accuracy: 0.4773\n",
            "Epoch 69/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4222 - accuracy: 0.8736 - val_loss: 1.1629 - val_accuracy: 0.4545\n",
            "Epoch 70/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.4108 - accuracy: 0.8851 - val_loss: 1.1616 - val_accuracy: 0.4545\n",
            "Epoch 71/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.3979 - accuracy: 0.8736 - val_loss: 1.1518 - val_accuracy: 0.4545\n",
            "Epoch 72/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.3986 - accuracy: 0.8506 - val_loss: 1.1429 - val_accuracy: 0.4545\n",
            "Epoch 73/300\n",
            "87/87 [==============================] - 0s 189us/step - loss: 0.3909 - accuracy: 0.8621 - val_loss: 1.1833 - val_accuracy: 0.4773\n",
            "Epoch 74/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3858 - accuracy: 0.8506 - val_loss: 1.1593 - val_accuracy: 0.4545\n",
            "Epoch 75/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3794 - accuracy: 0.8966 - val_loss: 1.1714 - val_accuracy: 0.4545\n",
            "Epoch 76/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3650 - accuracy: 0.8736 - val_loss: 1.1860 - val_accuracy: 0.4545\n",
            "Epoch 77/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.3617 - accuracy: 0.9195 - val_loss: 1.2000 - val_accuracy: 0.4545\n",
            "Epoch 78/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.3616 - accuracy: 0.8966 - val_loss: 1.1987 - val_accuracy: 0.4318\n",
            "Epoch 79/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.3676 - accuracy: 0.9080 - val_loss: 1.1697 - val_accuracy: 0.4545\n",
            "Epoch 80/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.3350 - accuracy: 0.9080 - val_loss: 1.2855 - val_accuracy: 0.4545\n",
            "Epoch 81/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.3490 - accuracy: 0.8966 - val_loss: 1.1999 - val_accuracy: 0.4545\n",
            "Epoch 82/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.3292 - accuracy: 0.8966 - val_loss: 1.2334 - val_accuracy: 0.4545\n",
            "Epoch 83/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3320 - accuracy: 0.9310 - val_loss: 1.2499 - val_accuracy: 0.4318\n",
            "Epoch 84/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3475 - accuracy: 0.8851 - val_loss: 1.2461 - val_accuracy: 0.4318\n",
            "Epoch 85/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3229 - accuracy: 0.9195 - val_loss: 1.2639 - val_accuracy: 0.4318\n",
            "Epoch 86/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.3152 - accuracy: 0.8966 - val_loss: 1.2758 - val_accuracy: 0.4318\n",
            "Epoch 87/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.3168 - accuracy: 0.8851 - val_loss: 1.3090 - val_accuracy: 0.4318\n",
            "Epoch 88/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3012 - accuracy: 0.9310 - val_loss: 1.3009 - val_accuracy: 0.4318\n",
            "Epoch 89/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3022 - accuracy: 0.9195 - val_loss: 1.2648 - val_accuracy: 0.4318\n",
            "Epoch 90/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2897 - accuracy: 0.9425 - val_loss: 1.3175 - val_accuracy: 0.4318\n",
            "Epoch 91/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.2839 - accuracy: 0.9195 - val_loss: 1.3312 - val_accuracy: 0.4318\n",
            "Epoch 92/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.2845 - accuracy: 0.9310 - val_loss: 1.3631 - val_accuracy: 0.4318\n",
            "Epoch 93/300\n",
            "87/87 [==============================] - 0s 309us/step - loss: 0.2876 - accuracy: 0.9080 - val_loss: 1.3217 - val_accuracy: 0.4318\n",
            "Epoch 94/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.2713 - accuracy: 0.9310 - val_loss: 1.3964 - val_accuracy: 0.3864\n",
            "Epoch 95/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.2694 - accuracy: 0.9080 - val_loss: 1.3068 - val_accuracy: 0.4318\n",
            "Epoch 96/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2714 - accuracy: 0.9425 - val_loss: 1.3367 - val_accuracy: 0.4318\n",
            "Epoch 97/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2630 - accuracy: 0.9425 - val_loss: 1.3563 - val_accuracy: 0.4318\n",
            "Epoch 98/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.2545 - accuracy: 0.9425 - val_loss: 1.3973 - val_accuracy: 0.4318\n",
            "Epoch 99/300\n",
            "87/87 [==============================] - 0s 301us/step - loss: 0.2487 - accuracy: 0.9425 - val_loss: 1.3931 - val_accuracy: 0.4091\n",
            "Epoch 100/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2494 - accuracy: 0.9540 - val_loss: 1.4026 - val_accuracy: 0.4091\n",
            "Epoch 101/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2350 - accuracy: 0.9540 - val_loss: 1.4558 - val_accuracy: 0.4091\n",
            "Epoch 102/300\n",
            "87/87 [==============================] - 0s 354us/step - loss: 0.2360 - accuracy: 0.9425 - val_loss: 1.3986 - val_accuracy: 0.4091\n",
            "Epoch 103/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2321 - accuracy: 0.9540 - val_loss: 1.4408 - val_accuracy: 0.4091\n",
            "Epoch 104/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.2256 - accuracy: 0.9540 - val_loss: 1.4427 - val_accuracy: 0.4545\n",
            "Epoch 105/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.2244 - accuracy: 0.9540 - val_loss: 1.3869 - val_accuracy: 0.4091\n",
            "Epoch 106/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.2161 - accuracy: 0.9655 - val_loss: 1.4882 - val_accuracy: 0.4091\n",
            "Epoch 107/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2167 - accuracy: 0.9655 - val_loss: 1.4817 - val_accuracy: 0.4091\n",
            "Epoch 108/300\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.2074 - accuracy: 0.9655 - val_loss: 1.5032 - val_accuracy: 0.4091\n",
            "Epoch 109/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2110 - accuracy: 0.9425 - val_loss: 1.4835 - val_accuracy: 0.4318\n",
            "Epoch 110/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.2049 - accuracy: 0.9655 - val_loss: 1.5239 - val_accuracy: 0.4318\n",
            "Epoch 111/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2048 - accuracy: 0.9655 - val_loss: 1.4703 - val_accuracy: 0.4091\n",
            "Epoch 112/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.2015 - accuracy: 0.9655 - val_loss: 1.5778 - val_accuracy: 0.3864\n",
            "Epoch 113/300\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.2025 - accuracy: 0.9655 - val_loss: 1.4598 - val_accuracy: 0.4318\n",
            "Epoch 114/300\n",
            "87/87 [==============================] - 0s 341us/step - loss: 0.2036 - accuracy: 0.9655 - val_loss: 1.5442 - val_accuracy: 0.4545\n",
            "Epoch 115/300\n",
            "87/87 [==============================] - 0s 309us/step - loss: 0.1876 - accuracy: 0.9770 - val_loss: 1.5561 - val_accuracy: 0.4091\n",
            "Epoch 116/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.1774 - accuracy: 0.9655 - val_loss: 1.5747 - val_accuracy: 0.4318\n",
            "Epoch 117/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.1884 - accuracy: 0.9770 - val_loss: 1.4990 - val_accuracy: 0.4318\n",
            "Epoch 118/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1848 - accuracy: 0.9655 - val_loss: 1.6224 - val_accuracy: 0.4773\n",
            "Epoch 119/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1833 - accuracy: 0.9770 - val_loss: 1.5572 - val_accuracy: 0.4318\n",
            "Epoch 120/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.1717 - accuracy: 0.9770 - val_loss: 1.6560 - val_accuracy: 0.4318\n",
            "Epoch 121/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1704 - accuracy: 0.9885 - val_loss: 1.6077 - val_accuracy: 0.4318\n",
            "Epoch 122/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1621 - accuracy: 0.9885 - val_loss: 1.6875 - val_accuracy: 0.4545\n",
            "Epoch 123/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1645 - accuracy: 0.9770 - val_loss: 1.6608 - val_accuracy: 0.4091\n",
            "Epoch 124/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1583 - accuracy: 0.9770 - val_loss: 1.6357 - val_accuracy: 0.4318\n",
            "Epoch 125/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1524 - accuracy: 0.9885 - val_loss: 1.6999 - val_accuracy: 0.4545\n",
            "Epoch 126/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1523 - accuracy: 0.9885 - val_loss: 1.6676 - val_accuracy: 0.4318\n",
            "Epoch 127/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.1520 - accuracy: 0.9770 - val_loss: 1.7095 - val_accuracy: 0.4091\n",
            "Epoch 128/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1430 - accuracy: 0.9885 - val_loss: 1.7098 - val_accuracy: 0.4318\n",
            "Epoch 129/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1447 - accuracy: 0.9885 - val_loss: 1.6554 - val_accuracy: 0.4545\n",
            "Epoch 130/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1482 - accuracy: 0.9885 - val_loss: 1.7289 - val_accuracy: 0.4545\n",
            "Epoch 131/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.1394 - accuracy: 0.9885 - val_loss: 1.7279 - val_accuracy: 0.4091\n",
            "Epoch 132/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1465 - accuracy: 0.9885 - val_loss: 1.7585 - val_accuracy: 0.4091\n",
            "Epoch 133/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1453 - accuracy: 0.9770 - val_loss: 1.7573 - val_accuracy: 0.4773\n",
            "Epoch 134/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1464 - accuracy: 0.9770 - val_loss: 1.7987 - val_accuracy: 0.4773\n",
            "Epoch 135/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1503 - accuracy: 0.9885 - val_loss: 1.7603 - val_accuracy: 0.4318\n",
            "Epoch 136/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1382 - accuracy: 0.9655 - val_loss: 1.9545 - val_accuracy: 0.4318\n",
            "Epoch 137/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1411 - accuracy: 0.9885 - val_loss: 1.7344 - val_accuracy: 0.4773\n",
            "Epoch 138/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1240 - accuracy: 0.9770 - val_loss: 1.9153 - val_accuracy: 0.4773\n",
            "Epoch 139/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.1325 - accuracy: 0.9770 - val_loss: 1.7803 - val_accuracy: 0.5227\n",
            "Epoch 140/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1231 - accuracy: 0.9885 - val_loss: 1.9141 - val_accuracy: 0.4091\n",
            "Epoch 141/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.1251 - accuracy: 0.9770 - val_loss: 1.8482 - val_accuracy: 0.4091\n",
            "Epoch 142/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.1150 - accuracy: 0.9885 - val_loss: 1.8956 - val_accuracy: 0.4773\n",
            "Epoch 143/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1116 - accuracy: 0.9885 - val_loss: 1.8796 - val_accuracy: 0.4318\n",
            "Epoch 144/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1114 - accuracy: 0.9885 - val_loss: 1.8973 - val_accuracy: 0.4545\n",
            "Epoch 145/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1135 - accuracy: 0.9885 - val_loss: 1.8922 - val_accuracy: 0.4545\n",
            "Epoch 146/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1153 - accuracy: 0.9885 - val_loss: 1.9819 - val_accuracy: 0.4545\n",
            "Epoch 147/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1057 - accuracy: 0.9885 - val_loss: 1.9473 - val_accuracy: 0.4773\n",
            "Epoch 148/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1007 - accuracy: 0.9885 - val_loss: 1.9152 - val_accuracy: 0.4773\n",
            "Epoch 149/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1002 - accuracy: 0.9885 - val_loss: 1.9756 - val_accuracy: 0.4773\n",
            "Epoch 150/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.0999 - accuracy: 0.9885 - val_loss: 1.9534 - val_accuracy: 0.4773\n",
            "Epoch 151/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.1003 - accuracy: 0.9885 - val_loss: 1.9986 - val_accuracy: 0.4773\n",
            "Epoch 152/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.0984 - accuracy: 0.9885 - val_loss: 2.0160 - val_accuracy: 0.4545\n",
            "Epoch 153/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0950 - accuracy: 0.9885 - val_loss: 1.9381 - val_accuracy: 0.5000\n",
            "Epoch 154/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0957 - accuracy: 0.9885 - val_loss: 1.9977 - val_accuracy: 0.4773\n",
            "Epoch 155/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0920 - accuracy: 0.9885 - val_loss: 2.0266 - val_accuracy: 0.4773\n",
            "Epoch 156/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.0869 - accuracy: 0.9885 - val_loss: 2.0239 - val_accuracy: 0.4773\n",
            "Epoch 157/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0873 - accuracy: 0.9885 - val_loss: 2.0340 - val_accuracy: 0.4773\n",
            "Epoch 158/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0847 - accuracy: 0.9885 - val_loss: 2.0625 - val_accuracy: 0.4773\n",
            "Epoch 159/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.0878 - accuracy: 0.9885 - val_loss: 2.0382 - val_accuracy: 0.4773\n",
            "Epoch 160/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0832 - accuracy: 0.9885 - val_loss: 2.1009 - val_accuracy: 0.4773\n",
            "Epoch 161/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0823 - accuracy: 0.9885 - val_loss: 2.0844 - val_accuracy: 0.4773\n",
            "Epoch 162/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.0813 - accuracy: 0.9885 - val_loss: 2.0970 - val_accuracy: 0.4773\n",
            "Epoch 163/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0792 - accuracy: 0.9885 - val_loss: 2.1034 - val_accuracy: 0.4773\n",
            "Epoch 164/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0792 - accuracy: 0.9885 - val_loss: 2.1113 - val_accuracy: 0.4773\n",
            "Epoch 165/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.0780 - accuracy: 0.9885 - val_loss: 2.1108 - val_accuracy: 0.4773\n",
            "Epoch 166/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0771 - accuracy: 0.9885 - val_loss: 2.1511 - val_accuracy: 0.4773\n",
            "Epoch 167/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.0753 - accuracy: 0.9885 - val_loss: 2.1424 - val_accuracy: 0.4773\n",
            "Epoch 168/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0746 - accuracy: 0.9885 - val_loss: 2.1414 - val_accuracy: 0.4773\n",
            "Epoch 169/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.0754 - accuracy: 0.9885 - val_loss: 2.1810 - val_accuracy: 0.4773\n",
            "Epoch 170/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0763 - accuracy: 0.9885 - val_loss: 2.1557 - val_accuracy: 0.4773\n",
            "Epoch 171/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0751 - accuracy: 0.9885 - val_loss: 2.2267 - val_accuracy: 0.4773\n",
            "Epoch 172/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.0759 - accuracy: 0.9885 - val_loss: 2.1469 - val_accuracy: 0.5000\n",
            "Epoch 173/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0777 - accuracy: 0.9885 - val_loss: 2.2068 - val_accuracy: 0.4773\n",
            "Epoch 174/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0809 - accuracy: 0.9885 - val_loss: 2.2969 - val_accuracy: 0.4773\n",
            "Epoch 175/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0762 - accuracy: 0.9885 - val_loss: 2.2460 - val_accuracy: 0.4773\n",
            "Epoch 176/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0708 - accuracy: 0.9885 - val_loss: 2.2276 - val_accuracy: 0.4773\n",
            "Epoch 177/300\n",
            "87/87 [==============================] - 0s 357us/step - loss: 0.0658 - accuracy: 0.9885 - val_loss: 2.2546 - val_accuracy: 0.4773\n",
            "Epoch 178/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.0655 - accuracy: 0.9885 - val_loss: 2.2316 - val_accuracy: 0.4773\n",
            "Epoch 179/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0659 - accuracy: 0.9885 - val_loss: 2.3022 - val_accuracy: 0.4773\n",
            "Epoch 180/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.0629 - accuracy: 0.9885 - val_loss: 2.2410 - val_accuracy: 0.5000\n",
            "Epoch 181/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0638 - accuracy: 0.9885 - val_loss: 2.3241 - val_accuracy: 0.4773\n",
            "Epoch 182/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0624 - accuracy: 0.9885 - val_loss: 2.3073 - val_accuracy: 0.4773\n",
            "Epoch 183/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.0634 - accuracy: 0.9885 - val_loss: 2.3258 - val_accuracy: 0.4773\n",
            "Epoch 184/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0626 - accuracy: 0.9885 - val_loss: 2.3529 - val_accuracy: 0.4773\n",
            "Epoch 185/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0599 - accuracy: 0.9885 - val_loss: 2.2982 - val_accuracy: 0.4773\n",
            "Epoch 186/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0584 - accuracy: 0.9885 - val_loss: 2.3240 - val_accuracy: 0.4773\n",
            "Epoch 187/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.0568 - accuracy: 0.9885 - val_loss: 2.3656 - val_accuracy: 0.4773\n",
            "Epoch 188/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0570 - accuracy: 0.9885 - val_loss: 2.3649 - val_accuracy: 0.4773\n",
            "Epoch 189/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0573 - accuracy: 0.9885 - val_loss: 2.3604 - val_accuracy: 0.4773\n",
            "Epoch 190/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0574 - accuracy: 0.9885 - val_loss: 2.4261 - val_accuracy: 0.4545\n",
            "Epoch 191/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0537 - accuracy: 0.9885 - val_loss: 2.3771 - val_accuracy: 0.4773\n",
            "Epoch 192/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0541 - accuracy: 0.9885 - val_loss: 2.4000 - val_accuracy: 0.4545\n",
            "Epoch 193/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0530 - accuracy: 0.9885 - val_loss: 2.4027 - val_accuracy: 0.4545\n",
            "Epoch 194/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0526 - accuracy: 0.9885 - val_loss: 2.4012 - val_accuracy: 0.4545\n",
            "Epoch 195/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.0525 - accuracy: 0.9885 - val_loss: 2.4547 - val_accuracy: 0.4545\n",
            "Epoch 196/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.0512 - accuracy: 0.9885 - val_loss: 2.4361 - val_accuracy: 0.4545\n",
            "Epoch 197/300\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.0518 - accuracy: 0.9885 - val_loss: 2.4540 - val_accuracy: 0.4773\n",
            "Epoch 198/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0560 - accuracy: 0.9885 - val_loss: 2.5094 - val_accuracy: 0.4545\n",
            "Epoch 199/300\n",
            "87/87 [==============================] - 0s 337us/step - loss: 0.0548 - accuracy: 0.9885 - val_loss: 2.4032 - val_accuracy: 0.5227\n",
            "Epoch 200/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0483 - accuracy: 0.9885 - val_loss: 2.5740 - val_accuracy: 0.4545\n",
            "Epoch 201/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0512 - accuracy: 0.9885 - val_loss: 2.4828 - val_accuracy: 0.4545\n",
            "Epoch 202/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0497 - accuracy: 0.9885 - val_loss: 2.5065 - val_accuracy: 0.4773\n",
            "Epoch 203/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.0513 - accuracy: 0.9885 - val_loss: 2.5891 - val_accuracy: 0.4545\n",
            "Epoch 204/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.0488 - accuracy: 0.9885 - val_loss: 2.4983 - val_accuracy: 0.4773\n",
            "Epoch 205/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0482 - accuracy: 0.9885 - val_loss: 2.5667 - val_accuracy: 0.4545\n",
            "Epoch 206/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.0463 - accuracy: 0.9885 - val_loss: 2.5564 - val_accuracy: 0.4545\n",
            "Epoch 207/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 2.5627 - val_accuracy: 0.4773\n",
            "Epoch 208/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.0449 - accuracy: 0.9885 - val_loss: 2.5773 - val_accuracy: 0.4545\n",
            "Epoch 209/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.0446 - accuracy: 0.9885 - val_loss: 2.5798 - val_accuracy: 0.4773\n",
            "Epoch 210/300\n",
            "87/87 [==============================] - 0s 346us/step - loss: 0.0433 - accuracy: 0.9885 - val_loss: 2.5826 - val_accuracy: 0.4773\n",
            "Epoch 211/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0445 - accuracy: 0.9885 - val_loss: 2.6214 - val_accuracy: 0.4773\n",
            "Epoch 212/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 2.6087 - val_accuracy: 0.4773\n",
            "Epoch 213/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0432 - accuracy: 0.9885 - val_loss: 2.6337 - val_accuracy: 0.4545\n",
            "Epoch 214/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.0423 - accuracy: 0.9885 - val_loss: 2.6370 - val_accuracy: 0.4773\n",
            "Epoch 215/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0419 - accuracy: 0.9885 - val_loss: 2.6478 - val_accuracy: 0.4773\n",
            "Epoch 216/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 2.6341 - val_accuracy: 0.4773\n",
            "Epoch 217/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0407 - accuracy: 0.9885 - val_loss: 2.6600 - val_accuracy: 0.4773\n",
            "Epoch 218/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0404 - accuracy: 0.9885 - val_loss: 2.7035 - val_accuracy: 0.4773\n",
            "Epoch 219/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 2.6833 - val_accuracy: 0.4773\n",
            "Epoch 220/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 2.7107 - val_accuracy: 0.4773\n",
            "Epoch 221/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0388 - accuracy: 0.9885 - val_loss: 2.6959 - val_accuracy: 0.4773\n",
            "Epoch 222/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 2.7289 - val_accuracy: 0.4773\n",
            "Epoch 223/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 2.7378 - val_accuracy: 0.4773\n",
            "Epoch 224/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0381 - accuracy: 0.9885 - val_loss: 2.7488 - val_accuracy: 0.4773\n",
            "Epoch 225/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 2.7553 - val_accuracy: 0.4773\n",
            "Epoch 226/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 2.7539 - val_accuracy: 0.4773\n",
            "Epoch 227/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 2.7778 - val_accuracy: 0.4773\n",
            "Epoch 228/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 2.7558 - val_accuracy: 0.4773\n",
            "Epoch 229/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 2.8186 - val_accuracy: 0.4773\n",
            "Epoch 230/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 2.7762 - val_accuracy: 0.4773\n",
            "Epoch 231/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 2.8096 - val_accuracy: 0.4773\n",
            "Epoch 232/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 2.8282 - val_accuracy: 0.4773\n",
            "Epoch 233/300\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 2.8333 - val_accuracy: 0.4773\n",
            "Epoch 234/300\n",
            "87/87 [==============================] - 0s 316us/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 2.8551 - val_accuracy: 0.4773\n",
            "Epoch 235/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 2.8444 - val_accuracy: 0.4773\n",
            "Epoch 236/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 2.8794 - val_accuracy: 0.4773\n",
            "Epoch 237/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 2.8294 - val_accuracy: 0.4773\n",
            "Epoch 238/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 2.9085 - val_accuracy: 0.4773\n",
            "Epoch 239/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 2.8931 - val_accuracy: 0.4773\n",
            "Epoch 240/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 2.8632 - val_accuracy: 0.4773\n",
            "Epoch 241/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 2.8997 - val_accuracy: 0.4773\n",
            "Epoch 242/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 2.9053 - val_accuracy: 0.4773\n",
            "Epoch 243/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 2.9113 - val_accuracy: 0.4773\n",
            "Epoch 244/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 2.9097 - val_accuracy: 0.4773\n",
            "Epoch 245/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 2.9518 - val_accuracy: 0.4773\n",
            "Epoch 246/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 2.9247 - val_accuracy: 0.4773\n",
            "Epoch 247/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 2.9508 - val_accuracy: 0.4773\n",
            "Epoch 248/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 2.9542 - val_accuracy: 0.4773\n",
            "Epoch 249/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 2.9506 - val_accuracy: 0.4773\n",
            "Epoch 250/300\n",
            "87/87 [==============================] - 0s 301us/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 2.9570 - val_accuracy: 0.4773\n",
            "Epoch 251/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 2.9804 - val_accuracy: 0.4773\n",
            "Epoch 252/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 2.9774 - val_accuracy: 0.4773\n",
            "Epoch 253/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 2.9960 - val_accuracy: 0.4773\n",
            "Epoch 254/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 2.9937 - val_accuracy: 0.4773\n",
            "Epoch 255/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 2.9894 - val_accuracy: 0.4773\n",
            "Epoch 256/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 3.0244 - val_accuracy: 0.4773\n",
            "Epoch 257/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 3.0028 - val_accuracy: 0.4773\n",
            "Epoch 258/300\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 3.0339 - val_accuracy: 0.4773\n",
            "Epoch 259/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 3.0584 - val_accuracy: 0.4773\n",
            "Epoch 260/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 3.0452 - val_accuracy: 0.4773\n",
            "Epoch 261/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 3.0501 - val_accuracy: 0.4773\n",
            "Epoch 262/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.0478 - val_accuracy: 0.4773\n",
            "Epoch 263/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 3.0646 - val_accuracy: 0.4773\n",
            "Epoch 264/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.0900 - val_accuracy: 0.4773\n",
            "Epoch 265/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 3.0514 - val_accuracy: 0.4545\n",
            "Epoch 266/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 3.0598 - val_accuracy: 0.4545\n",
            "Epoch 267/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 3.0759 - val_accuracy: 0.4773\n",
            "Epoch 268/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 3.0940 - val_accuracy: 0.4773\n",
            "Epoch 269/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 3.1134 - val_accuracy: 0.4773\n",
            "Epoch 270/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.1169 - val_accuracy: 0.4773\n",
            "Epoch 271/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.0953 - val_accuracy: 0.4773\n",
            "Epoch 272/300\n",
            "87/87 [==============================] - 0s 345us/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 3.1164 - val_accuracy: 0.4773\n",
            "Epoch 273/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 3.1204 - val_accuracy: 0.4773\n",
            "Epoch 274/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 3.1289 - val_accuracy: 0.4773\n",
            "Epoch 275/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 3.1617 - val_accuracy: 0.4773\n",
            "Epoch 276/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 3.1320 - val_accuracy: 0.4545\n",
            "Epoch 277/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 3.1361 - val_accuracy: 0.4545\n",
            "Epoch 278/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 3.1657 - val_accuracy: 0.4773\n",
            "Epoch 279/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 3.1722 - val_accuracy: 0.4773\n",
            "Epoch 280/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 3.1392 - val_accuracy: 0.4545\n",
            "Epoch 281/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 3.1960 - val_accuracy: 0.4773\n",
            "Epoch 282/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 3.1804 - val_accuracy: 0.4773\n",
            "Epoch 283/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.1781 - val_accuracy: 0.4773\n",
            "Epoch 284/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.1922 - val_accuracy: 0.4773\n",
            "Epoch 285/300\n",
            "87/87 [==============================] - 0s 371us/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 3.2124 - val_accuracy: 0.4773\n",
            "Epoch 286/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.2156 - val_accuracy: 0.4773\n",
            "Epoch 287/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.2114 - val_accuracy: 0.4773\n",
            "Epoch 288/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 3.1981 - val_accuracy: 0.4545\n",
            "Epoch 289/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 3.2086 - val_accuracy: 0.4545\n",
            "Epoch 290/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 3.2290 - val_accuracy: 0.4773\n",
            "Epoch 291/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 3.2207 - val_accuracy: 0.4773\n",
            "Epoch 292/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 3.2381 - val_accuracy: 0.4545\n",
            "Epoch 293/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.2499 - val_accuracy: 0.4773\n",
            "Epoch 294/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.2541 - val_accuracy: 0.4773\n",
            "Epoch 295/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 3.2452 - val_accuracy: 0.4545\n",
            "Epoch 296/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.2627 - val_accuracy: 0.4545\n",
            "Epoch 297/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 3.2552 - val_accuracy: 0.4545\n",
            "Epoch 298/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 3.2505 - val_accuracy: 0.4773\n",
            "Epoch 299/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 3.2866 - val_accuracy: 0.4545\n",
            "Epoch 300/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 3.2761 - val_accuracy: 0.4545\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/300\n",
            "88/88 [==============================] - 0s 967us/step - loss: 1.0662 - accuracy: 0.4318 - val_loss: 1.0866 - val_accuracy: 0.5116\n",
            "Epoch 2/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0130 - accuracy: 0.4432 - val_loss: 1.0446 - val_accuracy: 0.4884\n",
            "Epoch 3/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.9735 - accuracy: 0.4886 - val_loss: 1.0264 - val_accuracy: 0.4884\n",
            "Epoch 4/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9435 - accuracy: 0.4773 - val_loss: 1.0157 - val_accuracy: 0.5349\n",
            "Epoch 5/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9248 - accuracy: 0.4886 - val_loss: 1.0146 - val_accuracy: 0.5581\n",
            "Epoch 6/300\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.9101 - accuracy: 0.4886 - val_loss: 1.0136 - val_accuracy: 0.5581\n",
            "Epoch 7/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8995 - accuracy: 0.5114 - val_loss: 1.0085 - val_accuracy: 0.5116\n",
            "Epoch 8/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8910 - accuracy: 0.5455 - val_loss: 1.0090 - val_accuracy: 0.5116\n",
            "Epoch 9/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8787 - accuracy: 0.5682 - val_loss: 1.0108 - val_accuracy: 0.5116\n",
            "Epoch 10/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8729 - accuracy: 0.5795 - val_loss: 1.0127 - val_accuracy: 0.5116\n",
            "Epoch 11/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8625 - accuracy: 0.5795 - val_loss: 1.0133 - val_accuracy: 0.5349\n",
            "Epoch 12/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8562 - accuracy: 0.5795 - val_loss: 1.0136 - val_accuracy: 0.5116\n",
            "Epoch 13/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8516 - accuracy: 0.5682 - val_loss: 1.0215 - val_accuracy: 0.5116\n",
            "Epoch 14/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8441 - accuracy: 0.5909 - val_loss: 1.0149 - val_accuracy: 0.5349\n",
            "Epoch 15/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8358 - accuracy: 0.5795 - val_loss: 1.0188 - val_accuracy: 0.5349\n",
            "Epoch 16/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8347 - accuracy: 0.5795 - val_loss: 1.0173 - val_accuracy: 0.5116\n",
            "Epoch 17/300\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8220 - accuracy: 0.6023 - val_loss: 1.0213 - val_accuracy: 0.5349\n",
            "Epoch 18/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.8147 - accuracy: 0.5795 - val_loss: 1.0263 - val_accuracy: 0.5116\n",
            "Epoch 19/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8080 - accuracy: 0.6136 - val_loss: 1.0267 - val_accuracy: 0.5116\n",
            "Epoch 20/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8004 - accuracy: 0.6364 - val_loss: 1.0282 - val_accuracy: 0.5116\n",
            "Epoch 21/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.7913 - accuracy: 0.6591 - val_loss: 1.0349 - val_accuracy: 0.4884\n",
            "Epoch 22/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7837 - accuracy: 0.6591 - val_loss: 1.0384 - val_accuracy: 0.4884\n",
            "Epoch 23/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7749 - accuracy: 0.6705 - val_loss: 1.0422 - val_accuracy: 0.4884\n",
            "Epoch 24/300\n",
            "88/88 [==============================] - 0s 301us/step - loss: 0.7677 - accuracy: 0.6705 - val_loss: 1.0460 - val_accuracy: 0.4884\n",
            "Epoch 25/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.7624 - accuracy: 0.6705 - val_loss: 1.0484 - val_accuracy: 0.4884\n",
            "Epoch 26/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7502 - accuracy: 0.6818 - val_loss: 1.0539 - val_accuracy: 0.4884\n",
            "Epoch 27/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7397 - accuracy: 0.6818 - val_loss: 1.0561 - val_accuracy: 0.4884\n",
            "Epoch 28/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7328 - accuracy: 0.6932 - val_loss: 1.0619 - val_accuracy: 0.5116\n",
            "Epoch 29/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7237 - accuracy: 0.7045 - val_loss: 1.0663 - val_accuracy: 0.5116\n",
            "Epoch 30/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.7148 - accuracy: 0.6932 - val_loss: 1.0679 - val_accuracy: 0.5349\n",
            "Epoch 31/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.7042 - accuracy: 0.7159 - val_loss: 1.0677 - val_accuracy: 0.5349\n",
            "Epoch 32/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.7013 - accuracy: 0.7045 - val_loss: 1.0676 - val_accuracy: 0.5349\n",
            "Epoch 33/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.6901 - accuracy: 0.7273 - val_loss: 1.0754 - val_accuracy: 0.5116\n",
            "Epoch 34/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.6831 - accuracy: 0.7273 - val_loss: 1.0779 - val_accuracy: 0.5349\n",
            "Epoch 35/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.6744 - accuracy: 0.7273 - val_loss: 1.0844 - val_accuracy: 0.5349\n",
            "Epoch 36/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6655 - accuracy: 0.7273 - val_loss: 1.0855 - val_accuracy: 0.5349\n",
            "Epoch 37/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.6575 - accuracy: 0.7500 - val_loss: 1.0898 - val_accuracy: 0.5349\n",
            "Epoch 38/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.6568 - accuracy: 0.7500 - val_loss: 1.1023 - val_accuracy: 0.4884\n",
            "Epoch 39/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6460 - accuracy: 0.7500 - val_loss: 1.0955 - val_accuracy: 0.5349\n",
            "Epoch 40/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.6398 - accuracy: 0.7273 - val_loss: 1.0978 - val_accuracy: 0.5349\n",
            "Epoch 41/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.6305 - accuracy: 0.7386 - val_loss: 1.1067 - val_accuracy: 0.5116\n",
            "Epoch 42/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.6225 - accuracy: 0.7386 - val_loss: 1.1056 - val_accuracy: 0.5116\n",
            "Epoch 43/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.6133 - accuracy: 0.7386 - val_loss: 1.1086 - val_accuracy: 0.5116\n",
            "Epoch 44/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.6091 - accuracy: 0.7500 - val_loss: 1.1142 - val_accuracy: 0.5116\n",
            "Epoch 45/300\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.6004 - accuracy: 0.7500 - val_loss: 1.1128 - val_accuracy: 0.5349\n",
            "Epoch 46/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5943 - accuracy: 0.7614 - val_loss: 1.1175 - val_accuracy: 0.5116\n",
            "Epoch 47/300\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.5893 - accuracy: 0.7386 - val_loss: 1.1231 - val_accuracy: 0.5349\n",
            "Epoch 48/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.5807 - accuracy: 0.7386 - val_loss: 1.1318 - val_accuracy: 0.5349\n",
            "Epoch 49/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.5778 - accuracy: 0.7727 - val_loss: 1.1437 - val_accuracy: 0.5581\n",
            "Epoch 50/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.5757 - accuracy: 0.7614 - val_loss: 1.1357 - val_accuracy: 0.5349\n",
            "Epoch 51/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.5696 - accuracy: 0.7614 - val_loss: 1.1408 - val_accuracy: 0.5349\n",
            "Epoch 52/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.5569 - accuracy: 0.7727 - val_loss: 1.1535 - val_accuracy: 0.5581\n",
            "Epoch 53/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.5562 - accuracy: 0.7841 - val_loss: 1.1624 - val_accuracy: 0.5349\n",
            "Epoch 54/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5504 - accuracy: 0.7614 - val_loss: 1.1570 - val_accuracy: 0.5581\n",
            "Epoch 55/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.5413 - accuracy: 0.7727 - val_loss: 1.1606 - val_accuracy: 0.5581\n",
            "Epoch 56/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.5350 - accuracy: 0.7955 - val_loss: 1.1603 - val_accuracy: 0.5581\n",
            "Epoch 57/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.5298 - accuracy: 0.7727 - val_loss: 1.1699 - val_accuracy: 0.5581\n",
            "Epoch 58/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.5265 - accuracy: 0.7727 - val_loss: 1.1625 - val_accuracy: 0.5814\n",
            "Epoch 59/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5181 - accuracy: 0.7841 - val_loss: 1.1752 - val_accuracy: 0.5814\n",
            "Epoch 60/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5167 - accuracy: 0.7841 - val_loss: 1.1793 - val_accuracy: 0.6047\n",
            "Epoch 61/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5044 - accuracy: 0.7955 - val_loss: 1.1909 - val_accuracy: 0.5814\n",
            "Epoch 62/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5012 - accuracy: 0.7841 - val_loss: 1.1925 - val_accuracy: 0.6047\n",
            "Epoch 63/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.4991 - accuracy: 0.7955 - val_loss: 1.1949 - val_accuracy: 0.6047\n",
            "Epoch 64/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4900 - accuracy: 0.7841 - val_loss: 1.1937 - val_accuracy: 0.6047\n",
            "Epoch 65/300\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.4845 - accuracy: 0.8068 - val_loss: 1.2115 - val_accuracy: 0.6047\n",
            "Epoch 66/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.4852 - accuracy: 0.8295 - val_loss: 1.2135 - val_accuracy: 0.5814\n",
            "Epoch 67/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.4719 - accuracy: 0.8295 - val_loss: 1.2312 - val_accuracy: 0.6047\n",
            "Epoch 68/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4667 - accuracy: 0.8295 - val_loss: 1.2267 - val_accuracy: 0.6047\n",
            "Epoch 69/300\n",
            "88/88 [==============================] - 0s 323us/step - loss: 0.4688 - accuracy: 0.8182 - val_loss: 1.2211 - val_accuracy: 0.5814\n",
            "Epoch 70/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.4575 - accuracy: 0.8295 - val_loss: 1.2424 - val_accuracy: 0.5814\n",
            "Epoch 71/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.4523 - accuracy: 0.8182 - val_loss: 1.2448 - val_accuracy: 0.6047\n",
            "Epoch 72/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.4497 - accuracy: 0.8182 - val_loss: 1.2423 - val_accuracy: 0.5814\n",
            "Epoch 73/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4392 - accuracy: 0.8295 - val_loss: 1.2572 - val_accuracy: 0.5814\n",
            "Epoch 74/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.4422 - accuracy: 0.8295 - val_loss: 1.2576 - val_accuracy: 0.5814\n",
            "Epoch 75/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.4292 - accuracy: 0.8295 - val_loss: 1.2717 - val_accuracy: 0.5814\n",
            "Epoch 76/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.4250 - accuracy: 0.8182 - val_loss: 1.2796 - val_accuracy: 0.6047\n",
            "Epoch 77/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.4183 - accuracy: 0.8295 - val_loss: 1.2747 - val_accuracy: 0.5814\n",
            "Epoch 78/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.4120 - accuracy: 0.8409 - val_loss: 1.2875 - val_accuracy: 0.5814\n",
            "Epoch 79/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.4084 - accuracy: 0.8295 - val_loss: 1.2913 - val_accuracy: 0.5814\n",
            "Epoch 80/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4125 - accuracy: 0.8409 - val_loss: 1.2995 - val_accuracy: 0.5814\n",
            "Epoch 81/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.4096 - accuracy: 0.8409 - val_loss: 1.2989 - val_accuracy: 0.5581\n",
            "Epoch 82/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3907 - accuracy: 0.8523 - val_loss: 1.3245 - val_accuracy: 0.5814\n",
            "Epoch 83/300\n",
            "88/88 [==============================] - 0s 301us/step - loss: 0.3933 - accuracy: 0.8409 - val_loss: 1.3187 - val_accuracy: 0.5581\n",
            "Epoch 84/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.3824 - accuracy: 0.8523 - val_loss: 1.3294 - val_accuracy: 0.5814\n",
            "Epoch 85/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.3821 - accuracy: 0.8409 - val_loss: 1.3252 - val_accuracy: 0.5814\n",
            "Epoch 86/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.3771 - accuracy: 0.8523 - val_loss: 1.3390 - val_accuracy: 0.5581\n",
            "Epoch 87/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.3900 - accuracy: 0.8409 - val_loss: 1.3561 - val_accuracy: 0.5581\n",
            "Epoch 88/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.3624 - accuracy: 0.8523 - val_loss: 1.3478 - val_accuracy: 0.5116\n",
            "Epoch 89/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3635 - accuracy: 0.8523 - val_loss: 1.3920 - val_accuracy: 0.5581\n",
            "Epoch 90/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.3708 - accuracy: 0.8409 - val_loss: 1.3756 - val_accuracy: 0.5349\n",
            "Epoch 91/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.3476 - accuracy: 0.8636 - val_loss: 1.4030 - val_accuracy: 0.5814\n",
            "Epoch 92/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.3447 - accuracy: 0.8750 - val_loss: 1.4122 - val_accuracy: 0.5116\n",
            "Epoch 93/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.3341 - accuracy: 0.8523 - val_loss: 1.3973 - val_accuracy: 0.5116\n",
            "Epoch 94/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3371 - accuracy: 0.8523 - val_loss: 1.4220 - val_accuracy: 0.5349\n",
            "Epoch 95/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.3395 - accuracy: 0.8636 - val_loss: 1.4266 - val_accuracy: 0.5116\n",
            "Epoch 96/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3420 - accuracy: 0.8636 - val_loss: 1.4348 - val_accuracy: 0.4884\n",
            "Epoch 97/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.3264 - accuracy: 0.8864 - val_loss: 1.4392 - val_accuracy: 0.5814\n",
            "Epoch 98/300\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.3203 - accuracy: 0.8523 - val_loss: 1.4597 - val_accuracy: 0.4884\n",
            "Epoch 99/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.3081 - accuracy: 0.8750 - val_loss: 1.4684 - val_accuracy: 0.5349\n",
            "Epoch 100/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3133 - accuracy: 0.8750 - val_loss: 1.4696 - val_accuracy: 0.5116\n",
            "Epoch 101/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.2999 - accuracy: 0.8750 - val_loss: 1.4888 - val_accuracy: 0.5349\n",
            "Epoch 102/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.3040 - accuracy: 0.8750 - val_loss: 1.4924 - val_accuracy: 0.4884\n",
            "Epoch 103/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.2955 - accuracy: 0.8864 - val_loss: 1.5035 - val_accuracy: 0.4884\n",
            "Epoch 104/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.2903 - accuracy: 0.8977 - val_loss: 1.5362 - val_accuracy: 0.5116\n",
            "Epoch 105/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.2875 - accuracy: 0.8977 - val_loss: 1.5378 - val_accuracy: 0.4884\n",
            "Epoch 106/300\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.2810 - accuracy: 0.8750 - val_loss: 1.5597 - val_accuracy: 0.5116\n",
            "Epoch 107/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.2820 - accuracy: 0.9091 - val_loss: 1.5717 - val_accuracy: 0.4884\n",
            "Epoch 108/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.2683 - accuracy: 0.9091 - val_loss: 1.5689 - val_accuracy: 0.5116\n",
            "Epoch 109/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.2789 - accuracy: 0.8750 - val_loss: 1.5917 - val_accuracy: 0.5116\n",
            "Epoch 110/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.2716 - accuracy: 0.9205 - val_loss: 1.6327 - val_accuracy: 0.4419\n",
            "Epoch 111/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.2668 - accuracy: 0.9318 - val_loss: 1.6550 - val_accuracy: 0.4651\n",
            "Epoch 112/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2905 - accuracy: 0.9091 - val_loss: 1.6284 - val_accuracy: 0.5116\n",
            "Epoch 113/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.2558 - accuracy: 0.9432 - val_loss: 1.6778 - val_accuracy: 0.4651\n",
            "Epoch 114/300\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.2457 - accuracy: 0.9545 - val_loss: 1.6811 - val_accuracy: 0.4884\n",
            "Epoch 115/300\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.2497 - accuracy: 0.9318 - val_loss: 1.6906 - val_accuracy: 0.4884\n",
            "Epoch 116/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2367 - accuracy: 0.9432 - val_loss: 1.7013 - val_accuracy: 0.5349\n",
            "Epoch 117/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2334 - accuracy: 0.9318 - val_loss: 1.7006 - val_accuracy: 0.4884\n",
            "Epoch 118/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.2320 - accuracy: 0.9432 - val_loss: 1.7247 - val_accuracy: 0.4884\n",
            "Epoch 119/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.2234 - accuracy: 0.9773 - val_loss: 1.7456 - val_accuracy: 0.5116\n",
            "Epoch 120/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.2292 - accuracy: 0.9545 - val_loss: 1.7737 - val_accuracy: 0.4651\n",
            "Epoch 121/300\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.2281 - accuracy: 0.9659 - val_loss: 1.7634 - val_accuracy: 0.4884\n",
            "Epoch 122/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.2225 - accuracy: 0.9545 - val_loss: 1.7874 - val_accuracy: 0.4884\n",
            "Epoch 123/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.2097 - accuracy: 0.9545 - val_loss: 1.7876 - val_accuracy: 0.4651\n",
            "Epoch 124/300\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.2109 - accuracy: 0.9659 - val_loss: 1.8165 - val_accuracy: 0.5116\n",
            "Epoch 125/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.2049 - accuracy: 0.9659 - val_loss: 1.8158 - val_accuracy: 0.4884\n",
            "Epoch 126/300\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2009 - accuracy: 0.9773 - val_loss: 1.8631 - val_accuracy: 0.4884\n",
            "Epoch 127/300\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.2011 - accuracy: 0.9659 - val_loss: 1.8349 - val_accuracy: 0.4884\n",
            "Epoch 128/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.2018 - accuracy: 0.9659 - val_loss: 1.8639 - val_accuracy: 0.4884\n",
            "Epoch 129/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.1942 - accuracy: 0.9773 - val_loss: 1.8989 - val_accuracy: 0.4884\n",
            "Epoch 130/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.1883 - accuracy: 0.9773 - val_loss: 1.9010 - val_accuracy: 0.4884\n",
            "Epoch 131/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1875 - accuracy: 0.9773 - val_loss: 1.9322 - val_accuracy: 0.4884\n",
            "Epoch 132/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1768 - accuracy: 0.9886 - val_loss: 1.9380 - val_accuracy: 0.4884\n",
            "Epoch 133/300\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.1741 - accuracy: 0.9773 - val_loss: 1.9558 - val_accuracy: 0.4884\n",
            "Epoch 134/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1722 - accuracy: 0.9886 - val_loss: 1.9920 - val_accuracy: 0.4884\n",
            "Epoch 135/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1685 - accuracy: 0.9886 - val_loss: 1.9809 - val_accuracy: 0.4884\n",
            "Epoch 136/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.1658 - accuracy: 0.9886 - val_loss: 1.9994 - val_accuracy: 0.4884\n",
            "Epoch 137/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.1655 - accuracy: 0.9886 - val_loss: 2.0350 - val_accuracy: 0.4884\n",
            "Epoch 138/300\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.1588 - accuracy: 0.9886 - val_loss: 2.0383 - val_accuracy: 0.4884\n",
            "Epoch 139/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1576 - accuracy: 0.9886 - val_loss: 2.0661 - val_accuracy: 0.4884\n",
            "Epoch 140/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.1546 - accuracy: 0.9886 - val_loss: 2.0492 - val_accuracy: 0.4884\n",
            "Epoch 141/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.1502 - accuracy: 0.9886 - val_loss: 2.0881 - val_accuracy: 0.4884\n",
            "Epoch 142/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.1507 - accuracy: 0.9886 - val_loss: 2.0902 - val_accuracy: 0.4884\n",
            "Epoch 143/300\n",
            "88/88 [==============================] - 0s 330us/step - loss: 0.1460 - accuracy: 0.9886 - val_loss: 2.1112 - val_accuracy: 0.4884\n",
            "Epoch 144/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1423 - accuracy: 0.9886 - val_loss: 2.1272 - val_accuracy: 0.4884\n",
            "Epoch 145/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.1410 - accuracy: 0.9886 - val_loss: 2.1607 - val_accuracy: 0.4884\n",
            "Epoch 146/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1375 - accuracy: 0.9886 - val_loss: 2.1784 - val_accuracy: 0.4884\n",
            "Epoch 147/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.1376 - accuracy: 0.9886 - val_loss: 2.1748 - val_accuracy: 0.4884\n",
            "Epoch 148/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1369 - accuracy: 0.9773 - val_loss: 2.2064 - val_accuracy: 0.4884\n",
            "Epoch 149/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1319 - accuracy: 0.9886 - val_loss: 2.2234 - val_accuracy: 0.4884\n",
            "Epoch 150/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1328 - accuracy: 0.9886 - val_loss: 2.2297 - val_accuracy: 0.4884\n",
            "Epoch 151/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.1303 - accuracy: 0.9886 - val_loss: 2.2923 - val_accuracy: 0.4884\n",
            "Epoch 152/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.1221 - accuracy: 0.9886 - val_loss: 2.2707 - val_accuracy: 0.4884\n",
            "Epoch 153/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.1234 - accuracy: 0.9886 - val_loss: 2.2812 - val_accuracy: 0.4884\n",
            "Epoch 154/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1262 - accuracy: 0.9886 - val_loss: 2.3203 - val_accuracy: 0.4884\n",
            "Epoch 155/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1386 - accuracy: 0.9773 - val_loss: 2.3166 - val_accuracy: 0.4884\n",
            "Epoch 156/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1269 - accuracy: 0.9886 - val_loss: 2.3369 - val_accuracy: 0.4651\n",
            "Epoch 157/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1152 - accuracy: 0.9886 - val_loss: 2.3395 - val_accuracy: 0.5116\n",
            "Epoch 158/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1101 - accuracy: 0.9886 - val_loss: 2.3566 - val_accuracy: 0.4651\n",
            "Epoch 159/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.1116 - accuracy: 1.0000 - val_loss: 2.3604 - val_accuracy: 0.5116\n",
            "Epoch 160/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.1072 - accuracy: 0.9886 - val_loss: 2.4033 - val_accuracy: 0.4884\n",
            "Epoch 161/300\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 2.4024 - val_accuracy: 0.4651\n",
            "Epoch 162/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1063 - accuracy: 1.0000 - val_loss: 2.4001 - val_accuracy: 0.4884\n",
            "Epoch 163/300\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 2.4282 - val_accuracy: 0.4651\n",
            "Epoch 164/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.1018 - accuracy: 0.9886 - val_loss: 2.4513 - val_accuracy: 0.4651\n",
            "Epoch 165/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.1005 - accuracy: 1.0000 - val_loss: 2.4675 - val_accuracy: 0.4651\n",
            "Epoch 166/300\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.0957 - accuracy: 1.0000 - val_loss: 2.4707 - val_accuracy: 0.4651\n",
            "Epoch 167/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.0949 - accuracy: 1.0000 - val_loss: 2.4853 - val_accuracy: 0.4419\n",
            "Epoch 168/300\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 2.4769 - val_accuracy: 0.4651\n",
            "Epoch 169/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.0926 - accuracy: 1.0000 - val_loss: 2.5178 - val_accuracy: 0.4651\n",
            "Epoch 170/300\n",
            "88/88 [==============================] - 0s 315us/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 2.5252 - val_accuracy: 0.4651\n",
            "Epoch 171/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 2.5448 - val_accuracy: 0.4419\n",
            "Epoch 172/300\n",
            "88/88 [==============================] - 0s 288us/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 2.5425 - val_accuracy: 0.4651\n",
            "Epoch 173/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0852 - accuracy: 1.0000 - val_loss: 2.5456 - val_accuracy: 0.4651\n",
            "Epoch 174/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 2.5944 - val_accuracy: 0.4651\n",
            "Epoch 175/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.0850 - accuracy: 1.0000 - val_loss: 2.5937 - val_accuracy: 0.4651\n",
            "Epoch 176/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.0817 - accuracy: 1.0000 - val_loss: 2.6160 - val_accuracy: 0.4651\n",
            "Epoch 177/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 2.6187 - val_accuracy: 0.4651\n",
            "Epoch 178/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 2.6159 - val_accuracy: 0.4651\n",
            "Epoch 179/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 2.6458 - val_accuracy: 0.4651\n",
            "Epoch 180/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.0809 - accuracy: 1.0000 - val_loss: 2.6579 - val_accuracy: 0.4651\n",
            "Epoch 181/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.0780 - accuracy: 1.0000 - val_loss: 2.6412 - val_accuracy: 0.4651\n",
            "Epoch 182/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 2.7072 - val_accuracy: 0.4419\n",
            "Epoch 183/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 2.6876 - val_accuracy: 0.4651\n",
            "Epoch 184/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.0749 - accuracy: 1.0000 - val_loss: 2.6775 - val_accuracy: 0.4651\n",
            "Epoch 185/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 2.7157 - val_accuracy: 0.4651\n",
            "Epoch 186/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 2.7219 - val_accuracy: 0.4651\n",
            "Epoch 187/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 2.7299 - val_accuracy: 0.4651\n",
            "Epoch 188/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0683 - accuracy: 1.0000 - val_loss: 2.7406 - val_accuracy: 0.4651\n",
            "Epoch 189/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 2.7464 - val_accuracy: 0.4651\n",
            "Epoch 190/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 2.7822 - val_accuracy: 0.4651\n",
            "Epoch 191/300\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 2.7599 - val_accuracy: 0.4651\n",
            "Epoch 192/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 2.7982 - val_accuracy: 0.4651\n",
            "Epoch 193/300\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 2.8064 - val_accuracy: 0.4651\n",
            "Epoch 194/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 2.7940 - val_accuracy: 0.4651\n",
            "Epoch 195/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 2.8273 - val_accuracy: 0.4651\n",
            "Epoch 196/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0577 - accuracy: 1.0000 - val_loss: 2.8128 - val_accuracy: 0.4651\n",
            "Epoch 197/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 2.8391 - val_accuracy: 0.4651\n",
            "Epoch 198/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 2.8661 - val_accuracy: 0.4651\n",
            "Epoch 199/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0535 - accuracy: 1.0000 - val_loss: 2.8514 - val_accuracy: 0.4651\n",
            "Epoch 200/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 2.8812 - val_accuracy: 0.4651\n",
            "Epoch 201/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 2.8774 - val_accuracy: 0.4651\n",
            "Epoch 202/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 2.8980 - val_accuracy: 0.4651\n",
            "Epoch 203/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 2.9046 - val_accuracy: 0.4651\n",
            "Epoch 204/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 2.9043 - val_accuracy: 0.4651\n",
            "Epoch 205/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 2.9260 - val_accuracy: 0.4651\n",
            "Epoch 206/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 2.9279 - val_accuracy: 0.4651\n",
            "Epoch 207/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 2.9498 - val_accuracy: 0.4651\n",
            "Epoch 208/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 2.9464 - val_accuracy: 0.4651\n",
            "Epoch 209/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 2.9555 - val_accuracy: 0.4651\n",
            "Epoch 210/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 2.9612 - val_accuracy: 0.4651\n",
            "Epoch 211/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 2.9784 - val_accuracy: 0.4651\n",
            "Epoch 212/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 2.9942 - val_accuracy: 0.4651\n",
            "Epoch 213/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 2.9916 - val_accuracy: 0.4651\n",
            "Epoch 214/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 3.0060 - val_accuracy: 0.4651\n",
            "Epoch 215/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 3.0285 - val_accuracy: 0.4651\n",
            "Epoch 216/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 3.0157 - val_accuracy: 0.4651\n",
            "Epoch 217/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 3.0130 - val_accuracy: 0.4651\n",
            "Epoch 218/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 3.0381 - val_accuracy: 0.4651\n",
            "Epoch 219/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 3.0453 - val_accuracy: 0.4651\n",
            "Epoch 220/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 3.0694 - val_accuracy: 0.4651\n",
            "Epoch 221/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 3.0632 - val_accuracy: 0.4651\n",
            "Epoch 222/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 3.0505 - val_accuracy: 0.4884\n",
            "Epoch 223/300\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 3.0863 - val_accuracy: 0.4651\n",
            "Epoch 224/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 3.0901 - val_accuracy: 0.4651\n",
            "Epoch 225/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 3.0954 - val_accuracy: 0.4884\n",
            "Epoch 226/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 3.1148 - val_accuracy: 0.4884\n",
            "Epoch 227/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 3.1195 - val_accuracy: 0.4884\n",
            "Epoch 228/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 3.1175 - val_accuracy: 0.4884\n",
            "Epoch 229/300\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 3.1275 - val_accuracy: 0.4884\n",
            "Epoch 230/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 3.1397 - val_accuracy: 0.4884\n",
            "Epoch 231/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 3.1401 - val_accuracy: 0.4884\n",
            "Epoch 232/300\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 3.1535 - val_accuracy: 0.4884\n",
            "Epoch 233/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 3.1644 - val_accuracy: 0.4884\n",
            "Epoch 234/300\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 3.1750 - val_accuracy: 0.4884\n",
            "Epoch 235/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 3.1771 - val_accuracy: 0.4884\n",
            "Epoch 236/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 3.1932 - val_accuracy: 0.4884\n",
            "Epoch 237/300\n",
            "88/88 [==============================] - 0s 304us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 3.1963 - val_accuracy: 0.4884\n",
            "Epoch 238/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 3.2061 - val_accuracy: 0.4884\n",
            "Epoch 239/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 3.1973 - val_accuracy: 0.4884\n",
            "Epoch 240/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 3.2169 - val_accuracy: 0.4884\n",
            "Epoch 241/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 3.2394 - val_accuracy: 0.4884\n",
            "Epoch 242/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 3.2336 - val_accuracy: 0.4884\n",
            "Epoch 243/300\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.2410 - val_accuracy: 0.4884\n",
            "Epoch 244/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 3.2552 - val_accuracy: 0.4884\n",
            "Epoch 245/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 3.2541 - val_accuracy: 0.4884\n",
            "Epoch 246/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 3.2655 - val_accuracy: 0.4884\n",
            "Epoch 247/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 3.2506 - val_accuracy: 0.4884\n",
            "Epoch 248/300\n",
            "88/88 [==============================] - 0s 298us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.2765 - val_accuracy: 0.4884\n",
            "Epoch 249/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 3.2944 - val_accuracy: 0.4884\n",
            "Epoch 250/300\n",
            "88/88 [==============================] - 0s 316us/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 3.2810 - val_accuracy: 0.4884\n",
            "Epoch 251/300\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 3.3043 - val_accuracy: 0.4884\n",
            "Epoch 252/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 3.2994 - val_accuracy: 0.4884\n",
            "Epoch 253/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 3.3082 - val_accuracy: 0.4884\n",
            "Epoch 254/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.3156 - val_accuracy: 0.4884\n",
            "Epoch 255/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 3.3224 - val_accuracy: 0.4884\n",
            "Epoch 256/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 3.3359 - val_accuracy: 0.4884\n",
            "Epoch 257/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 3.3347 - val_accuracy: 0.4884\n",
            "Epoch 258/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 3.3283 - val_accuracy: 0.4884\n",
            "Epoch 259/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 3.3599 - val_accuracy: 0.4884\n",
            "Epoch 260/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 3.3568 - val_accuracy: 0.4884\n",
            "Epoch 261/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 3.3516 - val_accuracy: 0.4884\n",
            "Epoch 262/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 3.3630 - val_accuracy: 0.4884\n",
            "Epoch 263/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 3.3759 - val_accuracy: 0.4884\n",
            "Epoch 264/300\n",
            "88/88 [==============================] - 0s 317us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 3.3774 - val_accuracy: 0.4884\n",
            "Epoch 265/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 3.3774 - val_accuracy: 0.4884\n",
            "Epoch 266/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 3.3941 - val_accuracy: 0.4884\n",
            "Epoch 267/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 3.3953 - val_accuracy: 0.4884\n",
            "Epoch 268/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 3.4060 - val_accuracy: 0.4884\n",
            "Epoch 269/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 3.4090 - val_accuracy: 0.4884\n",
            "Epoch 270/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 3.4159 - val_accuracy: 0.4884\n",
            "Epoch 271/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 3.4232 - val_accuracy: 0.4884\n",
            "Epoch 272/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 3.4188 - val_accuracy: 0.4884\n",
            "Epoch 273/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 3.4309 - val_accuracy: 0.4884\n",
            "Epoch 274/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 3.4456 - val_accuracy: 0.4884\n",
            "Epoch 275/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 3.4352 - val_accuracy: 0.4884\n",
            "Epoch 276/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 3.4576 - val_accuracy: 0.4884\n",
            "Epoch 277/300\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 3.4616 - val_accuracy: 0.4884\n",
            "Epoch 278/300\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 3.4587 - val_accuracy: 0.4884\n",
            "Epoch 279/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 3.4577 - val_accuracy: 0.4884\n",
            "Epoch 280/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 3.4701 - val_accuracy: 0.4884\n",
            "Epoch 281/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 3.4809 - val_accuracy: 0.4884\n",
            "Epoch 282/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 3.4821 - val_accuracy: 0.4884\n",
            "Epoch 283/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 3.4842 - val_accuracy: 0.4884\n",
            "Epoch 284/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 3.5064 - val_accuracy: 0.4884\n",
            "Epoch 285/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 3.4935 - val_accuracy: 0.4884\n",
            "Epoch 286/300\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 3.5021 - val_accuracy: 0.4884\n",
            "Epoch 287/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 3.5176 - val_accuracy: 0.4884\n",
            "Epoch 288/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 3.5161 - val_accuracy: 0.4884\n",
            "Epoch 289/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 3.5214 - val_accuracy: 0.4884\n",
            "Epoch 290/300\n",
            "88/88 [==============================] - 0s 288us/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 3.5217 - val_accuracy: 0.4884\n",
            "Epoch 291/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 3.5352 - val_accuracy: 0.4884\n",
            "Epoch 292/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.5295 - val_accuracy: 0.4884\n",
            "Epoch 293/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.5363 - val_accuracy: 0.5116\n",
            "Epoch 294/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 3.5531 - val_accuracy: 0.4884\n",
            "Epoch 295/300\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.5534 - val_accuracy: 0.4884\n",
            "Epoch 296/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 3.5572 - val_accuracy: 0.4884\n",
            "Epoch 297/300\n",
            "88/88 [==============================] - 0s 286us/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.5672 - val_accuracy: 0.5116\n",
            "Epoch 298/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 3.5578 - val_accuracy: 0.4884\n",
            "Epoch 299/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 3.5701 - val_accuracy: 0.5116\n",
            "Epoch 300/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 3.5756 - val_accuracy: 0.4884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYxTUsmLOQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "20ca8c7d-1a0e-4834-ed09-eb7ff7039e32"
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data, train_labels_dec):\n",
        "  \n",
        "  #train_data_np = \n",
        "\n",
        "  partial_train_data = np.array([train_data[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "  #Z-score\n",
        "  scaler = StandardScaler()\n",
        "  partial_train_data_stand = scaler.fit_transform(partial_train_data)\n",
        "  val_data_stand = scaler.transform(val_data)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=7, svd_solver='full')\n",
        "  pca.fit(partial_train_data_stand)\n",
        "  partial_train_data_stand_pca = pca.transform(partial_train_data_stand)\n",
        "  val_data_stand_pca = pca.transform(val_data_stand)\n",
        "\n",
        "  #Z-score after PCA\n",
        "  scaler_2 = StandardScaler()\n",
        "  partial_train_data_stand_pca_stand = scaler_2.fit_transform(partial_train_data_stand_pca)\n",
        "  val_data_stand_pca_stand = scaler_2.transform(val_data_stand_pca)\n",
        "\n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data_stand_pca_stand, one_hot_partial_train_targets, validation_data=(val_data_stand_pca_stand, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=88)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-171-18be020ce64f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#train_data_np =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mpartial_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mpartial_train_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels_dec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-171-18be020ce64f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#train_data_np =\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mpartial_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mpartial_train_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels_dec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1xS2-BcJraQ",
        "colab_type": "code",
        "outputId": "be1256fc-76bf-49dd-ce81-61483874b366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "train_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   2,   4,   5,   8,  10,  11,  12,  13,  14,  17,  20,  21,\n",
              "        22,  23,  24,  25,  26,  27,  29,  30,  31,  33,  34,  37,  38,\n",
              "        39,  40,  41,  42,  43,  46,  47,  48,  49,  50,  55,  58,  60,\n",
              "        61,  62,  63,  64,  65,  67,  69,  70,  71,  73,  75,  76,  77,\n",
              "        79,  81,  82,  83,  84,  85,  87,  88,  89,  91,  92,  94,  96,\n",
              "        97,  98,  99, 100, 101, 103, 106, 107, 108, 110, 115, 116, 117,\n",
              "       118, 119, 121, 122, 124, 126, 127, 129, 130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "9186b836-7e3b-4d77-a67b-ae1097e62e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "51edaaba-4158-460c-8bcf-b0f4feb889f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "acaf3151-8673-4743-dadb-ee89b96dd7a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "6da816c5-be44-452a-c69a-ace68c9765a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8a162cd978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvHPOxaj/B4eMQ0YNQVm+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "1b75e3f0-5d99-4482-831b-127892721259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8a162b8710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "8bec803c-0ddc-48bd-f0a1-f57a8ef4879f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.2654 - acc: 0.3130\n",
            "Epoch 2/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.2582 - acc: 0.3206\n",
            "Epoch 3/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.2512 - acc: 0.3282\n",
            "Epoch 4/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.2441 - acc: 0.3511\n",
            "Epoch 5/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2377 - acc: 0.3588\n",
            "Epoch 6/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.2312 - acc: 0.3817\n",
            "Epoch 7/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.2248 - acc: 0.3817\n",
            "Epoch 8/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2190 - acc: 0.3893\n",
            "Epoch 9/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.2133 - acc: 0.3893\n",
            "Epoch 10/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.2078 - acc: 0.3893\n",
            "Epoch 11/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.2026 - acc: 0.3969\n",
            "Epoch 12/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1976 - acc: 0.3969\n",
            "Epoch 13/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.1924 - acc: 0.3969\n",
            "Epoch 14/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1874 - acc: 0.4046\n",
            "Epoch 15/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1828 - acc: 0.4198\n",
            "Epoch 16/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.1780 - acc: 0.4198\n",
            "Epoch 17/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.1734 - acc: 0.4275\n",
            "Epoch 18/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.1692 - acc: 0.4351\n",
            "Epoch 19/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1649 - acc: 0.4198\n",
            "Epoch 20/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.1608 - acc: 0.4198\n",
            "Epoch 21/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.1572 - acc: 0.4275\n",
            "Epoch 22/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.1535 - acc: 0.4275\n",
            "Epoch 23/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.1498 - acc: 0.4275\n",
            "Epoch 24/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1461 - acc: 0.4351\n",
            "Epoch 25/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1426 - acc: 0.4351\n",
            "Epoch 26/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.1394 - acc: 0.4351\n",
            "Epoch 27/1000\n",
            "131/131 [==============================] - 0s 225us/step - loss: 1.1360 - acc: 0.4351\n",
            "Epoch 28/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.1329 - acc: 0.4504\n",
            "Epoch 29/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1299 - acc: 0.4504\n",
            "Epoch 30/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 1.1269 - acc: 0.4504\n",
            "Epoch 31/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 1.1237 - acc: 0.4504\n",
            "Epoch 32/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.1209 - acc: 0.4504\n",
            "Epoch 33/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 1.1182 - acc: 0.4580\n",
            "Epoch 34/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.1154 - acc: 0.4580\n",
            "Epoch 35/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.1123 - acc: 0.4580\n",
            "Epoch 36/1000\n",
            "131/131 [==============================] - 0s 208us/step - loss: 1.1099 - acc: 0.4580\n",
            "Epoch 37/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.1074 - acc: 0.4656\n",
            "Epoch 38/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1051 - acc: 0.4656\n",
            "Epoch 39/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1026 - acc: 0.4733\n",
            "Epoch 40/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.1002 - acc: 0.4733\n",
            "Epoch 41/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.0979 - acc: 0.4809\n",
            "Epoch 42/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0960 - acc: 0.4809\n",
            "Epoch 43/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0938 - acc: 0.4809\n",
            "Epoch 44/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0914 - acc: 0.4809\n",
            "Epoch 45/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0893 - acc: 0.4809\n",
            "Epoch 46/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0873 - acc: 0.4809\n",
            "Epoch 47/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0852 - acc: 0.4809\n",
            "Epoch 48/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0832 - acc: 0.4885\n",
            "Epoch 49/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0815 - acc: 0.4962\n",
            "Epoch 50/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 1.0797 - acc: 0.4962\n",
            "Epoch 51/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 1.0779 - acc: 0.4962\n",
            "Epoch 52/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0761 - acc: 0.4962\n",
            "Epoch 53/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 1.0746 - acc: 0.4962\n",
            "Epoch 54/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0726 - acc: 0.4962\n",
            "Epoch 55/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0708 - acc: 0.4962\n",
            "Epoch 56/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0691 - acc: 0.5038\n",
            "Epoch 57/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0675 - acc: 0.5038\n",
            "Epoch 58/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0657 - acc: 0.5038\n",
            "Epoch 59/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0642 - acc: 0.5038\n",
            "Epoch 60/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.0625 - acc: 0.4962\n",
            "Epoch 61/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0610 - acc: 0.4962\n",
            "Epoch 62/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0594 - acc: 0.4962\n",
            "Epoch 63/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0579 - acc: 0.4962\n",
            "Epoch 64/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0564 - acc: 0.4962\n",
            "Epoch 65/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0551 - acc: 0.4962\n",
            "Epoch 66/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 1.0535 - acc: 0.4962\n",
            "Epoch 67/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0521 - acc: 0.4962\n",
            "Epoch 68/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.0506 - acc: 0.4962\n",
            "Epoch 69/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.0491 - acc: 0.4962\n",
            "Epoch 70/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0478 - acc: 0.4962\n",
            "Epoch 71/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0464 - acc: 0.4962\n",
            "Epoch 72/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 1.0452 - acc: 0.4962\n",
            "Epoch 73/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0438 - acc: 0.4962\n",
            "Epoch 74/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0426 - acc: 0.4962\n",
            "Epoch 75/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.0413 - acc: 0.4962\n",
            "Epoch 76/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0401 - acc: 0.4962\n",
            "Epoch 77/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 1.0388 - acc: 0.4962\n",
            "Epoch 78/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0375 - acc: 0.4962\n",
            "Epoch 79/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.0365 - acc: 0.4962\n",
            "Epoch 80/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.0355 - acc: 0.4962\n",
            "Epoch 81/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 1.0343 - acc: 0.4962\n",
            "Epoch 82/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0332 - acc: 0.4962\n",
            "Epoch 83/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0322 - acc: 0.4885\n",
            "Epoch 84/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0311 - acc: 0.4885\n",
            "Epoch 85/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0302 - acc: 0.4885\n",
            "Epoch 86/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 1.0291 - acc: 0.4885\n",
            "Epoch 87/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0281 - acc: 0.4885\n",
            "Epoch 88/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0271 - acc: 0.4885\n",
            "Epoch 89/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0262 - acc: 0.4885\n",
            "Epoch 90/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0250 - acc: 0.4885\n",
            "Epoch 91/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.0240 - acc: 0.4809\n",
            "Epoch 92/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0232 - acc: 0.4733\n",
            "Epoch 93/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0222 - acc: 0.4733\n",
            "Epoch 94/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0213 - acc: 0.4733\n",
            "Epoch 95/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0203 - acc: 0.4809\n",
            "Epoch 96/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0195 - acc: 0.4733\n",
            "Epoch 97/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0186 - acc: 0.4733\n",
            "Epoch 98/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0176 - acc: 0.4809\n",
            "Epoch 99/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 1.0168 - acc: 0.4809\n",
            "Epoch 100/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0161 - acc: 0.4809\n",
            "Epoch 101/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 1.0151 - acc: 0.4809\n",
            "Epoch 102/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0143 - acc: 0.4809\n",
            "Epoch 103/1000\n",
            "131/131 [==============================] - 0s 196us/step - loss: 1.0136 - acc: 0.4809\n",
            "Epoch 104/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0128 - acc: 0.4809\n",
            "Epoch 105/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0121 - acc: 0.4809\n",
            "Epoch 106/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0115 - acc: 0.4809\n",
            "Epoch 107/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0107 - acc: 0.4809\n",
            "Epoch 108/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.0101 - acc: 0.4809\n",
            "Epoch 109/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0094 - acc: 0.4809\n",
            "Epoch 110/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0087 - acc: 0.4885\n",
            "Epoch 111/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.0081 - acc: 0.4885\n",
            "Epoch 112/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0074 - acc: 0.4885\n",
            "Epoch 113/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0068 - acc: 0.4885\n",
            "Epoch 114/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0061 - acc: 0.4885\n",
            "Epoch 115/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0055 - acc: 0.4885\n",
            "Epoch 116/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0049 - acc: 0.4885\n",
            "Epoch 117/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0043 - acc: 0.4885\n",
            "Epoch 118/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.0037 - acc: 0.4885\n",
            "Epoch 119/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.0031 - acc: 0.4885\n",
            "Epoch 120/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.0024 - acc: 0.4885\n",
            "Epoch 121/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0020 - acc: 0.4885\n",
            "Epoch 122/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.0013 - acc: 0.4885\n",
            "Epoch 123/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0008 - acc: 0.4885\n",
            "Epoch 124/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0002 - acc: 0.4885\n",
            "Epoch 125/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9995 - acc: 0.4885\n",
            "Epoch 126/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9989 - acc: 0.4885\n",
            "Epoch 127/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9983 - acc: 0.4885\n",
            "Epoch 128/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9978 - acc: 0.4885\n",
            "Epoch 129/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9972 - acc: 0.4885\n",
            "Epoch 130/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9966 - acc: 0.4885\n",
            "Epoch 131/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9960 - acc: 0.4885\n",
            "Epoch 132/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9954 - acc: 0.4885\n",
            "Epoch 133/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9948 - acc: 0.4885\n",
            "Epoch 134/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9943 - acc: 0.4885\n",
            "Epoch 135/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9937 - acc: 0.4885\n",
            "Epoch 136/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9930 - acc: 0.4962\n",
            "Epoch 137/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9923 - acc: 0.4962\n",
            "Epoch 138/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9918 - acc: 0.4962\n",
            "Epoch 139/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9913 - acc: 0.4962\n",
            "Epoch 140/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9907 - acc: 0.4962\n",
            "Epoch 141/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9900 - acc: 0.5038\n",
            "Epoch 142/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9894 - acc: 0.5038\n",
            "Epoch 143/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9890 - acc: 0.5038\n",
            "Epoch 144/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9884 - acc: 0.5115\n",
            "Epoch 145/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9880 - acc: 0.5038\n",
            "Epoch 146/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9874 - acc: 0.5115\n",
            "Epoch 147/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9869 - acc: 0.5115\n",
            "Epoch 148/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9864 - acc: 0.5115\n",
            "Epoch 149/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9859 - acc: 0.5115\n",
            "Epoch 150/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9854 - acc: 0.5191\n",
            "Epoch 151/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9849 - acc: 0.5191\n",
            "Epoch 152/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9845 - acc: 0.5191\n",
            "Epoch 153/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9840 - acc: 0.5191\n",
            "Epoch 154/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9836 - acc: 0.5191\n",
            "Epoch 155/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9831 - acc: 0.5191\n",
            "Epoch 156/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9826 - acc: 0.5191\n",
            "Epoch 157/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9821 - acc: 0.5267\n",
            "Epoch 158/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9817 - acc: 0.5420\n",
            "Epoch 159/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9812 - acc: 0.5420\n",
            "Epoch 160/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9807 - acc: 0.5420\n",
            "Epoch 161/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9802 - acc: 0.5344\n",
            "Epoch 162/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9797 - acc: 0.5420\n",
            "Epoch 163/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9793 - acc: 0.5420\n",
            "Epoch 164/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9788 - acc: 0.5420\n",
            "Epoch 165/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9783 - acc: 0.5420\n",
            "Epoch 166/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9778 - acc: 0.5420\n",
            "Epoch 167/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9773 - acc: 0.5420\n",
            "Epoch 168/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9768 - acc: 0.5420\n",
            "Epoch 169/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9765 - acc: 0.5420\n",
            "Epoch 170/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9760 - acc: 0.5420\n",
            "Epoch 171/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9755 - acc: 0.5420\n",
            "Epoch 172/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9750 - acc: 0.5420\n",
            "Epoch 173/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9746 - acc: 0.5420\n",
            "Epoch 174/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9743 - acc: 0.5420\n",
            "Epoch 175/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9738 - acc: 0.5420\n",
            "Epoch 176/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9733 - acc: 0.5420\n",
            "Epoch 177/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9727 - acc: 0.5420\n",
            "Epoch 178/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9725 - acc: 0.5420\n",
            "Epoch 179/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9719 - acc: 0.5344\n",
            "Epoch 180/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9715 - acc: 0.5344\n",
            "Epoch 181/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9711 - acc: 0.5344\n",
            "Epoch 182/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9707 - acc: 0.5344\n",
            "Epoch 183/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9702 - acc: 0.5344\n",
            "Epoch 184/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9699 - acc: 0.5344\n",
            "Epoch 185/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9693 - acc: 0.5344\n",
            "Epoch 186/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.9690 - acc: 0.5344\n",
            "Epoch 187/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9686 - acc: 0.5344\n",
            "Epoch 188/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9682 - acc: 0.5344\n",
            "Epoch 189/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9679 - acc: 0.5344\n",
            "Epoch 190/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9676 - acc: 0.5344\n",
            "Epoch 191/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9670 - acc: 0.5344\n",
            "Epoch 192/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9666 - acc: 0.5344\n",
            "Epoch 193/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9662 - acc: 0.5344\n",
            "Epoch 194/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9658 - acc: 0.5344\n",
            "Epoch 195/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9655 - acc: 0.5344\n",
            "Epoch 196/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9651 - acc: 0.5344\n",
            "Epoch 197/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9648 - acc: 0.5344\n",
            "Epoch 198/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9645 - acc: 0.5344\n",
            "Epoch 199/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9641 - acc: 0.5344\n",
            "Epoch 200/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9638 - acc: 0.5344\n",
            "Epoch 201/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9635 - acc: 0.5344\n",
            "Epoch 202/1000\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.9631 - acc: 0.5344\n",
            "Epoch 203/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9628 - acc: 0.5344\n",
            "Epoch 204/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9625 - acc: 0.5344\n",
            "Epoch 205/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9621 - acc: 0.5344\n",
            "Epoch 206/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9619 - acc: 0.5344\n",
            "Epoch 207/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9617 - acc: 0.5344\n",
            "Epoch 208/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9613 - acc: 0.5344\n",
            "Epoch 209/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9610 - acc: 0.5344\n",
            "Epoch 210/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9607 - acc: 0.5344\n",
            "Epoch 211/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9603 - acc: 0.5344\n",
            "Epoch 212/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9600 - acc: 0.5344\n",
            "Epoch 213/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9598 - acc: 0.5344\n",
            "Epoch 214/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9595 - acc: 0.5344\n",
            "Epoch 215/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9591 - acc: 0.5344\n",
            "Epoch 216/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9589 - acc: 0.5344\n",
            "Epoch 217/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9585 - acc: 0.5420\n",
            "Epoch 218/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9584 - acc: 0.5344\n",
            "Epoch 219/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9580 - acc: 0.5420\n",
            "Epoch 220/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9577 - acc: 0.5420\n",
            "Epoch 221/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9575 - acc: 0.5420\n",
            "Epoch 222/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9571 - acc: 0.5496\n",
            "Epoch 223/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9568 - acc: 0.5420\n",
            "Epoch 224/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9565 - acc: 0.5496\n",
            "Epoch 225/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9563 - acc: 0.5496\n",
            "Epoch 226/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9561 - acc: 0.5496\n",
            "Epoch 227/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9557 - acc: 0.5496\n",
            "Epoch 228/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9554 - acc: 0.5496\n",
            "Epoch 229/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9551 - acc: 0.5496\n",
            "Epoch 230/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9548 - acc: 0.5496\n",
            "Epoch 231/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9546 - acc: 0.5496\n",
            "Epoch 232/1000\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.9544 - acc: 0.5496\n",
            "Epoch 233/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9541 - acc: 0.5496\n",
            "Epoch 234/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9540 - acc: 0.5344\n",
            "Epoch 235/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9536 - acc: 0.5420\n",
            "Epoch 236/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9535 - acc: 0.5344\n",
            "Epoch 237/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9532 - acc: 0.5344\n",
            "Epoch 238/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9529 - acc: 0.5420\n",
            "Epoch 239/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9526 - acc: 0.5420\n",
            "Epoch 240/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9523 - acc: 0.5420\n",
            "Epoch 241/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9522 - acc: 0.5420\n",
            "Epoch 242/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9519 - acc: 0.5420\n",
            "Epoch 243/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9517 - acc: 0.5420\n",
            "Epoch 244/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9514 - acc: 0.5420\n",
            "Epoch 245/1000\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.9512 - acc: 0.5344\n",
            "Epoch 246/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9509 - acc: 0.5344\n",
            "Epoch 247/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9506 - acc: 0.5344\n",
            "Epoch 248/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9505 - acc: 0.5344\n",
            "Epoch 249/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9501 - acc: 0.5420\n",
            "Epoch 250/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9499 - acc: 0.5420\n",
            "Epoch 251/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9496 - acc: 0.5344\n",
            "Epoch 252/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9494 - acc: 0.5344\n",
            "Epoch 253/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9492 - acc: 0.5344\n",
            "Epoch 254/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9489 - acc: 0.5420\n",
            "Epoch 255/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9488 - acc: 0.5420\n",
            "Epoch 256/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9485 - acc: 0.5420\n",
            "Epoch 257/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9483 - acc: 0.5420\n",
            "Epoch 258/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9480 - acc: 0.5420\n",
            "Epoch 259/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9478 - acc: 0.5344\n",
            "Epoch 260/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9476 - acc: 0.5420\n",
            "Epoch 261/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9475 - acc: 0.5420\n",
            "Epoch 262/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9472 - acc: 0.5420\n",
            "Epoch 263/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9470 - acc: 0.5420\n",
            "Epoch 264/1000\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.9467 - acc: 0.5496\n",
            "Epoch 265/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9466 - acc: 0.5496\n",
            "Epoch 266/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9463 - acc: 0.5496\n",
            "Epoch 267/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9461 - acc: 0.5496\n",
            "Epoch 268/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9459 - acc: 0.5496\n",
            "Epoch 269/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9457 - acc: 0.5573\n",
            "Epoch 270/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9456 - acc: 0.5573\n",
            "Epoch 271/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9453 - acc: 0.5649\n",
            "Epoch 272/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9451 - acc: 0.5649\n",
            "Epoch 273/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9449 - acc: 0.5573\n",
            "Epoch 274/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9447 - acc: 0.5573\n",
            "Epoch 275/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9445 - acc: 0.5649\n",
            "Epoch 276/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9443 - acc: 0.5649\n",
            "Epoch 277/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9441 - acc: 0.5725\n",
            "Epoch 278/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9439 - acc: 0.5649\n",
            "Epoch 279/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9437 - acc: 0.5725\n",
            "Epoch 280/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9435 - acc: 0.5725\n",
            "Epoch 281/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9435 - acc: 0.5725\n",
            "Epoch 282/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9432 - acc: 0.5725\n",
            "Epoch 283/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9430 - acc: 0.5725\n",
            "Epoch 284/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9428 - acc: 0.5725\n",
            "Epoch 285/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9426 - acc: 0.5725\n",
            "Epoch 286/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9423 - acc: 0.5725\n",
            "Epoch 287/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9422 - acc: 0.5725\n",
            "Epoch 288/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9420 - acc: 0.5725\n",
            "Epoch 289/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9418 - acc: 0.5725\n",
            "Epoch 290/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9417 - acc: 0.5725\n",
            "Epoch 291/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9414 - acc: 0.5725\n",
            "Epoch 292/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9413 - acc: 0.5725\n",
            "Epoch 293/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9411 - acc: 0.5725\n",
            "Epoch 294/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9409 - acc: 0.5725\n",
            "Epoch 295/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9408 - acc: 0.5725\n",
            "Epoch 296/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9405 - acc: 0.5725\n",
            "Epoch 297/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9404 - acc: 0.5725\n",
            "Epoch 298/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9402 - acc: 0.5725\n",
            "Epoch 299/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9401 - acc: 0.5725\n",
            "Epoch 300/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9399 - acc: 0.5725\n",
            "Epoch 301/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.9398 - acc: 0.5725\n",
            "Epoch 302/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9397 - acc: 0.5725\n",
            "Epoch 303/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9395 - acc: 0.5725\n",
            "Epoch 304/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9392 - acc: 0.5725\n",
            "Epoch 305/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9391 - acc: 0.5725\n",
            "Epoch 306/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 0.9390 - acc: 0.5725\n",
            "Epoch 307/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9388 - acc: 0.5725\n",
            "Epoch 308/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9386 - acc: 0.5725\n",
            "Epoch 309/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9386 - acc: 0.5725\n",
            "Epoch 310/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9384 - acc: 0.5725\n",
            "Epoch 311/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9381 - acc: 0.5725\n",
            "Epoch 312/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9379 - acc: 0.5725\n",
            "Epoch 313/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9378 - acc: 0.5725\n",
            "Epoch 314/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9376 - acc: 0.5725\n",
            "Epoch 315/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9375 - acc: 0.5725\n",
            "Epoch 316/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9375 - acc: 0.5725\n",
            "Epoch 317/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9372 - acc: 0.5725\n",
            "Epoch 318/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9370 - acc: 0.5725\n",
            "Epoch 319/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9368 - acc: 0.5725\n",
            "Epoch 320/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9368 - acc: 0.5725\n",
            "Epoch 321/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9365 - acc: 0.5725\n",
            "Epoch 322/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9363 - acc: 0.5725\n",
            "Epoch 323/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9364 - acc: 0.5725\n",
            "Epoch 324/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9360 - acc: 0.5725\n",
            "Epoch 325/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9358 - acc: 0.5725\n",
            "Epoch 326/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9357 - acc: 0.5725\n",
            "Epoch 327/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9355 - acc: 0.5725\n",
            "Epoch 328/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9354 - acc: 0.5725\n",
            "Epoch 329/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9352 - acc: 0.5725\n",
            "Epoch 330/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9351 - acc: 0.5725\n",
            "Epoch 331/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9351 - acc: 0.5725\n",
            "Epoch 332/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9349 - acc: 0.5725\n",
            "Epoch 333/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.9346 - acc: 0.5802\n",
            "Epoch 334/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9344 - acc: 0.5802\n",
            "Epoch 335/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9343 - acc: 0.5802\n",
            "Epoch 336/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9342 - acc: 0.5802\n",
            "Epoch 337/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9341 - acc: 0.5802\n",
            "Epoch 338/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9339 - acc: 0.5802\n",
            "Epoch 339/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9337 - acc: 0.5802\n",
            "Epoch 340/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9335 - acc: 0.5802\n",
            "Epoch 341/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9334 - acc: 0.5802\n",
            "Epoch 342/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9333 - acc: 0.5802\n",
            "Epoch 343/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9331 - acc: 0.5802\n",
            "Epoch 344/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 345/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 346/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9326 - acc: 0.5802\n",
            "Epoch 347/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9324 - acc: 0.5802\n",
            "Epoch 348/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9324 - acc: 0.5802\n",
            "Epoch 349/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9322 - acc: 0.5878\n",
            "Epoch 350/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9320 - acc: 0.5878\n",
            "Epoch 351/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9319 - acc: 0.5878\n",
            "Epoch 352/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9318 - acc: 0.5878\n",
            "Epoch 353/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9316 - acc: 0.5954\n",
            "Epoch 354/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9315 - acc: 0.5878\n",
            "Epoch 355/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9313 - acc: 0.5954\n",
            "Epoch 356/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9312 - acc: 0.5954\n",
            "Epoch 357/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9310 - acc: 0.5954\n",
            "Epoch 358/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9309 - acc: 0.5954\n",
            "Epoch 359/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9307 - acc: 0.5954\n",
            "Epoch 360/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9306 - acc: 0.5954\n",
            "Epoch 361/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9304 - acc: 0.5954\n",
            "Epoch 362/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9303 - acc: 0.5954\n",
            "Epoch 363/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9301 - acc: 0.5954\n",
            "Epoch 364/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9299 - acc: 0.5954\n",
            "Epoch 365/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9299 - acc: 0.5954\n",
            "Epoch 366/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9298 - acc: 0.5954\n",
            "Epoch 367/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9296 - acc: 0.5954\n",
            "Epoch 368/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9294 - acc: 0.5954\n",
            "Epoch 369/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9293 - acc: 0.5954\n",
            "Epoch 370/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9293 - acc: 0.5954\n",
            "Epoch 371/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9292 - acc: 0.5954\n",
            "Epoch 372/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9290 - acc: 0.5878\n",
            "Epoch 373/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9289 - acc: 0.5878\n",
            "Epoch 374/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9286 - acc: 0.5878\n",
            "Epoch 375/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9286 - acc: 0.5878\n",
            "Epoch 376/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9284 - acc: 0.5878\n",
            "Epoch 377/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9283 - acc: 0.5878\n",
            "Epoch 378/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9282 - acc: 0.5878\n",
            "Epoch 379/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9280 - acc: 0.5878\n",
            "Epoch 380/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9279 - acc: 0.5878\n",
            "Epoch 381/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9278 - acc: 0.5878\n",
            "Epoch 382/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9277 - acc: 0.5878\n",
            "Epoch 383/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9275 - acc: 0.5878\n",
            "Epoch 384/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9275 - acc: 0.5878\n",
            "Epoch 385/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9273 - acc: 0.5878\n",
            "Epoch 386/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9273 - acc: 0.5878\n",
            "Epoch 387/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9272 - acc: 0.5878\n",
            "Epoch 388/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9270 - acc: 0.5878\n",
            "Epoch 389/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9269 - acc: 0.5878\n",
            "Epoch 390/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9267 - acc: 0.5878\n",
            "Epoch 391/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9267 - acc: 0.5878\n",
            "Epoch 392/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9266 - acc: 0.5878\n",
            "Epoch 393/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9263 - acc: 0.5878\n",
            "Epoch 394/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9263 - acc: 0.5954\n",
            "Epoch 395/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9261 - acc: 0.5878\n",
            "Epoch 396/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9259 - acc: 0.5878\n",
            "Epoch 397/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9258 - acc: 0.5954\n",
            "Epoch 398/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9257 - acc: 0.5878\n",
            "Epoch 399/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9257 - acc: 0.5954\n",
            "Epoch 400/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9256 - acc: 0.5954\n",
            "Epoch 401/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9253 - acc: 0.5954\n",
            "Epoch 402/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9252 - acc: 0.5954\n",
            "Epoch 403/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9251 - acc: 0.5954\n",
            "Epoch 404/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9250 - acc: 0.5954\n",
            "Epoch 405/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9249 - acc: 0.5954\n",
            "Epoch 406/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9247 - acc: 0.5954\n",
            "Epoch 407/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9249 - acc: 0.5954\n",
            "Epoch 408/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9245 - acc: 0.5954\n",
            "Epoch 409/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9245 - acc: 0.5954\n",
            "Epoch 410/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9243 - acc: 0.5954\n",
            "Epoch 411/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9241 - acc: 0.5954\n",
            "Epoch 412/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9239 - acc: 0.5954\n",
            "Epoch 413/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9239 - acc: 0.5954\n",
            "Epoch 414/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9237 - acc: 0.5954\n",
            "Epoch 415/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9235 - acc: 0.5954\n",
            "Epoch 416/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9235 - acc: 0.5954\n",
            "Epoch 417/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9234 - acc: 0.5954\n",
            "Epoch 418/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9231 - acc: 0.5954\n",
            "Epoch 419/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9230 - acc: 0.5954\n",
            "Epoch 420/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9229 - acc: 0.5954\n",
            "Epoch 421/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9229 - acc: 0.5954\n",
            "Epoch 422/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9227 - acc: 0.5954\n",
            "Epoch 423/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9226 - acc: 0.5954\n",
            "Epoch 424/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9224 - acc: 0.5954\n",
            "Epoch 425/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9224 - acc: 0.5954\n",
            "Epoch 426/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9222 - acc: 0.5954\n",
            "Epoch 427/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9220 - acc: 0.5954\n",
            "Epoch 428/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9219 - acc: 0.5954\n",
            "Epoch 429/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9219 - acc: 0.5954\n",
            "Epoch 430/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9216 - acc: 0.5954\n",
            "Epoch 431/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9215 - acc: 0.5954\n",
            "Epoch 432/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9215 - acc: 0.5954\n",
            "Epoch 433/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9214 - acc: 0.5954\n",
            "Epoch 434/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9213 - acc: 0.5954\n",
            "Epoch 435/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9210 - acc: 0.5954\n",
            "Epoch 436/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9210 - acc: 0.5954\n",
            "Epoch 437/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9209 - acc: 0.5954\n",
            "Epoch 438/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9207 - acc: 0.6031\n",
            "Epoch 439/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9206 - acc: 0.6031\n",
            "Epoch 440/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9204 - acc: 0.6031\n",
            "Epoch 441/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9204 - acc: 0.6031\n",
            "Epoch 442/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9203 - acc: 0.6031\n",
            "Epoch 443/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9201 - acc: 0.6107\n",
            "Epoch 444/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9200 - acc: 0.6107\n",
            "Epoch 445/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9199 - acc: 0.6107\n",
            "Epoch 446/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9198 - acc: 0.6031\n",
            "Epoch 447/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9198 - acc: 0.6107\n",
            "Epoch 448/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9196 - acc: 0.6031\n",
            "Epoch 449/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9194 - acc: 0.6107\n",
            "Epoch 450/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9193 - acc: 0.6031\n",
            "Epoch 451/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9191 - acc: 0.6031\n",
            "Epoch 452/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9191 - acc: 0.6031\n",
            "Epoch 453/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9188 - acc: 0.5954\n",
            "Epoch 454/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9187 - acc: 0.5954\n",
            "Epoch 455/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9186 - acc: 0.6031\n",
            "Epoch 456/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9185 - acc: 0.5954\n",
            "Epoch 457/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9184 - acc: 0.6031\n",
            "Epoch 458/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9183 - acc: 0.6031\n",
            "Epoch 459/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9181 - acc: 0.6031\n",
            "Epoch 460/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9182 - acc: 0.5954\n",
            "Epoch 461/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9180 - acc: 0.6031\n",
            "Epoch 462/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9179 - acc: 0.5954\n",
            "Epoch 463/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9178 - acc: 0.6031\n",
            "Epoch 464/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9176 - acc: 0.5954\n",
            "Epoch 465/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9175 - acc: 0.5954\n",
            "Epoch 466/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9174 - acc: 0.6031\n",
            "Epoch 467/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9173 - acc: 0.5954\n",
            "Epoch 468/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9172 - acc: 0.5954\n",
            "Epoch 469/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9171 - acc: 0.5954\n",
            "Epoch 470/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9169 - acc: 0.6031\n",
            "Epoch 471/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9168 - acc: 0.6031\n",
            "Epoch 472/1000\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.9167 - acc: 0.6031\n",
            "Epoch 473/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9165 - acc: 0.6031\n",
            "Epoch 474/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9164 - acc: 0.6031\n",
            "Epoch 475/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9164 - acc: 0.5954\n",
            "Epoch 476/1000\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.9164 - acc: 0.5954\n",
            "Epoch 477/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9161 - acc: 0.6031\n",
            "Epoch 478/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9159 - acc: 0.6031\n",
            "Epoch 479/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9159 - acc: 0.6031\n",
            "Epoch 480/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9157 - acc: 0.6031\n",
            "Epoch 481/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9157 - acc: 0.6031\n",
            "Epoch 482/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9155 - acc: 0.6031\n",
            "Epoch 483/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9154 - acc: 0.6031\n",
            "Epoch 484/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9154 - acc: 0.6031\n",
            "Epoch 485/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9151 - acc: 0.6031\n",
            "Epoch 486/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9151 - acc: 0.5954\n",
            "Epoch 487/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9149 - acc: 0.6031\n",
            "Epoch 488/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9148 - acc: 0.6031\n",
            "Epoch 489/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9146 - acc: 0.6031\n",
            "Epoch 490/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9145 - acc: 0.6031\n",
            "Epoch 491/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9144 - acc: 0.6031\n",
            "Epoch 492/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9142 - acc: 0.5954\n",
            "Epoch 493/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9141 - acc: 0.5954\n",
            "Epoch 494/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9140 - acc: 0.5954\n",
            "Epoch 495/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9141 - acc: 0.6031\n",
            "Epoch 496/1000\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.9138 - acc: 0.5954\n",
            "Epoch 497/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9137 - acc: 0.6031\n",
            "Epoch 498/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9135 - acc: 0.5954\n",
            "Epoch 499/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9135 - acc: 0.5954\n",
            "Epoch 500/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.9134 - acc: 0.5954\n",
            "Epoch 501/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9132 - acc: 0.5954\n",
            "Epoch 502/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9132 - acc: 0.5954\n",
            "Epoch 503/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9130 - acc: 0.6031\n",
            "Epoch 504/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9128 - acc: 0.6031\n",
            "Epoch 505/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9128 - acc: 0.6031\n",
            "Epoch 506/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9127 - acc: 0.6031\n",
            "Epoch 507/1000\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.9126 - acc: 0.6031\n",
            "Epoch 508/1000\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.9125 - acc: 0.6031\n",
            "Epoch 509/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9123 - acc: 0.6031\n",
            "Epoch 510/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9122 - acc: 0.6031\n",
            "Epoch 511/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9121 - acc: 0.6031\n",
            "Epoch 512/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9119 - acc: 0.6031\n",
            "Epoch 513/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9120 - acc: 0.6031\n",
            "Epoch 514/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9118 - acc: 0.5954\n",
            "Epoch 515/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9116 - acc: 0.5954\n",
            "Epoch 516/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9115 - acc: 0.5954\n",
            "Epoch 517/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9114 - acc: 0.5954\n",
            "Epoch 518/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9113 - acc: 0.5954\n",
            "Epoch 519/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9112 - acc: 0.5954\n",
            "Epoch 520/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9112 - acc: 0.5954\n",
            "Epoch 521/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9109 - acc: 0.6031\n",
            "Epoch 522/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9109 - acc: 0.5954\n",
            "Epoch 523/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9107 - acc: 0.6031\n",
            "Epoch 524/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9106 - acc: 0.6031\n",
            "Epoch 525/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9106 - acc: 0.5954\n",
            "Epoch 526/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9105 - acc: 0.5954\n",
            "Epoch 527/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9103 - acc: 0.5954\n",
            "Epoch 528/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9103 - acc: 0.5954\n",
            "Epoch 529/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9101 - acc: 0.5954\n",
            "Epoch 530/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9100 - acc: 0.5954\n",
            "Epoch 531/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9099 - acc: 0.6031\n",
            "Epoch 532/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9097 - acc: 0.5954\n",
            "Epoch 533/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9096 - acc: 0.6031\n",
            "Epoch 534/1000\n",
            "131/131 [==============================] - 0s 240us/step - loss: 0.9096 - acc: 0.6031\n",
            "Epoch 535/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9095 - acc: 0.6031\n",
            "Epoch 536/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9094 - acc: 0.6031\n",
            "Epoch 537/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9093 - acc: 0.5954\n",
            "Epoch 538/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9091 - acc: 0.6031\n",
            "Epoch 539/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9092 - acc: 0.6031\n",
            "Epoch 540/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9089 - acc: 0.6031\n",
            "Epoch 541/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9089 - acc: 0.5954\n",
            "Epoch 542/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9087 - acc: 0.5954\n",
            "Epoch 543/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9086 - acc: 0.6031\n",
            "Epoch 544/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9085 - acc: 0.5954\n",
            "Epoch 545/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9086 - acc: 0.6031\n",
            "Epoch 546/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9083 - acc: 0.6031\n",
            "Epoch 547/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9083 - acc: 0.5954\n",
            "Epoch 548/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9082 - acc: 0.6031\n",
            "Epoch 549/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9080 - acc: 0.5954\n",
            "Epoch 550/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9080 - acc: 0.6031\n",
            "Epoch 551/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9079 - acc: 0.5954\n",
            "Epoch 552/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9079 - acc: 0.5954\n",
            "Epoch 553/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9077 - acc: 0.5954\n",
            "Epoch 554/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9077 - acc: 0.6031\n",
            "Epoch 555/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9074 - acc: 0.6031\n",
            "Epoch 556/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9074 - acc: 0.5954\n",
            "Epoch 557/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9074 - acc: 0.5954\n",
            "Epoch 558/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9072 - acc: 0.5954\n",
            "Epoch 559/1000\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.9071 - acc: 0.6031\n",
            "Epoch 560/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9070 - acc: 0.6031\n",
            "Epoch 561/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9070 - acc: 0.5954\n",
            "Epoch 562/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9069 - acc: 0.5954\n",
            "Epoch 563/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9069 - acc: 0.5954\n",
            "Epoch 564/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9067 - acc: 0.6031\n",
            "Epoch 565/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9066 - acc: 0.5954\n",
            "Epoch 566/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9065 - acc: 0.5954\n",
            "Epoch 567/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9063 - acc: 0.5954\n",
            "Epoch 568/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9063 - acc: 0.5954\n",
            "Epoch 569/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9062 - acc: 0.5954\n",
            "Epoch 570/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9062 - acc: 0.5954\n",
            "Epoch 571/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9061 - acc: 0.5954\n",
            "Epoch 572/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9060 - acc: 0.6031\n",
            "Epoch 573/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9060 - acc: 0.6031\n",
            "Epoch 574/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9058 - acc: 0.6031\n",
            "Epoch 575/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9057 - acc: 0.6031\n",
            "Epoch 576/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9056 - acc: 0.6031\n",
            "Epoch 577/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9055 - acc: 0.6031\n",
            "Epoch 578/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9055 - acc: 0.6031\n",
            "Epoch 579/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9052 - acc: 0.6031\n",
            "Epoch 580/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9052 - acc: 0.6031\n",
            "Epoch 581/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9051 - acc: 0.6031\n",
            "Epoch 582/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9049 - acc: 0.6031\n",
            "Epoch 583/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9051 - acc: 0.5954\n",
            "Epoch 584/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9048 - acc: 0.6031\n",
            "Epoch 585/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9047 - acc: 0.6031\n",
            "Epoch 586/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9046 - acc: 0.6031\n",
            "Epoch 587/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9045 - acc: 0.6031\n",
            "Epoch 588/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9043 - acc: 0.6031\n",
            "Epoch 589/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9045 - acc: 0.6031\n",
            "Epoch 590/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9043 - acc: 0.6031\n",
            "Epoch 591/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9042 - acc: 0.6031\n",
            "Epoch 592/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9041 - acc: 0.6031\n",
            "Epoch 593/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9041 - acc: 0.6031\n",
            "Epoch 594/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9038 - acc: 0.5954\n",
            "Epoch 595/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9038 - acc: 0.5954\n",
            "Epoch 596/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9037 - acc: 0.5954\n",
            "Epoch 597/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 598/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 599/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 600/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 601/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 602/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 603/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9030 - acc: 0.5954\n",
            "Epoch 604/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9030 - acc: 0.5954\n",
            "Epoch 605/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9029 - acc: 0.5954\n",
            "Epoch 606/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9028 - acc: 0.5954\n",
            "Epoch 607/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9027 - acc: 0.5954\n",
            "Epoch 608/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9026 - acc: 0.5954\n",
            "Epoch 609/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9026 - acc: 0.6031\n",
            "Epoch 610/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9024 - acc: 0.5954\n",
            "Epoch 611/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9023 - acc: 0.5954\n",
            "Epoch 612/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9023 - acc: 0.5954\n",
            "Epoch 613/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9022 - acc: 0.5954\n",
            "Epoch 614/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9021 - acc: 0.5954\n",
            "Epoch 615/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9020 - acc: 0.5954\n",
            "Epoch 616/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9019 - acc: 0.5954\n",
            "Epoch 617/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9017 - acc: 0.5954\n",
            "Epoch 618/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9017 - acc: 0.5954\n",
            "Epoch 619/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9016 - acc: 0.5954\n",
            "Epoch 620/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9016 - acc: 0.6031\n",
            "Epoch 621/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9015 - acc: 0.5954\n",
            "Epoch 622/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9012 - acc: 0.6031\n",
            "Epoch 623/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9012 - acc: 0.5954\n",
            "Epoch 624/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9011 - acc: 0.6031\n",
            "Epoch 625/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9012 - acc: 0.6031\n",
            "Epoch 626/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9009 - acc: 0.6031\n",
            "Epoch 627/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9008 - acc: 0.6031\n",
            "Epoch 628/1000\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.9008 - acc: 0.6031\n",
            "Epoch 629/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9007 - acc: 0.6031\n",
            "Epoch 630/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9006 - acc: 0.6031\n",
            "Epoch 631/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9005 - acc: 0.6031\n",
            "Epoch 632/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9005 - acc: 0.6031\n",
            "Epoch 633/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9004 - acc: 0.5954\n",
            "Epoch 634/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9003 - acc: 0.6031\n",
            "Epoch 635/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9002 - acc: 0.6031\n",
            "Epoch 636/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9002 - acc: 0.6031\n",
            "Epoch 637/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9001 - acc: 0.6031\n",
            "Epoch 638/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9001 - acc: 0.6031\n",
            "Epoch 639/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8998 - acc: 0.6031\n",
            "Epoch 640/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8999 - acc: 0.6031\n",
            "Epoch 641/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8996 - acc: 0.6031\n",
            "Epoch 642/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8996 - acc: 0.5954\n",
            "Epoch 643/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8995 - acc: 0.6031\n",
            "Epoch 644/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8994 - acc: 0.6031\n",
            "Epoch 645/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8994 - acc: 0.6031\n",
            "Epoch 646/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8993 - acc: 0.5954\n",
            "Epoch 647/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8991 - acc: 0.6031\n",
            "Epoch 648/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8991 - acc: 0.6107\n",
            "Epoch 649/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8990 - acc: 0.6107\n",
            "Epoch 650/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8989 - acc: 0.6107\n",
            "Epoch 651/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8989 - acc: 0.5954\n",
            "Epoch 652/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8987 - acc: 0.6031\n",
            "Epoch 653/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8987 - acc: 0.6107\n",
            "Epoch 654/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8985 - acc: 0.6107\n",
            "Epoch 655/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8985 - acc: 0.6031\n",
            "Epoch 656/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8984 - acc: 0.6031\n",
            "Epoch 657/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 658/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 659/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 660/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8981 - acc: 0.6031\n",
            "Epoch 661/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 662/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8980 - acc: 0.6031\n",
            "Epoch 663/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 664/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 665/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8976 - acc: 0.6031\n",
            "Epoch 666/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8974 - acc: 0.6031\n",
            "Epoch 667/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8975 - acc: 0.6107\n",
            "Epoch 668/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.8973 - acc: 0.6031\n",
            "Epoch 669/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8973 - acc: 0.6031\n",
            "Epoch 670/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8972 - acc: 0.6031\n",
            "Epoch 671/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8971 - acc: 0.6031\n",
            "Epoch 672/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8970 - acc: 0.6031\n",
            "Epoch 673/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8969 - acc: 0.6031\n",
            "Epoch 674/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8968 - acc: 0.6031\n",
            "Epoch 675/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8967 - acc: 0.6031\n",
            "Epoch 676/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8966 - acc: 0.6031\n",
            "Epoch 677/1000\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.8966 - acc: 0.6031\n",
            "Epoch 678/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8965 - acc: 0.6107\n",
            "Epoch 679/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8964 - acc: 0.6107\n",
            "Epoch 680/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8963 - acc: 0.5954\n",
            "Epoch 681/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8962 - acc: 0.6031\n",
            "Epoch 682/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8962 - acc: 0.6031\n",
            "Epoch 683/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8960 - acc: 0.6031\n",
            "Epoch 684/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8960 - acc: 0.6107\n",
            "Epoch 685/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8958 - acc: 0.6031\n",
            "Epoch 686/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8958 - acc: 0.6031\n",
            "Epoch 687/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8959 - acc: 0.6107\n",
            "Epoch 688/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8956 - acc: 0.6031\n",
            "Epoch 689/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8957 - acc: 0.5954\n",
            "Epoch 690/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.8955 - acc: 0.6031\n",
            "Epoch 691/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8953 - acc: 0.5954\n",
            "Epoch 692/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8954 - acc: 0.5954\n",
            "Epoch 693/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8952 - acc: 0.6031\n",
            "Epoch 694/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8952 - acc: 0.5954\n",
            "Epoch 695/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8952 - acc: 0.6031\n",
            "Epoch 696/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8951 - acc: 0.6031\n",
            "Epoch 697/1000\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 698/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 699/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 700/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8947 - acc: 0.6031\n",
            "Epoch 701/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8945 - acc: 0.6031\n",
            "Epoch 702/1000\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.8946 - acc: 0.5954\n",
            "Epoch 703/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8944 - acc: 0.5954\n",
            "Epoch 704/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8944 - acc: 0.5954\n",
            "Epoch 705/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8946 - acc: 0.5954\n",
            "Epoch 706/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8942 - acc: 0.6031\n",
            "Epoch 707/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8941 - acc: 0.6031\n",
            "Epoch 708/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8941 - acc: 0.6031\n",
            "Epoch 709/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8940 - acc: 0.6031\n",
            "Epoch 710/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8939 - acc: 0.6031\n",
            "Epoch 711/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8939 - acc: 0.6031\n",
            "Epoch 712/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8938 - acc: 0.6031\n",
            "Epoch 713/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8938 - acc: 0.6031\n",
            "Epoch 714/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8937 - acc: 0.6031\n",
            "Epoch 715/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8936 - acc: 0.6031\n",
            "Epoch 716/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8935 - acc: 0.6031\n",
            "Epoch 717/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8933 - acc: 0.6031\n",
            "Epoch 718/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8934 - acc: 0.6031\n",
            "Epoch 719/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8933 - acc: 0.6031\n",
            "Epoch 720/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8931 - acc: 0.6031\n",
            "Epoch 721/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8931 - acc: 0.6031\n",
            "Epoch 722/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8930 - acc: 0.6031\n",
            "Epoch 723/1000\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.8929 - acc: 0.6031\n",
            "Epoch 724/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8929 - acc: 0.6031\n",
            "Epoch 725/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8928 - acc: 0.6107\n",
            "Epoch 726/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8927 - acc: 0.6107\n",
            "Epoch 727/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8927 - acc: 0.6031\n",
            "Epoch 728/1000\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.8926 - acc: 0.6031\n",
            "Epoch 729/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8924 - acc: 0.6031\n",
            "Epoch 730/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8924 - acc: 0.6031\n",
            "Epoch 731/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8923 - acc: 0.6031\n",
            "Epoch 732/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8922 - acc: 0.6031\n",
            "Epoch 733/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8923 - acc: 0.6031\n",
            "Epoch 734/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8921 - acc: 0.6031\n",
            "Epoch 735/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8920 - acc: 0.6031\n",
            "Epoch 736/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8919 - acc: 0.6031\n",
            "Epoch 737/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8918 - acc: 0.6031\n",
            "Epoch 738/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8917 - acc: 0.6031\n",
            "Epoch 739/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8917 - acc: 0.6107\n",
            "Epoch 740/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8916 - acc: 0.6031\n",
            "Epoch 741/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8915 - acc: 0.6031\n",
            "Epoch 742/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8915 - acc: 0.6031\n",
            "Epoch 743/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8913 - acc: 0.6107\n",
            "Epoch 744/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8912 - acc: 0.6031\n",
            "Epoch 745/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8911 - acc: 0.6031\n",
            "Epoch 746/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8911 - acc: 0.6107\n",
            "Epoch 747/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8910 - acc: 0.6031\n",
            "Epoch 748/1000\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.8910 - acc: 0.6107\n",
            "Epoch 749/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8909 - acc: 0.6107\n",
            "Epoch 750/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8908 - acc: 0.6107\n",
            "Epoch 751/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8906 - acc: 0.6107\n",
            "Epoch 752/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8905 - acc: 0.6031\n",
            "Epoch 753/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8905 - acc: 0.6031\n",
            "Epoch 754/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8904 - acc: 0.6031\n",
            "Epoch 755/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8902 - acc: 0.6031\n",
            "Epoch 756/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8903 - acc: 0.6031\n",
            "Epoch 757/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8902 - acc: 0.6031\n",
            "Epoch 758/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8903 - acc: 0.6031\n",
            "Epoch 759/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8900 - acc: 0.6031\n",
            "Epoch 760/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8900 - acc: 0.6031\n",
            "Epoch 761/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8899 - acc: 0.6031\n",
            "Epoch 762/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8898 - acc: 0.6031\n",
            "Epoch 763/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8897 - acc: 0.6031\n",
            "Epoch 764/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8897 - acc: 0.6031\n",
            "Epoch 765/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8896 - acc: 0.6031\n",
            "Epoch 766/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8895 - acc: 0.6031\n",
            "Epoch 767/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8893 - acc: 0.6031\n",
            "Epoch 768/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8894 - acc: 0.6031\n",
            "Epoch 769/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8892 - acc: 0.6031\n",
            "Epoch 770/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8891 - acc: 0.6031\n",
            "Epoch 771/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8892 - acc: 0.6031\n",
            "Epoch 772/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8889 - acc: 0.6031\n",
            "Epoch 773/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8889 - acc: 0.6031\n",
            "Epoch 774/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8888 - acc: 0.6031\n",
            "Epoch 775/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8886 - acc: 0.6031\n",
            "Epoch 776/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8887 - acc: 0.6031\n",
            "Epoch 777/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8885 - acc: 0.6031\n",
            "Epoch 778/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8886 - acc: 0.6031\n",
            "Epoch 779/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8885 - acc: 0.6031\n",
            "Epoch 780/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8884 - acc: 0.6031\n",
            "Epoch 781/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 782/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 783/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 784/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8881 - acc: 0.6031\n",
            "Epoch 785/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8880 - acc: 0.6031\n",
            "Epoch 786/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8879 - acc: 0.6031\n",
            "Epoch 787/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8877 - acc: 0.6031\n",
            "Epoch 788/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 789/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 790/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 791/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8874 - acc: 0.6031\n",
            "Epoch 792/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8873 - acc: 0.6031\n",
            "Epoch 793/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8872 - acc: 0.6031\n",
            "Epoch 794/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8873 - acc: 0.6031\n",
            "Epoch 795/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8870 - acc: 0.6031\n",
            "Epoch 796/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8870 - acc: 0.6031\n",
            "Epoch 797/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8869 - acc: 0.6031\n",
            "Epoch 798/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8868 - acc: 0.6031\n",
            "Epoch 799/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8867 - acc: 0.6031\n",
            "Epoch 800/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 801/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 802/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 803/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8865 - acc: 0.6031\n",
            "Epoch 804/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 805/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 806/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 807/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8862 - acc: 0.6031\n",
            "Epoch 808/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8861 - acc: 0.6031\n",
            "Epoch 809/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.8859 - acc: 0.5954\n",
            "Epoch 810/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8859 - acc: 0.6031\n",
            "Epoch 811/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8858 - acc: 0.6031\n",
            "Epoch 812/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8858 - acc: 0.5954\n",
            "Epoch 813/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.8856 - acc: 0.6031\n",
            "Epoch 814/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8855 - acc: 0.6031\n",
            "Epoch 815/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8854 - acc: 0.6031\n",
            "Epoch 816/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8855 - acc: 0.6031\n",
            "Epoch 817/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8853 - acc: 0.6031\n",
            "Epoch 818/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8852 - acc: 0.6031\n",
            "Epoch 819/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8851 - acc: 0.6031\n",
            "Epoch 820/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8851 - acc: 0.6031\n",
            "Epoch 821/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8850 - acc: 0.6031\n",
            "Epoch 822/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8849 - acc: 0.6031\n",
            "Epoch 823/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8847 - acc: 0.6031\n",
            "Epoch 824/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8848 - acc: 0.6031\n",
            "Epoch 825/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8847 - acc: 0.6031\n",
            "Epoch 826/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8845 - acc: 0.6031\n",
            "Epoch 827/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8845 - acc: 0.6031\n",
            "Epoch 828/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8844 - acc: 0.6031\n",
            "Epoch 829/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8843 - acc: 0.6031\n",
            "Epoch 830/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 831/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 832/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8840 - acc: 0.6031\n",
            "Epoch 833/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 834/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8839 - acc: 0.6031\n",
            "Epoch 835/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8840 - acc: 0.6031\n",
            "Epoch 836/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8838 - acc: 0.6031\n",
            "Epoch 837/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8837 - acc: 0.6031\n",
            "Epoch 838/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 839/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 840/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 841/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 842/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 843/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 844/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8832 - acc: 0.6031\n",
            "Epoch 845/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8832 - acc: 0.6031\n",
            "Epoch 846/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8831 - acc: 0.6031\n",
            "Epoch 847/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 848/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 849/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 850/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8827 - acc: 0.6031\n",
            "Epoch 851/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8828 - acc: 0.6031\n",
            "Epoch 852/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 853/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 854/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 855/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8824 - acc: 0.6031\n",
            "Epoch 856/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8823 - acc: 0.6031\n",
            "Epoch 857/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8823 - acc: 0.6031\n",
            "Epoch 858/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8822 - acc: 0.6031\n",
            "Epoch 859/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8822 - acc: 0.6031\n",
            "Epoch 860/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8820 - acc: 0.6031\n",
            "Epoch 861/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 862/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 863/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8818 - acc: 0.6031\n",
            "Epoch 864/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8818 - acc: 0.6031\n",
            "Epoch 865/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 866/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8817 - acc: 0.6031\n",
            "Epoch 867/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8816 - acc: 0.6031\n",
            "Epoch 868/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8816 - acc: 0.6031\n",
            "Epoch 869/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8814 - acc: 0.6031\n",
            "Epoch 870/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8814 - acc: 0.6031\n",
            "Epoch 871/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 872/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 873/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 874/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8811 - acc: 0.6031\n",
            "Epoch 875/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8810 - acc: 0.6031\n",
            "Epoch 876/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 877/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 878/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8808 - acc: 0.6031\n",
            "Epoch 879/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 880/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8805 - acc: 0.6031\n",
            "Epoch 881/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8806 - acc: 0.6031\n",
            "Epoch 882/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8806 - acc: 0.6031\n",
            "Epoch 883/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8804 - acc: 0.6031\n",
            "Epoch 884/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8803 - acc: 0.6031\n",
            "Epoch 885/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8803 - acc: 0.6031\n",
            "Epoch 886/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8802 - acc: 0.6031\n",
            "Epoch 887/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8802 - acc: 0.6031\n",
            "Epoch 888/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 889/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8801 - acc: 0.6031\n",
            "Epoch 890/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8799 - acc: 0.6031\n",
            "Epoch 891/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 892/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 893/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8797 - acc: 0.6031\n",
            "Epoch 894/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8796 - acc: 0.6031\n",
            "Epoch 895/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.8795 - acc: 0.6031\n",
            "Epoch 896/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8795 - acc: 0.6031\n",
            "Epoch 897/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8794 - acc: 0.6031\n",
            "Epoch 898/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8794 - acc: 0.6031\n",
            "Epoch 899/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8793 - acc: 0.6031\n",
            "Epoch 900/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 901/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8793 - acc: 0.6031\n",
            "Epoch 902/1000\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 903/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 904/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8789 - acc: 0.6031\n",
            "Epoch 905/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8789 - acc: 0.6031\n",
            "Epoch 906/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8790 - acc: 0.6031\n",
            "Epoch 907/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 908/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 909/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 910/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.8785 - acc: 0.6031\n",
            "Epoch 911/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8784 - acc: 0.6031\n",
            "Epoch 912/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8783 - acc: 0.6031\n",
            "Epoch 913/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8782 - acc: 0.6031\n",
            "Epoch 914/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8781 - acc: 0.6031\n",
            "Epoch 915/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8781 - acc: 0.6031\n",
            "Epoch 916/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8780 - acc: 0.6031\n",
            "Epoch 917/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8780 - acc: 0.6031\n",
            "Epoch 918/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8779 - acc: 0.6031\n",
            "Epoch 919/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 920/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 921/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 922/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8776 - acc: 0.6031\n",
            "Epoch 923/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8776 - acc: 0.6031\n",
            "Epoch 924/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8776 - acc: 0.5954\n",
            "Epoch 925/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8775 - acc: 0.5954\n",
            "Epoch 926/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8774 - acc: 0.6031\n",
            "Epoch 927/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8773 - acc: 0.6031\n",
            "Epoch 928/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8772 - acc: 0.6031\n",
            "Epoch 929/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8771 - acc: 0.5954\n",
            "Epoch 930/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8770 - acc: 0.6031\n",
            "Epoch 931/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8769 - acc: 0.6031\n",
            "Epoch 932/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8769 - acc: 0.5954\n",
            "Epoch 933/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 934/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 935/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 936/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8767 - acc: 0.5954\n",
            "Epoch 937/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 938/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 939/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 940/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8763 - acc: 0.6031\n",
            "Epoch 941/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8763 - acc: 0.6031\n",
            "Epoch 942/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8762 - acc: 0.6031\n",
            "Epoch 943/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 944/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8761 - acc: 0.6031\n",
            "Epoch 945/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 946/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 947/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 948/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 949/1000\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.8758 - acc: 0.6031\n",
            "Epoch 950/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8759 - acc: 0.6031\n",
            "Epoch 951/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8755 - acc: 0.6031\n",
            "Epoch 952/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8756 - acc: 0.5954\n",
            "Epoch 953/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8754 - acc: 0.6031\n",
            "Epoch 954/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8753 - acc: 0.6031\n",
            "Epoch 955/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8754 - acc: 0.5954\n",
            "Epoch 956/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8753 - acc: 0.5954\n",
            "Epoch 957/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8754 - acc: 0.5954\n",
            "Epoch 958/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8752 - acc: 0.5954\n",
            "Epoch 959/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8751 - acc: 0.5954\n",
            "Epoch 960/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8750 - acc: 0.5954\n",
            "Epoch 961/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8750 - acc: 0.5954\n",
            "Epoch 962/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8749 - acc: 0.5954\n",
            "Epoch 963/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8749 - acc: 0.5954\n",
            "Epoch 964/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8747 - acc: 0.5954\n",
            "Epoch 965/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 966/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 967/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 968/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8745 - acc: 0.5954\n",
            "Epoch 969/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8744 - acc: 0.5954\n",
            "Epoch 970/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8742 - acc: 0.5954\n",
            "Epoch 971/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8744 - acc: 0.5954\n",
            "Epoch 972/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8743 - acc: 0.5954\n",
            "Epoch 973/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8742 - acc: 0.5954\n",
            "Epoch 974/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8741 - acc: 0.5954\n",
            "Epoch 975/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8740 - acc: 0.5954\n",
            "Epoch 976/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8740 - acc: 0.5954\n",
            "Epoch 977/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8741 - acc: 0.5954\n",
            "Epoch 978/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8739 - acc: 0.5954\n",
            "Epoch 979/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8739 - acc: 0.5954\n",
            "Epoch 980/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8738 - acc: 0.5954\n",
            "Epoch 981/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8737 - acc: 0.5954\n",
            "Epoch 982/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8737 - acc: 0.5954\n",
            "Epoch 983/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8736 - acc: 0.5954\n",
            "Epoch 984/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8735 - acc: 0.5954\n",
            "Epoch 985/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8734 - acc: 0.5954\n",
            "Epoch 986/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8734 - acc: 0.5954\n",
            "Epoch 987/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 988/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 989/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8731 - acc: 0.5954\n",
            "Epoch 990/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8732 - acc: 0.5954\n",
            "Epoch 991/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 992/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 993/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 994/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 995/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8729 - acc: 0.5954\n",
            "Epoch 996/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8728 - acc: 0.5954\n",
            "Epoch 997/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8728 - acc: 0.5954\n",
            "Epoch 998/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8727 - acc: 0.5954\n",
            "Epoch 999/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8726 - acc: 0.5954\n",
            "Epoch 1000/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.8726 - acc: 0.5954\n",
            "34/34 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "bfcdc956-e7d5-4402-ae3d-eaa3c774f1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "7054a667-3ec5-44b5-a16b-f12c892bba8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20588235294117646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "37ba438f-cae2-4838-f0e7-ed92399da4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "594298d5-60c0-4a23-881d-74e926814940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JAOqZLBxkXFs"
      },
      "source": [
        "##Z score dei dati dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DisVOwPBkXF8",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6vzVp7KkXGU",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "935b5b18-9aae-4692-f9b2-146a4dfeb1c0",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "694b93bf-ad7b-41e9-b6cf-bda980ceca36",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "81fcc46a-8d38-4f93-9983-72fbde63c8ca",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPe2TlzgfFVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7a56d89f-252b-4214-f72a-51283b50c7a1",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/500\n",
            "87/87 [==============================] - 1s 6ms/step - loss: 1.4384 - acc: 0.5172 - val_loss: 1.3342 - val_acc: 0.5000\n",
            "Epoch 2/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 1.4092 - acc: 0.5172 - val_loss: 1.3066 - val_acc: 0.5000\n",
            "Epoch 3/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 1.3804 - acc: 0.5172 - val_loss: 1.2809 - val_acc: 0.5000\n",
            "Epoch 4/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 1.3529 - acc: 0.5172 - val_loss: 1.2567 - val_acc: 0.5000\n",
            "Epoch 5/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 1.3268 - acc: 0.5172 - val_loss: 1.2325 - val_acc: 0.5000\n",
            "Epoch 6/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 1.3011 - acc: 0.5287 - val_loss: 1.2109 - val_acc: 0.5000\n",
            "Epoch 7/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 1.2766 - acc: 0.5287 - val_loss: 1.1892 - val_acc: 0.5000\n",
            "Epoch 8/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 1.2531 - acc: 0.5287 - val_loss: 1.1678 - val_acc: 0.5000\n",
            "Epoch 9/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.2303 - acc: 0.5287 - val_loss: 1.1481 - val_acc: 0.5227\n",
            "Epoch 10/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 1.2087 - acc: 0.5287 - val_loss: 1.1286 - val_acc: 0.5227\n",
            "Epoch 11/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 1.1878 - acc: 0.5287 - val_loss: 1.1098 - val_acc: 0.5227\n",
            "Epoch 12/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 1.1676 - acc: 0.5287 - val_loss: 1.0926 - val_acc: 0.5227\n",
            "Epoch 13/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 1.1485 - acc: 0.5287 - val_loss: 1.0750 - val_acc: 0.5227\n",
            "Epoch 14/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 1.1294 - acc: 0.5287 - val_loss: 1.0587 - val_acc: 0.5227\n",
            "Epoch 15/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 1.1113 - acc: 0.5287 - val_loss: 1.0429 - val_acc: 0.5227\n",
            "Epoch 16/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 1.0938 - acc: 0.5287 - val_loss: 1.0270 - val_acc: 0.5227\n",
            "Epoch 17/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 1.0770 - acc: 0.5287 - val_loss: 1.0123 - val_acc: 0.5227\n",
            "Epoch 18/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 1.0610 - acc: 0.5402 - val_loss: 0.9974 - val_acc: 0.5227\n",
            "Epoch 19/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 1.0453 - acc: 0.5517 - val_loss: 0.9834 - val_acc: 0.5455\n",
            "Epoch 20/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.0303 - acc: 0.5517 - val_loss: 0.9702 - val_acc: 0.5455\n",
            "Epoch 21/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 1.0157 - acc: 0.5517 - val_loss: 0.9575 - val_acc: 0.5682\n",
            "Epoch 22/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 1.0020 - acc: 0.5517 - val_loss: 0.9443 - val_acc: 0.5682\n",
            "Epoch 23/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.9880 - acc: 0.5517 - val_loss: 0.9330 - val_acc: 0.5682\n",
            "Epoch 24/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.9750 - acc: 0.5517 - val_loss: 0.9211 - val_acc: 0.5682\n",
            "Epoch 25/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9622 - acc: 0.5517 - val_loss: 0.9098 - val_acc: 0.5682\n",
            "Epoch 26/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9498 - acc: 0.5517 - val_loss: 0.8989 - val_acc: 0.5909\n",
            "Epoch 27/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9380 - acc: 0.5517 - val_loss: 0.8881 - val_acc: 0.5909\n",
            "Epoch 28/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9267 - acc: 0.5517 - val_loss: 0.8777 - val_acc: 0.5909\n",
            "Epoch 29/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9154 - acc: 0.5517 - val_loss: 0.8681 - val_acc: 0.5909\n",
            "Epoch 30/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.9046 - acc: 0.5517 - val_loss: 0.8585 - val_acc: 0.5909\n",
            "Epoch 31/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8941 - acc: 0.5517 - val_loss: 0.8488 - val_acc: 0.5909\n",
            "Epoch 32/500\n",
            "87/87 [==============================] - 0s 304us/step - loss: 0.8838 - acc: 0.5517 - val_loss: 0.8399 - val_acc: 0.6136\n",
            "Epoch 33/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8739 - acc: 0.5632 - val_loss: 0.8309 - val_acc: 0.6136\n",
            "Epoch 34/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8642 - acc: 0.5632 - val_loss: 0.8223 - val_acc: 0.6364\n",
            "Epoch 35/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.8550 - acc: 0.5747 - val_loss: 0.8139 - val_acc: 0.6364\n",
            "Epoch 36/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8461 - acc: 0.5747 - val_loss: 0.8062 - val_acc: 0.6364\n",
            "Epoch 37/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8375 - acc: 0.5747 - val_loss: 0.7985 - val_acc: 0.6364\n",
            "Epoch 38/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8292 - acc: 0.5747 - val_loss: 0.7909 - val_acc: 0.6364\n",
            "Epoch 39/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.8212 - acc: 0.5747 - val_loss: 0.7840 - val_acc: 0.6364\n",
            "Epoch 40/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8136 - acc: 0.5862 - val_loss: 0.7771 - val_acc: 0.6364\n",
            "Epoch 41/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.8058 - acc: 0.6092 - val_loss: 0.7704 - val_acc: 0.6591\n",
            "Epoch 42/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.7985 - acc: 0.6092 - val_loss: 0.7640 - val_acc: 0.6591\n",
            "Epoch 43/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7915 - acc: 0.6092 - val_loss: 0.7579 - val_acc: 0.6818\n",
            "Epoch 44/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7848 - acc: 0.6092 - val_loss: 0.7517 - val_acc: 0.7045\n",
            "Epoch 45/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.7781 - acc: 0.6092 - val_loss: 0.7458 - val_acc: 0.7273\n",
            "Epoch 46/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7715 - acc: 0.6207 - val_loss: 0.7401 - val_acc: 0.7273\n",
            "Epoch 47/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.7652 - acc: 0.6207 - val_loss: 0.7345 - val_acc: 0.7500\n",
            "Epoch 48/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7590 - acc: 0.6322 - val_loss: 0.7287 - val_acc: 0.7500\n",
            "Epoch 49/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7528 - acc: 0.6437 - val_loss: 0.7233 - val_acc: 0.7500\n",
            "Epoch 50/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.7468 - acc: 0.6437 - val_loss: 0.7181 - val_acc: 0.7955\n",
            "Epoch 51/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.7411 - acc: 0.6552 - val_loss: 0.7127 - val_acc: 0.7955\n",
            "Epoch 52/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.7353 - acc: 0.6782 - val_loss: 0.7077 - val_acc: 0.8182\n",
            "Epoch 53/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.7298 - acc: 0.6897 - val_loss: 0.7026 - val_acc: 0.8182\n",
            "Epoch 54/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.7244 - acc: 0.7241 - val_loss: 0.6975 - val_acc: 0.8409\n",
            "Epoch 55/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.7189 - acc: 0.7471 - val_loss: 0.6929 - val_acc: 0.8409\n",
            "Epoch 56/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.7137 - acc: 0.7586 - val_loss: 0.6881 - val_acc: 0.8409\n",
            "Epoch 57/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.7085 - acc: 0.7586 - val_loss: 0.6834 - val_acc: 0.8409\n",
            "Epoch 58/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7034 - acc: 0.7701 - val_loss: 0.6790 - val_acc: 0.8409\n",
            "Epoch 59/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.6985 - acc: 0.7816 - val_loss: 0.6745 - val_acc: 0.8409\n",
            "Epoch 60/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6936 - acc: 0.7701 - val_loss: 0.6703 - val_acc: 0.8409\n",
            "Epoch 61/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6889 - acc: 0.8046 - val_loss: 0.6661 - val_acc: 0.8636\n",
            "Epoch 62/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.6842 - acc: 0.8161 - val_loss: 0.6619 - val_acc: 0.8636\n",
            "Epoch 63/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.6797 - acc: 0.8276 - val_loss: 0.6579 - val_acc: 0.8636\n",
            "Epoch 64/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.6752 - acc: 0.8391 - val_loss: 0.6540 - val_acc: 0.8636\n",
            "Epoch 65/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.6708 - acc: 0.8506 - val_loss: 0.6500 - val_acc: 0.8636\n",
            "Epoch 66/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6665 - acc: 0.8506 - val_loss: 0.6461 - val_acc: 0.8636\n",
            "Epoch 67/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.6623 - acc: 0.8506 - val_loss: 0.6424 - val_acc: 0.8636\n",
            "Epoch 68/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.6582 - acc: 0.8506 - val_loss: 0.6385 - val_acc: 0.8864\n",
            "Epoch 69/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.6541 - acc: 0.8506 - val_loss: 0.6349 - val_acc: 0.9091\n",
            "Epoch 70/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6502 - acc: 0.8506 - val_loss: 0.6313 - val_acc: 0.9545\n",
            "Epoch 71/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.6462 - acc: 0.8506 - val_loss: 0.6277 - val_acc: 0.9545\n",
            "Epoch 72/500\n",
            "87/87 [==============================] - 0s 191us/step - loss: 0.6423 - acc: 0.8506 - val_loss: 0.6242 - val_acc: 0.9773\n",
            "Epoch 73/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6386 - acc: 0.8506 - val_loss: 0.6206 - val_acc: 0.9773\n",
            "Epoch 74/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6348 - acc: 0.8621 - val_loss: 0.6172 - val_acc: 0.9773\n",
            "Epoch 75/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.6311 - acc: 0.8621 - val_loss: 0.6138 - val_acc: 0.9773\n",
            "Epoch 76/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.6274 - acc: 0.8851 - val_loss: 0.6105 - val_acc: 0.9773\n",
            "Epoch 77/500\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.6238 - acc: 0.8851 - val_loss: 0.6072 - val_acc: 0.9773\n",
            "Epoch 78/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.6203 - acc: 0.8851 - val_loss: 0.6040 - val_acc: 0.9773\n",
            "Epoch 79/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.6169 - acc: 0.8966 - val_loss: 0.6007 - val_acc: 0.9773\n",
            "Epoch 80/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.6135 - acc: 0.9195 - val_loss: 0.5976 - val_acc: 0.9773\n",
            "Epoch 81/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.6102 - acc: 0.9195 - val_loss: 0.5944 - val_acc: 0.9773\n",
            "Epoch 82/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6069 - acc: 0.9310 - val_loss: 0.5913 - val_acc: 0.9773\n",
            "Epoch 83/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.6035 - acc: 0.9310 - val_loss: 0.5883 - val_acc: 0.9773\n",
            "Epoch 84/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6003 - acc: 0.9310 - val_loss: 0.5853 - val_acc: 0.9773\n",
            "Epoch 85/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5971 - acc: 0.9310 - val_loss: 0.5823 - val_acc: 0.9773\n",
            "Epoch 86/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.5940 - acc: 0.9310 - val_loss: 0.5792 - val_acc: 0.9773\n",
            "Epoch 87/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.5909 - acc: 0.9310 - val_loss: 0.5763 - val_acc: 0.9773\n",
            "Epoch 88/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5879 - acc: 0.9310 - val_loss: 0.5734 - val_acc: 0.9773\n",
            "Epoch 89/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.5848 - acc: 0.9425 - val_loss: 0.5705 - val_acc: 0.9773\n",
            "Epoch 90/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5818 - acc: 0.9425 - val_loss: 0.5676 - val_acc: 0.9773\n",
            "Epoch 91/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.5788 - acc: 0.9425 - val_loss: 0.5648 - val_acc: 0.9773\n",
            "Epoch 92/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5759 - acc: 0.9425 - val_loss: 0.5620 - val_acc: 0.9773\n",
            "Epoch 93/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5730 - acc: 0.9540 - val_loss: 0.5592 - val_acc: 0.9773\n",
            "Epoch 94/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5701 - acc: 0.9540 - val_loss: 0.5565 - val_acc: 0.9773\n",
            "Epoch 95/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5673 - acc: 0.9540 - val_loss: 0.5537 - val_acc: 0.9773\n",
            "Epoch 96/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.5644 - acc: 0.9540 - val_loss: 0.5510 - val_acc: 0.9773\n",
            "Epoch 97/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5615 - acc: 0.9540 - val_loss: 0.5483 - val_acc: 0.9773\n",
            "Epoch 98/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.5587 - acc: 0.9540 - val_loss: 0.5456 - val_acc: 1.0000\n",
            "Epoch 99/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5560 - acc: 0.9540 - val_loss: 0.5430 - val_acc: 1.0000\n",
            "Epoch 100/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5533 - acc: 0.9540 - val_loss: 0.5403 - val_acc: 1.0000\n",
            "Epoch 101/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5506 - acc: 0.9540 - val_loss: 0.5377 - val_acc: 1.0000\n",
            "Epoch 102/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.5479 - acc: 0.9540 - val_loss: 0.5351 - val_acc: 1.0000\n",
            "Epoch 103/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.5452 - acc: 0.9540 - val_loss: 0.5326 - val_acc: 1.0000\n",
            "Epoch 104/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5426 - acc: 0.9540 - val_loss: 0.5301 - val_acc: 1.0000\n",
            "Epoch 105/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.5399 - acc: 0.9540 - val_loss: 0.5277 - val_acc: 1.0000\n",
            "Epoch 106/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5373 - acc: 0.9540 - val_loss: 0.5252 - val_acc: 1.0000\n",
            "Epoch 107/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5348 - acc: 0.9540 - val_loss: 0.5228 - val_acc: 1.0000\n",
            "Epoch 108/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.5322 - acc: 0.9540 - val_loss: 0.5204 - val_acc: 1.0000\n",
            "Epoch 109/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5297 - acc: 0.9540 - val_loss: 0.5180 - val_acc: 1.0000\n",
            "Epoch 110/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.5272 - acc: 0.9540 - val_loss: 0.5157 - val_acc: 1.0000\n",
            "Epoch 111/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.5247 - acc: 0.9540 - val_loss: 0.5133 - val_acc: 1.0000\n",
            "Epoch 112/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5222 - acc: 0.9540 - val_loss: 0.5110 - val_acc: 1.0000\n",
            "Epoch 113/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.5198 - acc: 0.9540 - val_loss: 0.5088 - val_acc: 1.0000\n",
            "Epoch 114/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.5174 - acc: 0.9655 - val_loss: 0.5065 - val_acc: 1.0000\n",
            "Epoch 115/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.5151 - acc: 0.9655 - val_loss: 0.5043 - val_acc: 1.0000\n",
            "Epoch 116/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5128 - acc: 0.9655 - val_loss: 0.5020 - val_acc: 1.0000\n",
            "Epoch 117/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5104 - acc: 0.9655 - val_loss: 0.4999 - val_acc: 1.0000\n",
            "Epoch 118/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5081 - acc: 0.9655 - val_loss: 0.4977 - val_acc: 1.0000\n",
            "Epoch 119/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5058 - acc: 0.9655 - val_loss: 0.4955 - val_acc: 1.0000\n",
            "Epoch 120/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5035 - acc: 0.9655 - val_loss: 0.4934 - val_acc: 1.0000\n",
            "Epoch 121/500\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.5012 - acc: 0.9655 - val_loss: 0.4913 - val_acc: 1.0000\n",
            "Epoch 122/500\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.4990 - acc: 0.9655 - val_loss: 0.4892 - val_acc: 1.0000\n",
            "Epoch 123/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.4967 - acc: 0.9655 - val_loss: 0.4870 - val_acc: 1.0000\n",
            "Epoch 124/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4945 - acc: 0.9655 - val_loss: 0.4850 - val_acc: 1.0000\n",
            "Epoch 125/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4923 - acc: 0.9770 - val_loss: 0.4829 - val_acc: 1.0000\n",
            "Epoch 126/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.4903 - acc: 0.9770 - val_loss: 0.4809 - val_acc: 1.0000\n",
            "Epoch 127/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.4881 - acc: 0.9770 - val_loss: 0.4788 - val_acc: 1.0000\n",
            "Epoch 128/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.4859 - acc: 0.9770 - val_loss: 0.4768 - val_acc: 1.0000\n",
            "Epoch 129/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4838 - acc: 0.9770 - val_loss: 0.4747 - val_acc: 1.0000\n",
            "Epoch 130/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.4817 - acc: 0.9770 - val_loss: 0.4728 - val_acc: 1.0000\n",
            "Epoch 131/500\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.4797 - acc: 0.9770 - val_loss: 0.4708 - val_acc: 1.0000\n",
            "Epoch 132/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4777 - acc: 0.9770 - val_loss: 0.4688 - val_acc: 1.0000\n",
            "Epoch 133/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4757 - acc: 0.9770 - val_loss: 0.4669 - val_acc: 1.0000\n",
            "Epoch 134/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.4737 - acc: 0.9770 - val_loss: 0.4649 - val_acc: 1.0000\n",
            "Epoch 135/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.4718 - acc: 0.9770 - val_loss: 0.4630 - val_acc: 1.0000\n",
            "Epoch 136/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4698 - acc: 0.9770 - val_loss: 0.4611 - val_acc: 1.0000\n",
            "Epoch 137/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4679 - acc: 0.9770 - val_loss: 0.4592 - val_acc: 1.0000\n",
            "Epoch 138/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.4660 - acc: 0.9770 - val_loss: 0.4574 - val_acc: 1.0000\n",
            "Epoch 139/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4641 - acc: 0.9770 - val_loss: 0.4554 - val_acc: 1.0000\n",
            "Epoch 140/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4623 - acc: 0.9770 - val_loss: 0.4536 - val_acc: 1.0000\n",
            "Epoch 141/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4604 - acc: 0.9770 - val_loss: 0.4518 - val_acc: 1.0000\n",
            "Epoch 142/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4586 - acc: 0.9770 - val_loss: 0.4500 - val_acc: 1.0000\n",
            "Epoch 143/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4568 - acc: 0.9770 - val_loss: 0.4482 - val_acc: 1.0000\n",
            "Epoch 144/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4550 - acc: 0.9770 - val_loss: 0.4464 - val_acc: 1.0000\n",
            "Epoch 145/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.4532 - acc: 0.9770 - val_loss: 0.4445 - val_acc: 1.0000\n",
            "Epoch 146/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4514 - acc: 0.9885 - val_loss: 0.4427 - val_acc: 1.0000\n",
            "Epoch 147/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4497 - acc: 0.9885 - val_loss: 0.4409 - val_acc: 1.0000\n",
            "Epoch 148/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4480 - acc: 0.9885 - val_loss: 0.4392 - val_acc: 1.0000\n",
            "Epoch 149/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4462 - acc: 0.9885 - val_loss: 0.4374 - val_acc: 1.0000\n",
            "Epoch 150/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4445 - acc: 0.9885 - val_loss: 0.4357 - val_acc: 1.0000\n",
            "Epoch 151/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4428 - acc: 0.9885 - val_loss: 0.4339 - val_acc: 1.0000\n",
            "Epoch 152/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4411 - acc: 0.9885 - val_loss: 0.4322 - val_acc: 1.0000\n",
            "Epoch 153/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4394 - acc: 0.9885 - val_loss: 0.4305 - val_acc: 1.0000\n",
            "Epoch 154/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4378 - acc: 0.9885 - val_loss: 0.4288 - val_acc: 1.0000\n",
            "Epoch 155/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4361 - acc: 0.9885 - val_loss: 0.4271 - val_acc: 1.0000\n",
            "Epoch 156/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4344 - acc: 0.9885 - val_loss: 0.4255 - val_acc: 1.0000\n",
            "Epoch 157/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4328 - acc: 0.9885 - val_loss: 0.4237 - val_acc: 1.0000\n",
            "Epoch 158/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4313 - acc: 0.9885 - val_loss: 0.4221 - val_acc: 1.0000\n",
            "Epoch 159/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4296 - acc: 0.9885 - val_loss: 0.4204 - val_acc: 1.0000\n",
            "Epoch 160/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4279 - acc: 0.9885 - val_loss: 0.4187 - val_acc: 1.0000\n",
            "Epoch 161/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4263 - acc: 0.9885 - val_loss: 0.4171 - val_acc: 1.0000\n",
            "Epoch 162/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4247 - acc: 0.9885 - val_loss: 0.4155 - val_acc: 1.0000\n",
            "Epoch 163/500\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.4231 - acc: 0.9885 - val_loss: 0.4138 - val_acc: 1.0000\n",
            "Epoch 164/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4215 - acc: 0.9885 - val_loss: 0.4122 - val_acc: 1.0000\n",
            "Epoch 165/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4199 - acc: 0.9885 - val_loss: 0.4106 - val_acc: 1.0000\n",
            "Epoch 166/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4184 - acc: 0.9885 - val_loss: 0.4090 - val_acc: 1.0000\n",
            "Epoch 167/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4168 - acc: 0.9885 - val_loss: 0.4074 - val_acc: 1.0000\n",
            "Epoch 168/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4152 - acc: 0.9885 - val_loss: 0.4058 - val_acc: 1.0000\n",
            "Epoch 169/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4137 - acc: 0.9885 - val_loss: 0.4042 - val_acc: 1.0000\n",
            "Epoch 170/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.4122 - acc: 0.9885 - val_loss: 0.4027 - val_acc: 0.9773\n",
            "Epoch 171/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.4106 - acc: 0.9885 - val_loss: 0.4011 - val_acc: 0.9773\n",
            "Epoch 172/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4091 - acc: 0.9885 - val_loss: 0.3995 - val_acc: 0.9773\n",
            "Epoch 173/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.4076 - acc: 0.9885 - val_loss: 0.3980 - val_acc: 0.9773\n",
            "Epoch 174/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4061 - acc: 0.9885 - val_loss: 0.3964 - val_acc: 0.9773\n",
            "Epoch 175/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4046 - acc: 0.9885 - val_loss: 0.3949 - val_acc: 0.9773\n",
            "Epoch 176/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4032 - acc: 0.9885 - val_loss: 0.3933 - val_acc: 0.9773\n",
            "Epoch 177/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4017 - acc: 0.9885 - val_loss: 0.3918 - val_acc: 0.9773\n",
            "Epoch 178/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4002 - acc: 0.9885 - val_loss: 0.3903 - val_acc: 0.9773\n",
            "Epoch 179/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3987 - acc: 0.9885 - val_loss: 0.3888 - val_acc: 0.9773\n",
            "Epoch 180/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.3973 - acc: 0.9885 - val_loss: 0.3873 - val_acc: 0.9773\n",
            "Epoch 181/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3959 - acc: 0.9885 - val_loss: 0.3858 - val_acc: 0.9773\n",
            "Epoch 182/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3944 - acc: 0.9885 - val_loss: 0.3843 - val_acc: 0.9773\n",
            "Epoch 183/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3930 - acc: 0.9885 - val_loss: 0.3829 - val_acc: 0.9773\n",
            "Epoch 184/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.3916 - acc: 0.9885 - val_loss: 0.3814 - val_acc: 0.9773\n",
            "Epoch 185/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.3902 - acc: 0.9885 - val_loss: 0.3800 - val_acc: 0.9773\n",
            "Epoch 186/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3888 - acc: 0.9885 - val_loss: 0.3784 - val_acc: 0.9773\n",
            "Epoch 187/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3873 - acc: 0.9885 - val_loss: 0.3770 - val_acc: 0.9773\n",
            "Epoch 188/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.3860 - acc: 0.9885 - val_loss: 0.3756 - val_acc: 0.9773\n",
            "Epoch 189/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.3846 - acc: 0.9885 - val_loss: 0.3741 - val_acc: 0.9773\n",
            "Epoch 190/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3832 - acc: 0.9885 - val_loss: 0.3727 - val_acc: 0.9773\n",
            "Epoch 191/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.3818 - acc: 0.9885 - val_loss: 0.3713 - val_acc: 0.9773\n",
            "Epoch 192/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.3804 - acc: 0.9885 - val_loss: 0.3699 - val_acc: 0.9773\n",
            "Epoch 193/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3791 - acc: 0.9885 - val_loss: 0.3685 - val_acc: 0.9773\n",
            "Epoch 194/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.3777 - acc: 0.9885 - val_loss: 0.3671 - val_acc: 0.9773\n",
            "Epoch 195/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3764 - acc: 0.9885 - val_loss: 0.3657 - val_acc: 0.9773\n",
            "Epoch 196/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3750 - acc: 0.9885 - val_loss: 0.3643 - val_acc: 0.9773\n",
            "Epoch 197/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3737 - acc: 0.9885 - val_loss: 0.3629 - val_acc: 0.9773\n",
            "Epoch 198/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3724 - acc: 0.9885 - val_loss: 0.3616 - val_acc: 0.9773\n",
            "Epoch 199/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3710 - acc: 0.9885 - val_loss: 0.3602 - val_acc: 0.9773\n",
            "Epoch 200/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3697 - acc: 0.9885 - val_loss: 0.3589 - val_acc: 0.9773\n",
            "Epoch 201/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3685 - acc: 0.9885 - val_loss: 0.3575 - val_acc: 0.9773\n",
            "Epoch 202/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.3671 - acc: 0.9885 - val_loss: 0.3562 - val_acc: 0.9773\n",
            "Epoch 203/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3658 - acc: 1.0000 - val_loss: 0.3549 - val_acc: 0.9773\n",
            "Epoch 204/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3645 - acc: 1.0000 - val_loss: 0.3536 - val_acc: 0.9773\n",
            "Epoch 205/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3633 - acc: 1.0000 - val_loss: 0.3522 - val_acc: 0.9773\n",
            "Epoch 206/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3620 - acc: 1.0000 - val_loss: 0.3509 - val_acc: 0.9773\n",
            "Epoch 207/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.3607 - acc: 1.0000 - val_loss: 0.3496 - val_acc: 0.9773\n",
            "Epoch 208/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3594 - acc: 1.0000 - val_loss: 0.3483 - val_acc: 0.9773\n",
            "Epoch 209/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3581 - acc: 1.0000 - val_loss: 0.3470 - val_acc: 0.9773\n",
            "Epoch 210/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3569 - acc: 1.0000 - val_loss: 0.3457 - val_acc: 0.9773\n",
            "Epoch 211/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.3556 - acc: 1.0000 - val_loss: 0.3444 - val_acc: 0.9773\n",
            "Epoch 212/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3544 - acc: 1.0000 - val_loss: 0.3432 - val_acc: 0.9773\n",
            "Epoch 213/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3531 - acc: 1.0000 - val_loss: 0.3419 - val_acc: 0.9773\n",
            "Epoch 214/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3519 - acc: 1.0000 - val_loss: 0.3406 - val_acc: 0.9773\n",
            "Epoch 215/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3507 - acc: 1.0000 - val_loss: 0.3393 - val_acc: 0.9773\n",
            "Epoch 216/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3494 - acc: 1.0000 - val_loss: 0.3381 - val_acc: 0.9773\n",
            "Epoch 217/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3482 - acc: 1.0000 - val_loss: 0.3369 - val_acc: 0.9773\n",
            "Epoch 218/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.3470 - acc: 1.0000 - val_loss: 0.3356 - val_acc: 0.9773\n",
            "Epoch 219/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.3458 - acc: 1.0000 - val_loss: 0.3344 - val_acc: 0.9773\n",
            "Epoch 220/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3446 - acc: 1.0000 - val_loss: 0.3331 - val_acc: 0.9773\n",
            "Epoch 221/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3435 - acc: 1.0000 - val_loss: 0.3319 - val_acc: 0.9773\n",
            "Epoch 222/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.3423 - acc: 1.0000 - val_loss: 0.3307 - val_acc: 0.9773\n",
            "Epoch 223/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.3411 - acc: 1.0000 - val_loss: 0.3295 - val_acc: 0.9773\n",
            "Epoch 224/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3399 - acc: 1.0000 - val_loss: 0.3283 - val_acc: 0.9773\n",
            "Epoch 225/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3387 - acc: 1.0000 - val_loss: 0.3271 - val_acc: 0.9773\n",
            "Epoch 226/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3376 - acc: 1.0000 - val_loss: 0.3259 - val_acc: 0.9773\n",
            "Epoch 227/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3364 - acc: 1.0000 - val_loss: 0.3248 - val_acc: 0.9773\n",
            "Epoch 228/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3352 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 0.9773\n",
            "Epoch 229/500\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.3341 - acc: 1.0000 - val_loss: 0.3224 - val_acc: 0.9773\n",
            "Epoch 230/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3330 - acc: 1.0000 - val_loss: 0.3213 - val_acc: 0.9773\n",
            "Epoch 231/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.3318 - acc: 1.0000 - val_loss: 0.3201 - val_acc: 0.9773\n",
            "Epoch 232/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3307 - acc: 1.0000 - val_loss: 0.3189 - val_acc: 0.9773\n",
            "Epoch 233/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3295 - acc: 1.0000 - val_loss: 0.3178 - val_acc: 0.9773\n",
            "Epoch 234/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.3284 - acc: 1.0000 - val_loss: 0.3167 - val_acc: 0.9773\n",
            "Epoch 235/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3273 - acc: 1.0000 - val_loss: 0.3155 - val_acc: 0.9773\n",
            "Epoch 236/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3262 - acc: 1.0000 - val_loss: 0.3144 - val_acc: 0.9773\n",
            "Epoch 237/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3251 - acc: 1.0000 - val_loss: 0.3133 - val_acc: 0.9773\n",
            "Epoch 238/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3240 - acc: 1.0000 - val_loss: 0.3122 - val_acc: 0.9773\n",
            "Epoch 239/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3229 - acc: 1.0000 - val_loss: 0.3111 - val_acc: 0.9773\n",
            "Epoch 240/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3218 - acc: 1.0000 - val_loss: 0.3099 - val_acc: 0.9773\n",
            "Epoch 241/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3208 - acc: 1.0000 - val_loss: 0.3089 - val_acc: 0.9773\n",
            "Epoch 242/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.3197 - acc: 1.0000 - val_loss: 0.3078 - val_acc: 0.9773\n",
            "Epoch 243/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.3186 - acc: 1.0000 - val_loss: 0.3067 - val_acc: 0.9773\n",
            "Epoch 244/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3175 - acc: 1.0000 - val_loss: 0.3056 - val_acc: 0.9773\n",
            "Epoch 245/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3164 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.9773\n",
            "Epoch 246/500\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.3154 - acc: 1.0000 - val_loss: 0.3034 - val_acc: 0.9773\n",
            "Epoch 247/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3143 - acc: 1.0000 - val_loss: 0.3024 - val_acc: 0.9773\n",
            "Epoch 248/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.3133 - acc: 1.0000 - val_loss: 0.3013 - val_acc: 0.9773\n",
            "Epoch 249/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.3122 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.9773\n",
            "Epoch 250/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.3112 - acc: 1.0000 - val_loss: 0.2992 - val_acc: 0.9773\n",
            "Epoch 251/500\n",
            "87/87 [==============================] - 0s 194us/step - loss: 0.3101 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 0.9773\n",
            "Epoch 252/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.3091 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 0.9773\n",
            "Epoch 253/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.3081 - acc: 1.0000 - val_loss: 0.2961 - val_acc: 0.9773\n",
            "Epoch 254/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3071 - acc: 1.0000 - val_loss: 0.2951 - val_acc: 0.9773\n",
            "Epoch 255/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3060 - acc: 1.0000 - val_loss: 0.2941 - val_acc: 0.9773\n",
            "Epoch 256/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3050 - acc: 1.0000 - val_loss: 0.2931 - val_acc: 0.9773\n",
            "Epoch 257/500\n",
            "87/87 [==============================] - 0s 354us/step - loss: 0.3040 - acc: 1.0000 - val_loss: 0.2921 - val_acc: 0.9773\n",
            "Epoch 258/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3030 - acc: 1.0000 - val_loss: 0.2911 - val_acc: 0.9773\n",
            "Epoch 259/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3020 - acc: 1.0000 - val_loss: 0.2901 - val_acc: 0.9773\n",
            "Epoch 260/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.3011 - acc: 1.0000 - val_loss: 0.2891 - val_acc: 0.9773\n",
            "Epoch 261/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3001 - acc: 1.0000 - val_loss: 0.2881 - val_acc: 0.9773\n",
            "Epoch 262/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.2991 - acc: 1.0000 - val_loss: 0.2871 - val_acc: 0.9773\n",
            "Epoch 263/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.2981 - acc: 1.0000 - val_loss: 0.2862 - val_acc: 0.9773\n",
            "Epoch 264/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.2971 - acc: 1.0000 - val_loss: 0.2852 - val_acc: 0.9773\n",
            "Epoch 265/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2962 - acc: 1.0000 - val_loss: 0.2842 - val_acc: 0.9773\n",
            "Epoch 266/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.2953 - acc: 1.0000 - val_loss: 0.2833 - val_acc: 0.9773\n",
            "Epoch 267/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2943 - acc: 1.0000 - val_loss: 0.2823 - val_acc: 0.9773\n",
            "Epoch 268/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.2933 - acc: 1.0000 - val_loss: 0.2814 - val_acc: 0.9773\n",
            "Epoch 269/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2924 - acc: 1.0000 - val_loss: 0.2804 - val_acc: 0.9773\n",
            "Epoch 270/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2914 - acc: 1.0000 - val_loss: 0.2795 - val_acc: 0.9773\n",
            "Epoch 271/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2905 - acc: 1.0000 - val_loss: 0.2786 - val_acc: 0.9773\n",
            "Epoch 272/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2896 - acc: 1.0000 - val_loss: 0.2777 - val_acc: 0.9773\n",
            "Epoch 273/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2887 - acc: 1.0000 - val_loss: 0.2767 - val_acc: 0.9773\n",
            "Epoch 274/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2877 - acc: 1.0000 - val_loss: 0.2758 - val_acc: 0.9773\n",
            "Epoch 275/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2868 - acc: 1.0000 - val_loss: 0.2749 - val_acc: 0.9773\n",
            "Epoch 276/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2859 - acc: 1.0000 - val_loss: 0.2740 - val_acc: 0.9773\n",
            "Epoch 277/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.2850 - acc: 1.0000 - val_loss: 0.2731 - val_acc: 0.9773\n",
            "Epoch 278/500\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.2841 - acc: 1.0000 - val_loss: 0.2722 - val_acc: 0.9773\n",
            "Epoch 279/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2832 - acc: 1.0000 - val_loss: 0.2713 - val_acc: 0.9773\n",
            "Epoch 280/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2823 - acc: 1.0000 - val_loss: 0.2705 - val_acc: 0.9773\n",
            "Epoch 281/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2814 - acc: 1.0000 - val_loss: 0.2696 - val_acc: 0.9773\n",
            "Epoch 282/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.2805 - acc: 1.0000 - val_loss: 0.2687 - val_acc: 0.9773\n",
            "Epoch 283/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2797 - acc: 1.0000 - val_loss: 0.2678 - val_acc: 0.9773\n",
            "Epoch 284/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2788 - acc: 1.0000 - val_loss: 0.2670 - val_acc: 0.9773\n",
            "Epoch 285/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2779 - acc: 1.0000 - val_loss: 0.2661 - val_acc: 0.9773\n",
            "Epoch 286/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.2771 - acc: 1.0000 - val_loss: 0.2653 - val_acc: 0.9773\n",
            "Epoch 287/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.2762 - acc: 1.0000 - val_loss: 0.2644 - val_acc: 0.9773\n",
            "Epoch 288/500\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.2754 - acc: 1.0000 - val_loss: 0.2635 - val_acc: 0.9773\n",
            "Epoch 289/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2745 - acc: 1.0000 - val_loss: 0.2627 - val_acc: 0.9773\n",
            "Epoch 290/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2737 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 0.9773\n",
            "Epoch 291/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.2728 - acc: 1.0000 - val_loss: 0.2611 - val_acc: 0.9773\n",
            "Epoch 292/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2720 - acc: 1.0000 - val_loss: 0.2602 - val_acc: 0.9773\n",
            "Epoch 293/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2712 - acc: 1.0000 - val_loss: 0.2594 - val_acc: 0.9773\n",
            "Epoch 294/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2703 - acc: 1.0000 - val_loss: 0.2586 - val_acc: 0.9773\n",
            "Epoch 295/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2695 - acc: 1.0000 - val_loss: 0.2578 - val_acc: 0.9773\n",
            "Epoch 296/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2687 - acc: 1.0000 - val_loss: 0.2570 - val_acc: 0.9773\n",
            "Epoch 297/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2679 - acc: 1.0000 - val_loss: 0.2562 - val_acc: 0.9773\n",
            "Epoch 298/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.2671 - acc: 1.0000 - val_loss: 0.2554 - val_acc: 0.9773\n",
            "Epoch 299/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.2662 - acc: 1.0000 - val_loss: 0.2546 - val_acc: 0.9773\n",
            "Epoch 300/500\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.2654 - acc: 1.0000 - val_loss: 0.2538 - val_acc: 0.9773\n",
            "Epoch 301/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.2646 - acc: 1.0000 - val_loss: 0.2530 - val_acc: 0.9773\n",
            "Epoch 302/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.2638 - acc: 1.0000 - val_loss: 0.2522 - val_acc: 0.9773\n",
            "Epoch 303/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2630 - acc: 1.0000 - val_loss: 0.2515 - val_acc: 0.9773\n",
            "Epoch 304/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.2622 - acc: 1.0000 - val_loss: 0.2507 - val_acc: 0.9773\n",
            "Epoch 305/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2615 - acc: 1.0000 - val_loss: 0.2499 - val_acc: 0.9773\n",
            "Epoch 306/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2607 - acc: 1.0000 - val_loss: 0.2492 - val_acc: 0.9773\n",
            "Epoch 307/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.2599 - acc: 1.0000 - val_loss: 0.2484 - val_acc: 0.9773\n",
            "Epoch 308/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.2591 - acc: 1.0000 - val_loss: 0.2476 - val_acc: 0.9773\n",
            "Epoch 309/500\n",
            "87/87 [==============================] - 0s 311us/step - loss: 0.2584 - acc: 1.0000 - val_loss: 0.2469 - val_acc: 0.9773\n",
            "Epoch 310/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2576 - acc: 1.0000 - val_loss: 0.2461 - val_acc: 0.9773\n",
            "Epoch 311/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2568 - acc: 1.0000 - val_loss: 0.2454 - val_acc: 0.9773\n",
            "Epoch 312/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.2560 - acc: 1.0000 - val_loss: 0.2447 - val_acc: 0.9773\n",
            "Epoch 313/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2553 - acc: 1.0000 - val_loss: 0.2439 - val_acc: 0.9773\n",
            "Epoch 314/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.2546 - acc: 1.0000 - val_loss: 0.2432 - val_acc: 0.9773\n",
            "Epoch 315/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.2538 - acc: 1.0000 - val_loss: 0.2425 - val_acc: 0.9773\n",
            "Epoch 316/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2530 - acc: 1.0000 - val_loss: 0.2417 - val_acc: 0.9773\n",
            "Epoch 317/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.2523 - acc: 1.0000 - val_loss: 0.2410 - val_acc: 0.9773\n",
            "Epoch 318/500\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.2516 - acc: 1.0000 - val_loss: 0.2403 - val_acc: 0.9773\n",
            "Epoch 319/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2508 - acc: 1.0000 - val_loss: 0.2396 - val_acc: 0.9773\n",
            "Epoch 320/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2501 - acc: 1.0000 - val_loss: 0.2389 - val_acc: 0.9773\n",
            "Epoch 321/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.2494 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 0.9773\n",
            "Epoch 322/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2487 - acc: 1.0000 - val_loss: 0.2375 - val_acc: 0.9773\n",
            "Epoch 323/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2479 - acc: 1.0000 - val_loss: 0.2368 - val_acc: 0.9773\n",
            "Epoch 324/500\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.2472 - acc: 1.0000 - val_loss: 0.2361 - val_acc: 0.9773\n",
            "Epoch 325/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2465 - acc: 1.0000 - val_loss: 0.2354 - val_acc: 0.9773\n",
            "Epoch 326/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2458 - acc: 1.0000 - val_loss: 0.2347 - val_acc: 0.9773\n",
            "Epoch 327/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2451 - acc: 1.0000 - val_loss: 0.2340 - val_acc: 0.9773\n",
            "Epoch 328/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2444 - acc: 1.0000 - val_loss: 0.2334 - val_acc: 0.9773\n",
            "Epoch 329/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2437 - acc: 1.0000 - val_loss: 0.2327 - val_acc: 0.9773\n",
            "Epoch 330/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2430 - acc: 1.0000 - val_loss: 0.2320 - val_acc: 0.9773\n",
            "Epoch 331/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2423 - acc: 1.0000 - val_loss: 0.2313 - val_acc: 0.9773\n",
            "Epoch 332/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2416 - acc: 1.0000 - val_loss: 0.2307 - val_acc: 0.9773\n",
            "Epoch 333/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2409 - acc: 1.0000 - val_loss: 0.2300 - val_acc: 0.9773\n",
            "Epoch 334/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.2402 - acc: 1.0000 - val_loss: 0.2294 - val_acc: 0.9773\n",
            "Epoch 335/500\n",
            "87/87 [==============================] - 0s 192us/step - loss: 0.2395 - acc: 1.0000 - val_loss: 0.2287 - val_acc: 0.9773\n",
            "Epoch 336/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.2388 - acc: 1.0000 - val_loss: 0.2281 - val_acc: 0.9773\n",
            "Epoch 337/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2382 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 0.9773\n",
            "Epoch 338/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2375 - acc: 1.0000 - val_loss: 0.2268 - val_acc: 0.9773\n",
            "Epoch 339/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2368 - acc: 1.0000 - val_loss: 0.2261 - val_acc: 0.9773\n",
            "Epoch 340/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2362 - acc: 1.0000 - val_loss: 0.2255 - val_acc: 0.9773\n",
            "Epoch 341/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2355 - acc: 1.0000 - val_loss: 0.2249 - val_acc: 0.9773\n",
            "Epoch 342/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2349 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 0.9773\n",
            "Epoch 343/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2342 - acc: 1.0000 - val_loss: 0.2236 - val_acc: 0.9773\n",
            "Epoch 344/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2335 - acc: 1.0000 - val_loss: 0.2230 - val_acc: 0.9773\n",
            "Epoch 345/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.2329 - acc: 1.0000 - val_loss: 0.2224 - val_acc: 0.9773\n",
            "Epoch 346/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2322 - acc: 1.0000 - val_loss: 0.2217 - val_acc: 0.9773\n",
            "Epoch 347/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2316 - acc: 1.0000 - val_loss: 0.2211 - val_acc: 0.9773\n",
            "Epoch 348/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2309 - acc: 1.0000 - val_loss: 0.2205 - val_acc: 0.9773\n",
            "Epoch 349/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2303 - acc: 1.0000 - val_loss: 0.2199 - val_acc: 0.9773\n",
            "Epoch 350/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2297 - acc: 1.0000 - val_loss: 0.2193 - val_acc: 0.9773\n",
            "Epoch 351/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2291 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.9773\n",
            "Epoch 352/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2284 - acc: 1.0000 - val_loss: 0.2181 - val_acc: 0.9773\n",
            "Epoch 353/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2278 - acc: 1.0000 - val_loss: 0.2175 - val_acc: 0.9773\n",
            "Epoch 354/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2272 - acc: 1.0000 - val_loss: 0.2169 - val_acc: 0.9773\n",
            "Epoch 355/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2265 - acc: 1.0000 - val_loss: 0.2163 - val_acc: 0.9773\n",
            "Epoch 356/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2259 - acc: 1.0000 - val_loss: 0.2158 - val_acc: 0.9773\n",
            "Epoch 357/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.2253 - acc: 1.0000 - val_loss: 0.2152 - val_acc: 0.9773\n",
            "Epoch 358/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.2247 - acc: 1.0000 - val_loss: 0.2146 - val_acc: 0.9773\n",
            "Epoch 359/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2241 - acc: 1.0000 - val_loss: 0.2140 - val_acc: 0.9773\n",
            "Epoch 360/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2235 - acc: 1.0000 - val_loss: 0.2134 - val_acc: 0.9773\n",
            "Epoch 361/500\n",
            "87/87 [==============================] - 0s 193us/step - loss: 0.2229 - acc: 1.0000 - val_loss: 0.2129 - val_acc: 0.9773\n",
            "Epoch 362/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2223 - acc: 1.0000 - val_loss: 0.2123 - val_acc: 0.9773\n",
            "Epoch 363/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2217 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.9773\n",
            "Epoch 364/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2211 - acc: 1.0000 - val_loss: 0.2112 - val_acc: 0.9773\n",
            "Epoch 365/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2205 - acc: 1.0000 - val_loss: 0.2106 - val_acc: 0.9773\n",
            "Epoch 366/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2199 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.9773\n",
            "Epoch 367/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2193 - acc: 1.0000 - val_loss: 0.2095 - val_acc: 0.9773\n",
            "Epoch 368/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2187 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.9773\n",
            "Epoch 369/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2181 - acc: 1.0000 - val_loss: 0.2084 - val_acc: 0.9773\n",
            "Epoch 370/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2176 - acc: 1.0000 - val_loss: 0.2079 - val_acc: 0.9773\n",
            "Epoch 371/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2170 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.9773\n",
            "Epoch 372/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.2164 - acc: 1.0000 - val_loss: 0.2068 - val_acc: 0.9773\n",
            "Epoch 373/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2158 - acc: 1.0000 - val_loss: 0.2063 - val_acc: 0.9773\n",
            "Epoch 374/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2153 - acc: 1.0000 - val_loss: 0.2057 - val_acc: 0.9773\n",
            "Epoch 375/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2147 - acc: 1.0000 - val_loss: 0.2052 - val_acc: 0.9773\n",
            "Epoch 376/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2141 - acc: 1.0000 - val_loss: 0.2047 - val_acc: 0.9773\n",
            "Epoch 377/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2136 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.9773\n",
            "Epoch 378/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2130 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.9773\n",
            "Epoch 379/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.2125 - acc: 1.0000 - val_loss: 0.2031 - val_acc: 0.9773\n",
            "Epoch 380/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2119 - acc: 1.0000 - val_loss: 0.2026 - val_acc: 0.9773\n",
            "Epoch 381/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2114 - acc: 1.0000 - val_loss: 0.2021 - val_acc: 0.9773\n",
            "Epoch 382/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2108 - acc: 1.0000 - val_loss: 0.2016 - val_acc: 0.9773\n",
            "Epoch 383/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2103 - acc: 1.0000 - val_loss: 0.2011 - val_acc: 0.9773\n",
            "Epoch 384/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.2097 - acc: 1.0000 - val_loss: 0.2006 - val_acc: 0.9773\n",
            "Epoch 385/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2092 - acc: 1.0000 - val_loss: 0.2000 - val_acc: 0.9773\n",
            "Epoch 386/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2086 - acc: 1.0000 - val_loss: 0.1995 - val_acc: 0.9773\n",
            "Epoch 387/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2081 - acc: 1.0000 - val_loss: 0.1991 - val_acc: 0.9773\n",
            "Epoch 388/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2076 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9773\n",
            "Epoch 389/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2070 - acc: 1.0000 - val_loss: 0.1981 - val_acc: 0.9773\n",
            "Epoch 390/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.2065 - acc: 1.0000 - val_loss: 0.1976 - val_acc: 0.9773\n",
            "Epoch 391/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2060 - acc: 1.0000 - val_loss: 0.1971 - val_acc: 0.9773\n",
            "Epoch 392/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2054 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 0.9773\n",
            "Epoch 393/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2049 - acc: 1.0000 - val_loss: 0.1961 - val_acc: 0.9773\n",
            "Epoch 394/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.2044 - acc: 1.0000 - val_loss: 0.1956 - val_acc: 0.9773\n",
            "Epoch 395/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2039 - acc: 1.0000 - val_loss: 0.1951 - val_acc: 0.9773\n",
            "Epoch 396/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2034 - acc: 1.0000 - val_loss: 0.1947 - val_acc: 0.9773\n",
            "Epoch 397/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.2029 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.9773\n",
            "Epoch 398/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2023 - acc: 1.0000 - val_loss: 0.1937 - val_acc: 0.9773\n",
            "Epoch 399/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2018 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.9773\n",
            "Epoch 400/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2013 - acc: 1.0000 - val_loss: 0.1928 - val_acc: 0.9773\n",
            "Epoch 401/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2008 - acc: 1.0000 - val_loss: 0.1923 - val_acc: 0.9773\n",
            "Epoch 402/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2003 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9773\n",
            "Epoch 403/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1998 - acc: 1.0000 - val_loss: 0.1914 - val_acc: 0.9773\n",
            "Epoch 404/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1993 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9773\n",
            "Epoch 405/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1988 - acc: 1.0000 - val_loss: 0.1905 - val_acc: 0.9773\n",
            "Epoch 406/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1983 - acc: 1.0000 - val_loss: 0.1900 - val_acc: 0.9773\n",
            "Epoch 407/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.1978 - acc: 1.0000 - val_loss: 0.1896 - val_acc: 0.9773\n",
            "Epoch 408/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1973 - acc: 1.0000 - val_loss: 0.1891 - val_acc: 0.9773\n",
            "Epoch 409/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.1969 - acc: 1.0000 - val_loss: 0.1887 - val_acc: 0.9773\n",
            "Epoch 410/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1964 - acc: 1.0000 - val_loss: 0.1882 - val_acc: 0.9773\n",
            "Epoch 411/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.1959 - acc: 1.0000 - val_loss: 0.1878 - val_acc: 0.9773\n",
            "Epoch 412/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1954 - acc: 1.0000 - val_loss: 0.1873 - val_acc: 0.9773\n",
            "Epoch 413/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1949 - acc: 1.0000 - val_loss: 0.1869 - val_acc: 0.9773\n",
            "Epoch 414/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1945 - acc: 1.0000 - val_loss: 0.1864 - val_acc: 0.9773\n",
            "Epoch 415/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1940 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.9773\n",
            "Epoch 416/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1935 - acc: 1.0000 - val_loss: 0.1856 - val_acc: 0.9773\n",
            "Epoch 417/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1930 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9773\n",
            "Epoch 418/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1926 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 0.9773\n",
            "Epoch 419/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1921 - acc: 1.0000 - val_loss: 0.1843 - val_acc: 0.9773\n",
            "Epoch 420/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1916 - acc: 1.0000 - val_loss: 0.1839 - val_acc: 0.9773\n",
            "Epoch 421/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1912 - acc: 1.0000 - val_loss: 0.1834 - val_acc: 0.9773\n",
            "Epoch 422/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1907 - acc: 1.0000 - val_loss: 0.1830 - val_acc: 0.9773\n",
            "Epoch 423/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.1902 - acc: 1.0000 - val_loss: 0.1826 - val_acc: 0.9773\n",
            "Epoch 424/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1898 - acc: 1.0000 - val_loss: 0.1822 - val_acc: 0.9773\n",
            "Epoch 425/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1893 - acc: 1.0000 - val_loss: 0.1818 - val_acc: 0.9773\n",
            "Epoch 426/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1889 - acc: 1.0000 - val_loss: 0.1813 - val_acc: 0.9773\n",
            "Epoch 427/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1884 - acc: 1.0000 - val_loss: 0.1809 - val_acc: 0.9773\n",
            "Epoch 428/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1880 - acc: 1.0000 - val_loss: 0.1805 - val_acc: 0.9773\n",
            "Epoch 429/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1875 - acc: 1.0000 - val_loss: 0.1801 - val_acc: 0.9773\n",
            "Epoch 430/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1871 - acc: 1.0000 - val_loss: 0.1797 - val_acc: 0.9773\n",
            "Epoch 431/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1867 - acc: 1.0000 - val_loss: 0.1793 - val_acc: 0.9773\n",
            "Epoch 432/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1862 - acc: 1.0000 - val_loss: 0.1789 - val_acc: 0.9773\n",
            "Epoch 433/500\n",
            "87/87 [==============================] - 0s 191us/step - loss: 0.1858 - acc: 1.0000 - val_loss: 0.1785 - val_acc: 0.9773\n",
            "Epoch 434/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.1853 - acc: 1.0000 - val_loss: 0.1781 - val_acc: 0.9773\n",
            "Epoch 435/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.1849 - acc: 1.0000 - val_loss: 0.1777 - val_acc: 0.9773\n",
            "Epoch 436/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1845 - acc: 1.0000 - val_loss: 0.1773 - val_acc: 0.9773\n",
            "Epoch 437/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1840 - acc: 1.0000 - val_loss: 0.1769 - val_acc: 0.9773\n",
            "Epoch 438/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1836 - acc: 1.0000 - val_loss: 0.1765 - val_acc: 0.9773\n",
            "Epoch 439/500\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.1832 - acc: 1.0000 - val_loss: 0.1761 - val_acc: 0.9773\n",
            "Epoch 440/500\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.1828 - acc: 1.0000 - val_loss: 0.1757 - val_acc: 0.9773\n",
            "Epoch 441/500\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.1823 - acc: 1.0000 - val_loss: 0.1754 - val_acc: 0.9773\n",
            "Epoch 442/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1819 - acc: 1.0000 - val_loss: 0.1750 - val_acc: 0.9773\n",
            "Epoch 443/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.1815 - acc: 1.0000 - val_loss: 0.1746 - val_acc: 0.9773\n",
            "Epoch 444/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.1811 - acc: 1.0000 - val_loss: 0.1742 - val_acc: 0.9773\n",
            "Epoch 445/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.1806 - acc: 1.0000 - val_loss: 0.1738 - val_acc: 0.9773\n",
            "Epoch 446/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1802 - acc: 1.0000 - val_loss: 0.1735 - val_acc: 0.9773\n",
            "Epoch 447/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1798 - acc: 1.0000 - val_loss: 0.1731 - val_acc: 0.9773\n",
            "Epoch 448/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1794 - acc: 1.0000 - val_loss: 0.1727 - val_acc: 0.9773\n",
            "Epoch 449/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.1790 - acc: 1.0000 - val_loss: 0.1723 - val_acc: 0.9773\n",
            "Epoch 450/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1786 - acc: 1.0000 - val_loss: 0.1720 - val_acc: 0.9773\n",
            "Epoch 451/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1782 - acc: 1.0000 - val_loss: 0.1716 - val_acc: 0.9773\n",
            "Epoch 452/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.1777 - acc: 1.0000 - val_loss: 0.1712 - val_acc: 0.9773\n",
            "Epoch 453/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.1774 - acc: 1.0000 - val_loss: 0.1709 - val_acc: 0.9773\n",
            "Epoch 454/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1770 - acc: 1.0000 - val_loss: 0.1705 - val_acc: 0.9773\n",
            "Epoch 455/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.1765 - acc: 1.0000 - val_loss: 0.1701 - val_acc: 0.9773\n",
            "Epoch 456/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1761 - acc: 1.0000 - val_loss: 0.1698 - val_acc: 0.9773\n",
            "Epoch 457/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1757 - acc: 1.0000 - val_loss: 0.1694 - val_acc: 0.9773\n",
            "Epoch 458/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1754 - acc: 1.0000 - val_loss: 0.1691 - val_acc: 0.9773\n",
            "Epoch 459/500\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.1750 - acc: 1.0000 - val_loss: 0.1687 - val_acc: 0.9773\n",
            "Epoch 460/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.1746 - acc: 1.0000 - val_loss: 0.1683 - val_acc: 0.9773\n",
            "Epoch 461/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1742 - acc: 1.0000 - val_loss: 0.1680 - val_acc: 0.9773\n",
            "Epoch 462/500\n",
            "87/87 [==============================] - 0s 300us/step - loss: 0.1738 - acc: 1.0000 - val_loss: 0.1676 - val_acc: 0.9773\n",
            "Epoch 463/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1734 - acc: 1.0000 - val_loss: 0.1673 - val_acc: 0.9773\n",
            "Epoch 464/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1730 - acc: 1.0000 - val_loss: 0.1669 - val_acc: 0.9773\n",
            "Epoch 465/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1726 - acc: 1.0000 - val_loss: 0.1666 - val_acc: 0.9773\n",
            "Epoch 466/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.1722 - acc: 1.0000 - val_loss: 0.1663 - val_acc: 0.9773\n",
            "Epoch 467/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1718 - acc: 1.0000 - val_loss: 0.1659 - val_acc: 0.9773\n",
            "Epoch 468/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1715 - acc: 1.0000 - val_loss: 0.1656 - val_acc: 0.9773\n",
            "Epoch 469/500\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.1711 - acc: 1.0000 - val_loss: 0.1652 - val_acc: 0.9773\n",
            "Epoch 470/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1707 - acc: 1.0000 - val_loss: 0.1649 - val_acc: 0.9773\n",
            "Epoch 471/500\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.1703 - acc: 1.0000 - val_loss: 0.1646 - val_acc: 0.9773\n",
            "Epoch 472/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.1700 - acc: 1.0000 - val_loss: 0.1642 - val_acc: 0.9773\n",
            "Epoch 473/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1696 - acc: 1.0000 - val_loss: 0.1639 - val_acc: 0.9773\n",
            "Epoch 474/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.1692 - acc: 1.0000 - val_loss: 0.1635 - val_acc: 0.9773\n",
            "Epoch 475/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.1688 - acc: 1.0000 - val_loss: 0.1632 - val_acc: 0.9773\n",
            "Epoch 476/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1685 - acc: 1.0000 - val_loss: 0.1629 - val_acc: 0.9773\n",
            "Epoch 477/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1681 - acc: 1.0000 - val_loss: 0.1626 - val_acc: 0.9773\n",
            "Epoch 478/500\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.1677 - acc: 1.0000 - val_loss: 0.1622 - val_acc: 0.9773\n",
            "Epoch 479/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1674 - acc: 1.0000 - val_loss: 0.1619 - val_acc: 0.9773\n",
            "Epoch 480/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1670 - acc: 1.0000 - val_loss: 0.1616 - val_acc: 0.9773\n",
            "Epoch 481/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1666 - acc: 1.0000 - val_loss: 0.1613 - val_acc: 0.9773\n",
            "Epoch 482/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1663 - acc: 1.0000 - val_loss: 0.1609 - val_acc: 0.9773\n",
            "Epoch 483/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.1659 - acc: 1.0000 - val_loss: 0.1606 - val_acc: 0.9773\n",
            "Epoch 484/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1656 - acc: 1.0000 - val_loss: 0.1603 - val_acc: 0.9773\n",
            "Epoch 485/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.1652 - acc: 1.0000 - val_loss: 0.1600 - val_acc: 0.9773\n",
            "Epoch 486/500\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.1649 - acc: 1.0000 - val_loss: 0.1597 - val_acc: 0.9773\n",
            "Epoch 487/500\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.1645 - acc: 1.0000 - val_loss: 0.1594 - val_acc: 0.9773\n",
            "Epoch 488/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1641 - acc: 1.0000 - val_loss: 0.1590 - val_acc: 0.9773\n",
            "Epoch 489/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1638 - acc: 1.0000 - val_loss: 0.1587 - val_acc: 0.9773\n",
            "Epoch 490/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.1634 - acc: 1.0000 - val_loss: 0.1584 - val_acc: 0.9773\n",
            "Epoch 491/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.1631 - acc: 1.0000 - val_loss: 0.1581 - val_acc: 0.9773\n",
            "Epoch 492/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1627 - acc: 1.0000 - val_loss: 0.1578 - val_acc: 0.9773\n",
            "Epoch 493/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.1624 - acc: 1.0000 - val_loss: 0.1575 - val_acc: 0.9773\n",
            "Epoch 494/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1621 - acc: 1.0000 - val_loss: 0.1572 - val_acc: 0.9773\n",
            "Epoch 495/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.1617 - acc: 1.0000 - val_loss: 0.1569 - val_acc: 0.9773\n",
            "Epoch 496/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1614 - acc: 1.0000 - val_loss: 0.1566 - val_acc: 0.9773\n",
            "Epoch 497/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1610 - acc: 1.0000 - val_loss: 0.1563 - val_acc: 0.9773\n",
            "Epoch 498/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1607 - acc: 1.0000 - val_loss: 0.1560 - val_acc: 0.9773\n",
            "Epoch 499/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1604 - acc: 1.0000 - val_loss: 0.1557 - val_acc: 0.9773\n",
            "Epoch 500/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1600 - acc: 1.0000 - val_loss: 0.1554 - val_acc: 0.9773\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/500\n",
            "87/87 [==============================] - 1s 6ms/step - loss: 1.5416 - acc: 0.0115 - val_loss: 1.4528 - val_acc: 0.0227\n",
            "Epoch 2/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.5163 - acc: 0.0115 - val_loss: 1.4317 - val_acc: 0.0455\n",
            "Epoch 3/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.4920 - acc: 0.0230 - val_loss: 1.4114 - val_acc: 0.0455\n",
            "Epoch 4/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 1.4684 - acc: 0.0115 - val_loss: 1.3915 - val_acc: 0.0455\n",
            "Epoch 5/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.4456 - acc: 0.0230 - val_loss: 1.3726 - val_acc: 0.0455\n",
            "Epoch 6/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 1.4237 - acc: 0.0230 - val_loss: 1.3539 - val_acc: 0.0455\n",
            "Epoch 7/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 1.4027 - acc: 0.0460 - val_loss: 1.3364 - val_acc: 0.0455\n",
            "Epoch 8/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 1.3822 - acc: 0.0460 - val_loss: 1.3193 - val_acc: 0.0455\n",
            "Epoch 9/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 1.3625 - acc: 0.0460 - val_loss: 1.3022 - val_acc: 0.0909\n",
            "Epoch 10/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.3433 - acc: 0.0460 - val_loss: 1.2861 - val_acc: 0.1136\n",
            "Epoch 11/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 1.3246 - acc: 0.0575 - val_loss: 1.2707 - val_acc: 0.1136\n",
            "Epoch 12/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.3068 - acc: 0.0575 - val_loss: 1.2556 - val_acc: 0.1591\n",
            "Epoch 13/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.2891 - acc: 0.0805 - val_loss: 1.2405 - val_acc: 0.1591\n",
            "Epoch 14/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 1.2722 - acc: 0.0920 - val_loss: 1.2261 - val_acc: 0.1591\n",
            "Epoch 15/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 1.2557 - acc: 0.1034 - val_loss: 1.2122 - val_acc: 0.1818\n",
            "Epoch 16/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 1.2396 - acc: 0.1379 - val_loss: 1.1986 - val_acc: 0.1818\n",
            "Epoch 17/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 1.2241 - acc: 0.1494 - val_loss: 1.1853 - val_acc: 0.1818\n",
            "Epoch 18/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.2089 - acc: 0.1494 - val_loss: 1.1726 - val_acc: 0.1818\n",
            "Epoch 19/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.1942 - acc: 0.2069 - val_loss: 1.1602 - val_acc: 0.2273\n",
            "Epoch 20/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 1.1802 - acc: 0.2299 - val_loss: 1.1477 - val_acc: 0.2500\n",
            "Epoch 21/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.1662 - acc: 0.2184 - val_loss: 1.1361 - val_acc: 0.2727\n",
            "Epoch 22/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 1.1527 - acc: 0.2414 - val_loss: 1.1242 - val_acc: 0.3182\n",
            "Epoch 23/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 1.1394 - acc: 0.2989 - val_loss: 1.1129 - val_acc: 0.3636\n",
            "Epoch 24/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.1266 - acc: 0.3448 - val_loss: 1.1018 - val_acc: 0.3864\n",
            "Epoch 25/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 1.1141 - acc: 0.3908 - val_loss: 1.0910 - val_acc: 0.4091\n",
            "Epoch 26/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 1.1018 - acc: 0.4253 - val_loss: 1.0803 - val_acc: 0.4545\n",
            "Epoch 27/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.0899 - acc: 0.4828 - val_loss: 1.0703 - val_acc: 0.4773\n",
            "Epoch 28/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 1.0782 - acc: 0.5172 - val_loss: 1.0602 - val_acc: 0.4773\n",
            "Epoch 29/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 1.0669 - acc: 0.5172 - val_loss: 1.0503 - val_acc: 0.5000\n",
            "Epoch 30/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.0557 - acc: 0.5172 - val_loss: 1.0407 - val_acc: 0.5000\n",
            "Epoch 31/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 1.0448 - acc: 0.5172 - val_loss: 1.0313 - val_acc: 0.5909\n",
            "Epoch 32/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 1.0341 - acc: 0.5287 - val_loss: 1.0220 - val_acc: 0.6364\n",
            "Epoch 33/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 1.0238 - acc: 0.6207 - val_loss: 1.0129 - val_acc: 0.6591\n",
            "Epoch 34/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.0136 - acc: 0.7011 - val_loss: 1.0042 - val_acc: 0.6591\n",
            "Epoch 35/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.0037 - acc: 0.7586 - val_loss: 0.9954 - val_acc: 0.6591\n",
            "Epoch 36/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.9937 - acc: 0.7701 - val_loss: 0.9869 - val_acc: 0.6818\n",
            "Epoch 37/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.9842 - acc: 0.7816 - val_loss: 0.9785 - val_acc: 0.7045\n",
            "Epoch 38/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.9750 - acc: 0.8161 - val_loss: 0.9704 - val_acc: 0.7727\n",
            "Epoch 39/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.9657 - acc: 0.8391 - val_loss: 0.9624 - val_acc: 0.8409\n",
            "Epoch 40/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9568 - acc: 0.8391 - val_loss: 0.9546 - val_acc: 0.8636\n",
            "Epoch 41/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.9482 - acc: 0.8506 - val_loss: 0.9470 - val_acc: 0.8636\n",
            "Epoch 42/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.9395 - acc: 0.8506 - val_loss: 0.9395 - val_acc: 0.8636\n",
            "Epoch 43/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.9312 - acc: 0.8506 - val_loss: 0.9320 - val_acc: 0.8636\n",
            "Epoch 44/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.9230 - acc: 0.8506 - val_loss: 0.9249 - val_acc: 0.8636\n",
            "Epoch 45/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.9150 - acc: 0.8506 - val_loss: 0.9178 - val_acc: 0.8864\n",
            "Epoch 46/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.9074 - acc: 0.8621 - val_loss: 0.9108 - val_acc: 0.8864\n",
            "Epoch 47/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8996 - acc: 0.8621 - val_loss: 0.9040 - val_acc: 0.8864\n",
            "Epoch 48/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8922 - acc: 0.8621 - val_loss: 0.8972 - val_acc: 0.8864\n",
            "Epoch 49/500\n",
            "87/87 [==============================] - 0s 314us/step - loss: 0.8847 - acc: 0.8621 - val_loss: 0.8906 - val_acc: 0.8864\n",
            "Epoch 50/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8775 - acc: 0.8621 - val_loss: 0.8841 - val_acc: 0.8864\n",
            "Epoch 51/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8704 - acc: 0.8621 - val_loss: 0.8777 - val_acc: 0.8864\n",
            "Epoch 52/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8635 - acc: 0.8621 - val_loss: 0.8713 - val_acc: 0.8864\n",
            "Epoch 53/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8568 - acc: 0.8621 - val_loss: 0.8652 - val_acc: 0.8864\n",
            "Epoch 54/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8500 - acc: 0.8621 - val_loss: 0.8591 - val_acc: 0.8864\n",
            "Epoch 55/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8435 - acc: 0.8621 - val_loss: 0.8530 - val_acc: 0.8864\n",
            "Epoch 56/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8369 - acc: 0.8621 - val_loss: 0.8472 - val_acc: 0.8864\n",
            "Epoch 57/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8307 - acc: 0.8621 - val_loss: 0.8414 - val_acc: 0.8864\n",
            "Epoch 58/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8244 - acc: 0.8621 - val_loss: 0.8356 - val_acc: 0.8864\n",
            "Epoch 59/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8182 - acc: 0.8621 - val_loss: 0.8299 - val_acc: 0.8864\n",
            "Epoch 60/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8123 - acc: 0.8621 - val_loss: 0.8242 - val_acc: 0.8864\n",
            "Epoch 61/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8064 - acc: 0.8621 - val_loss: 0.8189 - val_acc: 0.8864\n",
            "Epoch 62/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8006 - acc: 0.8621 - val_loss: 0.8134 - val_acc: 0.8864\n",
            "Epoch 63/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7950 - acc: 0.8621 - val_loss: 0.8081 - val_acc: 0.8864\n",
            "Epoch 64/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.7895 - acc: 0.8621 - val_loss: 0.8028 - val_acc: 0.8864\n",
            "Epoch 65/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7839 - acc: 0.8621 - val_loss: 0.7976 - val_acc: 0.8864\n",
            "Epoch 66/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7785 - acc: 0.8621 - val_loss: 0.7925 - val_acc: 0.8864\n",
            "Epoch 67/500\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.7732 - acc: 0.8621 - val_loss: 0.7875 - val_acc: 0.8864\n",
            "Epoch 68/500\n",
            "87/87 [==============================] - 0s 318us/step - loss: 0.7679 - acc: 0.8621 - val_loss: 0.7824 - val_acc: 0.8864\n",
            "Epoch 69/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.7628 - acc: 0.8621 - val_loss: 0.7775 - val_acc: 0.8864\n",
            "Epoch 70/500\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.7577 - acc: 0.8621 - val_loss: 0.7727 - val_acc: 0.8864\n",
            "Epoch 71/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7528 - acc: 0.8621 - val_loss: 0.7679 - val_acc: 0.8864\n",
            "Epoch 72/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7478 - acc: 0.8621 - val_loss: 0.7632 - val_acc: 0.8864\n",
            "Epoch 73/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.7430 - acc: 0.8506 - val_loss: 0.7584 - val_acc: 0.8864\n",
            "Epoch 74/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.7382 - acc: 0.8506 - val_loss: 0.7538 - val_acc: 0.8864\n",
            "Epoch 75/500\n",
            "87/87 [==============================] - 0s 191us/step - loss: 0.7335 - acc: 0.8506 - val_loss: 0.7493 - val_acc: 0.8864\n",
            "Epoch 76/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7290 - acc: 0.8506 - val_loss: 0.7446 - val_acc: 0.8864\n",
            "Epoch 77/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.7243 - acc: 0.8506 - val_loss: 0.7402 - val_acc: 0.8864\n",
            "Epoch 78/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.7198 - acc: 0.8506 - val_loss: 0.7359 - val_acc: 0.8864\n",
            "Epoch 79/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7154 - acc: 0.8506 - val_loss: 0.7315 - val_acc: 0.8864\n",
            "Epoch 80/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7110 - acc: 0.8506 - val_loss: 0.7272 - val_acc: 0.8864\n",
            "Epoch 81/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7068 - acc: 0.8506 - val_loss: 0.7229 - val_acc: 0.8864\n",
            "Epoch 82/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.7024 - acc: 0.8506 - val_loss: 0.7188 - val_acc: 0.8864\n",
            "Epoch 83/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.6982 - acc: 0.8506 - val_loss: 0.7146 - val_acc: 0.8864\n",
            "Epoch 84/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6941 - acc: 0.8506 - val_loss: 0.7105 - val_acc: 0.8864\n",
            "Epoch 85/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.6901 - acc: 0.8506 - val_loss: 0.7065 - val_acc: 0.8864\n",
            "Epoch 86/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.6860 - acc: 0.8506 - val_loss: 0.7025 - val_acc: 0.8864\n",
            "Epoch 87/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6820 - acc: 0.8506 - val_loss: 0.6985 - val_acc: 0.8864\n",
            "Epoch 88/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6782 - acc: 0.8506 - val_loss: 0.6946 - val_acc: 0.8864\n",
            "Epoch 89/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6744 - acc: 0.8506 - val_loss: 0.6907 - val_acc: 0.8864\n",
            "Epoch 90/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6706 - acc: 0.8506 - val_loss: 0.6868 - val_acc: 0.8864\n",
            "Epoch 91/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6668 - acc: 0.8506 - val_loss: 0.6832 - val_acc: 0.8864\n",
            "Epoch 92/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6632 - acc: 0.8506 - val_loss: 0.6795 - val_acc: 0.8864\n",
            "Epoch 93/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6596 - acc: 0.8506 - val_loss: 0.6758 - val_acc: 0.8864\n",
            "Epoch 94/500\n",
            "87/87 [==============================] - 0s 295us/step - loss: 0.6561 - acc: 0.8506 - val_loss: 0.6722 - val_acc: 0.8864\n",
            "Epoch 95/500\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.6527 - acc: 0.8506 - val_loss: 0.6687 - val_acc: 0.8864\n",
            "Epoch 96/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6492 - acc: 0.8506 - val_loss: 0.6652 - val_acc: 0.8864\n",
            "Epoch 97/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.6459 - acc: 0.8506 - val_loss: 0.6617 - val_acc: 0.8864\n",
            "Epoch 98/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.6425 - acc: 0.8506 - val_loss: 0.6583 - val_acc: 0.8864\n",
            "Epoch 99/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.6392 - acc: 0.8506 - val_loss: 0.6548 - val_acc: 0.8864\n",
            "Epoch 100/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.6359 - acc: 0.8506 - val_loss: 0.6514 - val_acc: 0.8864\n",
            "Epoch 101/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.6327 - acc: 0.8506 - val_loss: 0.6481 - val_acc: 0.8864\n",
            "Epoch 102/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.6295 - acc: 0.8506 - val_loss: 0.6448 - val_acc: 0.8864\n",
            "Epoch 103/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.6263 - acc: 0.8506 - val_loss: 0.6415 - val_acc: 0.8864\n",
            "Epoch 104/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.6232 - acc: 0.8506 - val_loss: 0.6382 - val_acc: 0.8864\n",
            "Epoch 105/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.6201 - acc: 0.8506 - val_loss: 0.6350 - val_acc: 0.8864\n",
            "Epoch 106/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.6170 - acc: 0.8506 - val_loss: 0.6317 - val_acc: 0.8864\n",
            "Epoch 107/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.6139 - acc: 0.8506 - val_loss: 0.6286 - val_acc: 0.8864\n",
            "Epoch 108/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.6109 - acc: 0.8506 - val_loss: 0.6254 - val_acc: 0.8864\n",
            "Epoch 109/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.6080 - acc: 0.8506 - val_loss: 0.6223 - val_acc: 0.8864\n",
            "Epoch 110/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.6049 - acc: 0.8506 - val_loss: 0.6193 - val_acc: 0.8864\n",
            "Epoch 111/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.6020 - acc: 0.8506 - val_loss: 0.6162 - val_acc: 0.8864\n",
            "Epoch 112/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.5991 - acc: 0.8506 - val_loss: 0.6132 - val_acc: 0.8864\n",
            "Epoch 113/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.5963 - acc: 0.8506 - val_loss: 0.6102 - val_acc: 0.8864\n",
            "Epoch 114/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.5935 - acc: 0.8506 - val_loss: 0.6073 - val_acc: 0.8864\n",
            "Epoch 115/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.5906 - acc: 0.8506 - val_loss: 0.6043 - val_acc: 0.8864\n",
            "Epoch 116/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.5879 - acc: 0.8506 - val_loss: 0.6014 - val_acc: 0.8864\n",
            "Epoch 117/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.5851 - acc: 0.8506 - val_loss: 0.5985 - val_acc: 0.8864\n",
            "Epoch 118/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.5824 - acc: 0.8506 - val_loss: 0.5957 - val_acc: 0.8864\n",
            "Epoch 119/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.5797 - acc: 0.8506 - val_loss: 0.5928 - val_acc: 0.8864\n",
            "Epoch 120/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5770 - acc: 0.8506 - val_loss: 0.5900 - val_acc: 0.8864\n",
            "Epoch 121/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5743 - acc: 0.8506 - val_loss: 0.5872 - val_acc: 0.8864\n",
            "Epoch 122/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5716 - acc: 0.8506 - val_loss: 0.5844 - val_acc: 0.8864\n",
            "Epoch 123/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5690 - acc: 0.8506 - val_loss: 0.5817 - val_acc: 0.8864\n",
            "Epoch 124/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.5664 - acc: 0.8506 - val_loss: 0.5790 - val_acc: 0.8864\n",
            "Epoch 125/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5638 - acc: 0.8506 - val_loss: 0.5763 - val_acc: 0.8864\n",
            "Epoch 126/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.5613 - acc: 0.8506 - val_loss: 0.5736 - val_acc: 0.8864\n",
            "Epoch 127/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5587 - acc: 0.8506 - val_loss: 0.5709 - val_acc: 0.8864\n",
            "Epoch 128/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.5562 - acc: 0.8506 - val_loss: 0.5683 - val_acc: 0.8864\n",
            "Epoch 129/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5537 - acc: 0.8506 - val_loss: 0.5657 - val_acc: 0.8864\n",
            "Epoch 130/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.5512 - acc: 0.8506 - val_loss: 0.5631 - val_acc: 0.8864\n",
            "Epoch 131/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5488 - acc: 0.8506 - val_loss: 0.5604 - val_acc: 0.8864\n",
            "Epoch 132/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5463 - acc: 0.8506 - val_loss: 0.5579 - val_acc: 0.8864\n",
            "Epoch 133/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5439 - acc: 0.8506 - val_loss: 0.5553 - val_acc: 0.8864\n",
            "Epoch 134/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5415 - acc: 0.8506 - val_loss: 0.5528 - val_acc: 0.8864\n",
            "Epoch 135/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5391 - acc: 0.8506 - val_loss: 0.5503 - val_acc: 0.8864\n",
            "Epoch 136/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5367 - acc: 0.8506 - val_loss: 0.5477 - val_acc: 0.8864\n",
            "Epoch 137/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.5343 - acc: 0.8506 - val_loss: 0.5452 - val_acc: 0.8864\n",
            "Epoch 138/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5320 - acc: 0.8506 - val_loss: 0.5428 - val_acc: 0.8864\n",
            "Epoch 139/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5297 - acc: 0.8506 - val_loss: 0.5403 - val_acc: 0.8864\n",
            "Epoch 140/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5274 - acc: 0.8506 - val_loss: 0.5379 - val_acc: 0.8864\n",
            "Epoch 141/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5251 - acc: 0.8506 - val_loss: 0.5355 - val_acc: 0.8864\n",
            "Epoch 142/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5228 - acc: 0.8506 - val_loss: 0.5331 - val_acc: 0.8864\n",
            "Epoch 143/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.5205 - acc: 0.8506 - val_loss: 0.5307 - val_acc: 0.8864\n",
            "Epoch 144/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.5183 - acc: 0.8506 - val_loss: 0.5283 - val_acc: 0.8864\n",
            "Epoch 145/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5160 - acc: 0.8506 - val_loss: 0.5260 - val_acc: 0.8864\n",
            "Epoch 146/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5138 - acc: 0.8506 - val_loss: 0.5236 - val_acc: 0.8864\n",
            "Epoch 147/500\n",
            "87/87 [==============================] - 0s 326us/step - loss: 0.5116 - acc: 0.8506 - val_loss: 0.5213 - val_acc: 0.8864\n",
            "Epoch 148/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5094 - acc: 0.8506 - val_loss: 0.5190 - val_acc: 0.8864\n",
            "Epoch 149/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5073 - acc: 0.8506 - val_loss: 0.5168 - val_acc: 0.8864\n",
            "Epoch 150/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5051 - acc: 0.8506 - val_loss: 0.5145 - val_acc: 0.8864\n",
            "Epoch 151/500\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.5029 - acc: 0.8506 - val_loss: 0.5122 - val_acc: 0.8864\n",
            "Epoch 152/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5008 - acc: 0.8506 - val_loss: 0.5100 - val_acc: 0.8864\n",
            "Epoch 153/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.4987 - acc: 0.8506 - val_loss: 0.5077 - val_acc: 0.8864\n",
            "Epoch 154/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4965 - acc: 0.8506 - val_loss: 0.5055 - val_acc: 0.8864\n",
            "Epoch 155/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4945 - acc: 0.8506 - val_loss: 0.5033 - val_acc: 0.8864\n",
            "Epoch 156/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.4924 - acc: 0.8506 - val_loss: 0.5011 - val_acc: 0.8864\n",
            "Epoch 157/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4903 - acc: 0.8506 - val_loss: 0.4989 - val_acc: 0.8864\n",
            "Epoch 158/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4883 - acc: 0.8506 - val_loss: 0.4968 - val_acc: 0.8864\n",
            "Epoch 159/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4862 - acc: 0.8506 - val_loss: 0.4946 - val_acc: 0.8864\n",
            "Epoch 160/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4842 - acc: 0.8506 - val_loss: 0.4925 - val_acc: 0.8864\n",
            "Epoch 161/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4822 - acc: 0.8506 - val_loss: 0.4904 - val_acc: 0.8864\n",
            "Epoch 162/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4801 - acc: 0.8506 - val_loss: 0.4883 - val_acc: 0.8864\n",
            "Epoch 163/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4781 - acc: 0.8506 - val_loss: 0.4863 - val_acc: 0.8864\n",
            "Epoch 164/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4762 - acc: 0.8506 - val_loss: 0.4842 - val_acc: 0.8864\n",
            "Epoch 165/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.4741 - acc: 0.8506 - val_loss: 0.4822 - val_acc: 0.8864\n",
            "Epoch 166/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4722 - acc: 0.8506 - val_loss: 0.4801 - val_acc: 0.8864\n",
            "Epoch 167/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.4702 - acc: 0.8506 - val_loss: 0.4781 - val_acc: 0.8864\n",
            "Epoch 168/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4682 - acc: 0.8506 - val_loss: 0.4760 - val_acc: 0.8864\n",
            "Epoch 169/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.4663 - acc: 0.8506 - val_loss: 0.4740 - val_acc: 0.8864\n",
            "Epoch 170/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4644 - acc: 0.8506 - val_loss: 0.4720 - val_acc: 0.8864\n",
            "Epoch 171/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.4625 - acc: 0.8506 - val_loss: 0.4700 - val_acc: 0.8864\n",
            "Epoch 172/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4605 - acc: 0.8506 - val_loss: 0.4680 - val_acc: 0.8864\n",
            "Epoch 173/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4586 - acc: 0.8506 - val_loss: 0.4660 - val_acc: 0.8864\n",
            "Epoch 174/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4567 - acc: 0.8506 - val_loss: 0.4642 - val_acc: 0.8864\n",
            "Epoch 175/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.4549 - acc: 0.8506 - val_loss: 0.4622 - val_acc: 0.8864\n",
            "Epoch 176/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4530 - acc: 0.8506 - val_loss: 0.4603 - val_acc: 0.8636\n",
            "Epoch 177/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4511 - acc: 0.8506 - val_loss: 0.4584 - val_acc: 0.8636\n",
            "Epoch 178/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4492 - acc: 0.8506 - val_loss: 0.4565 - val_acc: 0.8636\n",
            "Epoch 179/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4474 - acc: 0.8506 - val_loss: 0.4546 - val_acc: 0.8636\n",
            "Epoch 180/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.4455 - acc: 0.8506 - val_loss: 0.4527 - val_acc: 0.8636\n",
            "Epoch 181/500\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.4437 - acc: 0.8506 - val_loss: 0.4508 - val_acc: 0.8636\n",
            "Epoch 182/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.4419 - acc: 0.8506 - val_loss: 0.4490 - val_acc: 0.8636\n",
            "Epoch 183/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4401 - acc: 0.8506 - val_loss: 0.4471 - val_acc: 0.8636\n",
            "Epoch 184/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.4383 - acc: 0.8506 - val_loss: 0.4453 - val_acc: 0.8636\n",
            "Epoch 185/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4365 - acc: 0.8506 - val_loss: 0.4434 - val_acc: 0.8636\n",
            "Epoch 186/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4347 - acc: 0.8506 - val_loss: 0.4417 - val_acc: 0.8636\n",
            "Epoch 187/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4329 - acc: 0.8506 - val_loss: 0.4398 - val_acc: 0.8636\n",
            "Epoch 188/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4312 - acc: 0.8506 - val_loss: 0.4380 - val_acc: 0.8636\n",
            "Epoch 189/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.4294 - acc: 0.8506 - val_loss: 0.4363 - val_acc: 0.8636\n",
            "Epoch 190/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4277 - acc: 0.8506 - val_loss: 0.4345 - val_acc: 0.8636\n",
            "Epoch 191/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4259 - acc: 0.8506 - val_loss: 0.4328 - val_acc: 0.8636\n",
            "Epoch 192/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.4242 - acc: 0.8506 - val_loss: 0.4310 - val_acc: 0.8636\n",
            "Epoch 193/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.4225 - acc: 0.8506 - val_loss: 0.4292 - val_acc: 0.8636\n",
            "Epoch 194/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.4208 - acc: 0.8506 - val_loss: 0.4275 - val_acc: 0.8636\n",
            "Epoch 195/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.4191 - acc: 0.8506 - val_loss: 0.4258 - val_acc: 0.8636\n",
            "Epoch 196/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.4174 - acc: 0.8506 - val_loss: 0.4241 - val_acc: 0.8636\n",
            "Epoch 197/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4158 - acc: 0.8506 - val_loss: 0.4224 - val_acc: 0.8636\n",
            "Epoch 198/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4141 - acc: 0.8506 - val_loss: 0.4208 - val_acc: 0.8636\n",
            "Epoch 199/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4124 - acc: 0.8506 - val_loss: 0.4191 - val_acc: 0.8636\n",
            "Epoch 200/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4107 - acc: 0.8506 - val_loss: 0.4175 - val_acc: 0.8636\n",
            "Epoch 201/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4091 - acc: 0.8506 - val_loss: 0.4158 - val_acc: 0.8636\n",
            "Epoch 202/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.4074 - acc: 0.8506 - val_loss: 0.4142 - val_acc: 0.8636\n",
            "Epoch 203/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4058 - acc: 0.8506 - val_loss: 0.4126 - val_acc: 0.8636\n",
            "Epoch 204/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4042 - acc: 0.8621 - val_loss: 0.4110 - val_acc: 0.8636\n",
            "Epoch 205/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.4025 - acc: 0.8621 - val_loss: 0.4095 - val_acc: 0.8636\n",
            "Epoch 206/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4010 - acc: 0.8621 - val_loss: 0.4079 - val_acc: 0.8636\n",
            "Epoch 207/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3993 - acc: 0.8621 - val_loss: 0.4063 - val_acc: 0.8636\n",
            "Epoch 208/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.3977 - acc: 0.8621 - val_loss: 0.4048 - val_acc: 0.8636\n",
            "Epoch 209/500\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.3961 - acc: 0.8621 - val_loss: 0.4033 - val_acc: 0.8636\n",
            "Epoch 210/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3946 - acc: 0.8621 - val_loss: 0.4017 - val_acc: 0.8636\n",
            "Epoch 211/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3930 - acc: 0.8621 - val_loss: 0.4002 - val_acc: 0.8636\n",
            "Epoch 212/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3914 - acc: 0.8621 - val_loss: 0.3987 - val_acc: 0.8636\n",
            "Epoch 213/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.3898 - acc: 0.8621 - val_loss: 0.3972 - val_acc: 0.8636\n",
            "Epoch 214/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3883 - acc: 0.8621 - val_loss: 0.3957 - val_acc: 0.8636\n",
            "Epoch 215/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3867 - acc: 0.8621 - val_loss: 0.3942 - val_acc: 0.8636\n",
            "Epoch 216/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.3852 - acc: 0.8621 - val_loss: 0.3928 - val_acc: 0.8636\n",
            "Epoch 217/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3837 - acc: 0.8621 - val_loss: 0.3913 - val_acc: 0.8636\n",
            "Epoch 218/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3821 - acc: 0.8621 - val_loss: 0.3898 - val_acc: 0.8636\n",
            "Epoch 219/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3806 - acc: 0.8621 - val_loss: 0.3884 - val_acc: 0.8636\n",
            "Epoch 220/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.3791 - acc: 0.8621 - val_loss: 0.3870 - val_acc: 0.8636\n",
            "Epoch 221/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3776 - acc: 0.8621 - val_loss: 0.3857 - val_acc: 0.8636\n",
            "Epoch 222/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3762 - acc: 0.8621 - val_loss: 0.3843 - val_acc: 0.8636\n",
            "Epoch 223/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.3747 - acc: 0.8621 - val_loss: 0.3829 - val_acc: 0.8636\n",
            "Epoch 224/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3733 - acc: 0.8621 - val_loss: 0.3815 - val_acc: 0.8636\n",
            "Epoch 225/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.3719 - acc: 0.8621 - val_loss: 0.3802 - val_acc: 0.8636\n",
            "Epoch 226/500\n",
            "87/87 [==============================] - 0s 316us/step - loss: 0.3706 - acc: 0.8621 - val_loss: 0.3788 - val_acc: 0.8636\n",
            "Epoch 227/500\n",
            "87/87 [==============================] - 0s 355us/step - loss: 0.3692 - acc: 0.8621 - val_loss: 0.3775 - val_acc: 0.8636\n",
            "Epoch 228/500\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.3678 - acc: 0.8621 - val_loss: 0.3762 - val_acc: 0.8636\n",
            "Epoch 229/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.3664 - acc: 0.8621 - val_loss: 0.3749 - val_acc: 0.8636\n",
            "Epoch 230/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.3651 - acc: 0.8621 - val_loss: 0.3736 - val_acc: 0.8636\n",
            "Epoch 231/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3637 - acc: 0.8621 - val_loss: 0.3723 - val_acc: 0.8636\n",
            "Epoch 232/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.3624 - acc: 0.8621 - val_loss: 0.3710 - val_acc: 0.8636\n",
            "Epoch 233/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3611 - acc: 0.8621 - val_loss: 0.3697 - val_acc: 0.8636\n",
            "Epoch 234/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3597 - acc: 0.8621 - val_loss: 0.3685 - val_acc: 0.8636\n",
            "Epoch 235/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3584 - acc: 0.8621 - val_loss: 0.3673 - val_acc: 0.8636\n",
            "Epoch 236/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.3571 - acc: 0.8621 - val_loss: 0.3660 - val_acc: 0.8636\n",
            "Epoch 237/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3558 - acc: 0.8621 - val_loss: 0.3648 - val_acc: 0.8636\n",
            "Epoch 238/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3545 - acc: 0.8621 - val_loss: 0.3636 - val_acc: 0.8636\n",
            "Epoch 239/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3532 - acc: 0.8621 - val_loss: 0.3624 - val_acc: 0.8636\n",
            "Epoch 240/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3520 - acc: 0.8621 - val_loss: 0.3612 - val_acc: 0.8636\n",
            "Epoch 241/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.3507 - acc: 0.8621 - val_loss: 0.3601 - val_acc: 0.8636\n",
            "Epoch 242/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3496 - acc: 0.8621 - val_loss: 0.3589 - val_acc: 0.8636\n",
            "Epoch 243/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.3483 - acc: 0.8621 - val_loss: 0.3578 - val_acc: 0.8636\n",
            "Epoch 244/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.3471 - acc: 0.8621 - val_loss: 0.3566 - val_acc: 0.8636\n",
            "Epoch 245/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3459 - acc: 0.8621 - val_loss: 0.3555 - val_acc: 0.8636\n",
            "Epoch 246/500\n",
            "87/87 [==============================] - 0s 313us/step - loss: 0.3447 - acc: 0.8621 - val_loss: 0.3543 - val_acc: 0.8636\n",
            "Epoch 247/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.3435 - acc: 0.8621 - val_loss: 0.3532 - val_acc: 0.8636\n",
            "Epoch 248/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3424 - acc: 0.8621 - val_loss: 0.3521 - val_acc: 0.8636\n",
            "Epoch 249/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.3412 - acc: 0.8621 - val_loss: 0.3510 - val_acc: 0.8636\n",
            "Epoch 250/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3401 - acc: 0.8621 - val_loss: 0.3499 - val_acc: 0.8636\n",
            "Epoch 251/500\n",
            "87/87 [==============================] - 0s 313us/step - loss: 0.3389 - acc: 0.8621 - val_loss: 0.3488 - val_acc: 0.8636\n",
            "Epoch 252/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.3378 - acc: 0.8621 - val_loss: 0.3477 - val_acc: 0.8636\n",
            "Epoch 253/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3367 - acc: 0.8621 - val_loss: 0.3466 - val_acc: 0.8636\n",
            "Epoch 254/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3356 - acc: 0.8621 - val_loss: 0.3456 - val_acc: 0.8636\n",
            "Epoch 255/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3344 - acc: 0.8621 - val_loss: 0.3445 - val_acc: 0.8636\n",
            "Epoch 256/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.3333 - acc: 0.8621 - val_loss: 0.3434 - val_acc: 0.8636\n",
            "Epoch 257/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3322 - acc: 0.8621 - val_loss: 0.3424 - val_acc: 0.8636\n",
            "Epoch 258/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3311 - acc: 0.8621 - val_loss: 0.3413 - val_acc: 0.8636\n",
            "Epoch 259/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3300 - acc: 0.8621 - val_loss: 0.3403 - val_acc: 0.8636\n",
            "Epoch 260/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.3290 - acc: 0.8621 - val_loss: 0.3393 - val_acc: 0.8636\n",
            "Epoch 261/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.3279 - acc: 0.8621 - val_loss: 0.3382 - val_acc: 0.8636\n",
            "Epoch 262/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3269 - acc: 0.8621 - val_loss: 0.3372 - val_acc: 0.8636\n",
            "Epoch 263/500\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.3258 - acc: 0.8621 - val_loss: 0.3363 - val_acc: 0.8636\n",
            "Epoch 264/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.3248 - acc: 0.8621 - val_loss: 0.3353 - val_acc: 0.8636\n",
            "Epoch 265/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.3237 - acc: 0.8621 - val_loss: 0.3343 - val_acc: 0.8636\n",
            "Epoch 266/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3227 - acc: 0.8621 - val_loss: 0.3333 - val_acc: 0.8636\n",
            "Epoch 267/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3217 - acc: 0.8621 - val_loss: 0.3323 - val_acc: 0.8636\n",
            "Epoch 268/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3207 - acc: 0.8621 - val_loss: 0.3314 - val_acc: 0.8636\n",
            "Epoch 269/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3197 - acc: 0.8621 - val_loss: 0.3304 - val_acc: 0.8636\n",
            "Epoch 270/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3187 - acc: 0.8621 - val_loss: 0.3295 - val_acc: 0.8636\n",
            "Epoch 271/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3178 - acc: 0.8621 - val_loss: 0.3286 - val_acc: 0.8636\n",
            "Epoch 272/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.3168 - acc: 0.8621 - val_loss: 0.3277 - val_acc: 0.8636\n",
            "Epoch 273/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3158 - acc: 0.8621 - val_loss: 0.3267 - val_acc: 0.8636\n",
            "Epoch 274/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3149 - acc: 0.8621 - val_loss: 0.3258 - val_acc: 0.8636\n",
            "Epoch 275/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3139 - acc: 0.8621 - val_loss: 0.3249 - val_acc: 0.8636\n",
            "Epoch 276/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.3130 - acc: 0.8621 - val_loss: 0.3240 - val_acc: 0.8636\n",
            "Epoch 277/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3120 - acc: 0.8621 - val_loss: 0.3231 - val_acc: 0.8636\n",
            "Epoch 278/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3111 - acc: 0.8621 - val_loss: 0.3222 - val_acc: 0.8636\n",
            "Epoch 279/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3102 - acc: 0.8621 - val_loss: 0.3213 - val_acc: 0.8636\n",
            "Epoch 280/500\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.3092 - acc: 0.8621 - val_loss: 0.3205 - val_acc: 0.8636\n",
            "Epoch 281/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3083 - acc: 0.8621 - val_loss: 0.3196 - val_acc: 0.8636\n",
            "Epoch 282/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.3074 - acc: 0.8621 - val_loss: 0.3187 - val_acc: 0.8636\n",
            "Epoch 283/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3065 - acc: 0.8621 - val_loss: 0.3178 - val_acc: 0.8636\n",
            "Epoch 284/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.3056 - acc: 0.8621 - val_loss: 0.3170 - val_acc: 0.8636\n",
            "Epoch 285/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3047 - acc: 0.8621 - val_loss: 0.3161 - val_acc: 0.8636\n",
            "Epoch 286/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3038 - acc: 0.8621 - val_loss: 0.3153 - val_acc: 0.8636\n",
            "Epoch 287/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3029 - acc: 0.8621 - val_loss: 0.3144 - val_acc: 0.8636\n",
            "Epoch 288/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3020 - acc: 0.8621 - val_loss: 0.3136 - val_acc: 0.8636\n",
            "Epoch 289/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.3011 - acc: 0.8621 - val_loss: 0.3128 - val_acc: 0.8636\n",
            "Epoch 290/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3003 - acc: 0.8621 - val_loss: 0.3119 - val_acc: 0.8636\n",
            "Epoch 291/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.2994 - acc: 0.8621 - val_loss: 0.3111 - val_acc: 0.8636\n",
            "Epoch 292/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2985 - acc: 0.8621 - val_loss: 0.3103 - val_acc: 0.8636\n",
            "Epoch 293/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2976 - acc: 0.8621 - val_loss: 0.3095 - val_acc: 0.8636\n",
            "Epoch 294/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2968 - acc: 0.8621 - val_loss: 0.3087 - val_acc: 0.8636\n",
            "Epoch 295/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2959 - acc: 0.8621 - val_loss: 0.3078 - val_acc: 0.8636\n",
            "Epoch 296/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2951 - acc: 0.8621 - val_loss: 0.3070 - val_acc: 0.8636\n",
            "Epoch 297/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.2942 - acc: 0.8621 - val_loss: 0.3062 - val_acc: 0.8636\n",
            "Epoch 298/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2934 - acc: 0.8621 - val_loss: 0.3054 - val_acc: 0.8636\n",
            "Epoch 299/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2925 - acc: 0.8621 - val_loss: 0.3046 - val_acc: 0.8636\n",
            "Epoch 300/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2917 - acc: 0.8621 - val_loss: 0.3038 - val_acc: 0.8636\n",
            "Epoch 301/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.2908 - acc: 0.8621 - val_loss: 0.3031 - val_acc: 0.8636\n",
            "Epoch 302/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.2900 - acc: 0.8621 - val_loss: 0.3023 - val_acc: 0.8636\n",
            "Epoch 303/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2892 - acc: 0.8621 - val_loss: 0.3015 - val_acc: 0.8636\n",
            "Epoch 304/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2883 - acc: 0.8621 - val_loss: 0.3007 - val_acc: 0.8636\n",
            "Epoch 305/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2875 - acc: 0.8621 - val_loss: 0.2999 - val_acc: 0.8636\n",
            "Epoch 306/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2867 - acc: 0.8621 - val_loss: 0.2992 - val_acc: 0.8636\n",
            "Epoch 307/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2859 - acc: 0.8621 - val_loss: 0.2984 - val_acc: 0.8636\n",
            "Epoch 308/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2851 - acc: 0.8621 - val_loss: 0.2976 - val_acc: 0.8636\n",
            "Epoch 309/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2843 - acc: 0.8621 - val_loss: 0.2969 - val_acc: 0.8636\n",
            "Epoch 310/500\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.2834 - acc: 0.8621 - val_loss: 0.2961 - val_acc: 0.8636\n",
            "Epoch 311/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2826 - acc: 0.8621 - val_loss: 0.2954 - val_acc: 0.8636\n",
            "Epoch 312/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2818 - acc: 0.8621 - val_loss: 0.2946 - val_acc: 0.8636\n",
            "Epoch 313/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2810 - acc: 0.8621 - val_loss: 0.2939 - val_acc: 0.8636\n",
            "Epoch 314/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2803 - acc: 0.8621 - val_loss: 0.2931 - val_acc: 0.8636\n",
            "Epoch 315/500\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.2795 - acc: 0.8621 - val_loss: 0.2923 - val_acc: 0.8636\n",
            "Epoch 316/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.2787 - acc: 0.8621 - val_loss: 0.2916 - val_acc: 0.8636\n",
            "Epoch 317/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2779 - acc: 0.8621 - val_loss: 0.2909 - val_acc: 0.8636\n",
            "Epoch 318/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.2771 - acc: 0.8621 - val_loss: 0.2902 - val_acc: 0.8636\n",
            "Epoch 319/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2764 - acc: 0.8621 - val_loss: 0.2895 - val_acc: 0.8636\n",
            "Epoch 320/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2756 - acc: 0.8621 - val_loss: 0.2887 - val_acc: 0.8636\n",
            "Epoch 321/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2748 - acc: 0.8621 - val_loss: 0.2880 - val_acc: 0.8636\n",
            "Epoch 322/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2741 - acc: 0.8621 - val_loss: 0.2873 - val_acc: 0.8636\n",
            "Epoch 323/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2733 - acc: 0.8621 - val_loss: 0.2866 - val_acc: 0.8636\n",
            "Epoch 324/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2726 - acc: 0.8621 - val_loss: 0.2859 - val_acc: 0.8636\n",
            "Epoch 325/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2718 - acc: 0.8621 - val_loss: 0.2851 - val_acc: 0.8636\n",
            "Epoch 326/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2711 - acc: 0.8621 - val_loss: 0.2844 - val_acc: 0.8636\n",
            "Epoch 327/500\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2704 - acc: 0.8621 - val_loss: 0.2837 - val_acc: 0.8636\n",
            "Epoch 328/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2696 - acc: 0.8621 - val_loss: 0.2830 - val_acc: 0.8636\n",
            "Epoch 329/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.2688 - acc: 0.8621 - val_loss: 0.2823 - val_acc: 0.8636\n",
            "Epoch 330/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2681 - acc: 0.8621 - val_loss: 0.2816 - val_acc: 0.8636\n",
            "Epoch 331/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2674 - acc: 0.8621 - val_loss: 0.2809 - val_acc: 0.8636\n",
            "Epoch 332/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2666 - acc: 0.8621 - val_loss: 0.2802 - val_acc: 0.8636\n",
            "Epoch 333/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.2659 - acc: 0.8621 - val_loss: 0.2795 - val_acc: 0.8636\n",
            "Epoch 334/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2652 - acc: 0.8621 - val_loss: 0.2788 - val_acc: 0.8636\n",
            "Epoch 335/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2644 - acc: 0.8621 - val_loss: 0.2781 - val_acc: 0.8636\n",
            "Epoch 336/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2637 - acc: 0.8621 - val_loss: 0.2775 - val_acc: 0.8636\n",
            "Epoch 337/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2630 - acc: 0.8621 - val_loss: 0.2768 - val_acc: 0.8636\n",
            "Epoch 338/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2623 - acc: 0.8621 - val_loss: 0.2761 - val_acc: 0.8636\n",
            "Epoch 339/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2616 - acc: 0.8621 - val_loss: 0.2754 - val_acc: 0.8636\n",
            "Epoch 340/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2608 - acc: 0.8621 - val_loss: 0.2747 - val_acc: 0.8636\n",
            "Epoch 341/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2601 - acc: 0.8621 - val_loss: 0.2740 - val_acc: 0.8636\n",
            "Epoch 342/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2594 - acc: 0.8621 - val_loss: 0.2734 - val_acc: 0.8636\n",
            "Epoch 343/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2587 - acc: 0.8621 - val_loss: 0.2727 - val_acc: 0.8636\n",
            "Epoch 344/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.2580 - acc: 0.8621 - val_loss: 0.2720 - val_acc: 0.8636\n",
            "Epoch 345/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2572 - acc: 0.8621 - val_loss: 0.2714 - val_acc: 0.8636\n",
            "Epoch 346/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2565 - acc: 0.8621 - val_loss: 0.2707 - val_acc: 0.8636\n",
            "Epoch 347/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2558 - acc: 0.8621 - val_loss: 0.2700 - val_acc: 0.8636\n",
            "Epoch 348/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2551 - acc: 0.8621 - val_loss: 0.2694 - val_acc: 0.8636\n",
            "Epoch 349/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.2544 - acc: 0.8621 - val_loss: 0.2687 - val_acc: 0.8636\n",
            "Epoch 350/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2537 - acc: 0.8621 - val_loss: 0.2681 - val_acc: 0.8636\n",
            "Epoch 351/500\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.2530 - acc: 0.8621 - val_loss: 0.2674 - val_acc: 0.8636\n",
            "Epoch 352/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2524 - acc: 0.8621 - val_loss: 0.2667 - val_acc: 0.8636\n",
            "Epoch 353/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2517 - acc: 0.8621 - val_loss: 0.2661 - val_acc: 0.8636\n",
            "Epoch 354/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2510 - acc: 0.8621 - val_loss: 0.2654 - val_acc: 0.8636\n",
            "Epoch 355/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.2503 - acc: 0.8621 - val_loss: 0.2648 - val_acc: 0.8636\n",
            "Epoch 356/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.2496 - acc: 0.8621 - val_loss: 0.2642 - val_acc: 0.8636\n",
            "Epoch 357/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2489 - acc: 0.8621 - val_loss: 0.2635 - val_acc: 0.8636\n",
            "Epoch 358/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2483 - acc: 0.8621 - val_loss: 0.2628 - val_acc: 0.8636\n",
            "Epoch 359/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2476 - acc: 0.8621 - val_loss: 0.2622 - val_acc: 0.8636\n",
            "Epoch 360/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2469 - acc: 0.8621 - val_loss: 0.2616 - val_acc: 0.8636\n",
            "Epoch 361/500\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.2462 - acc: 0.8621 - val_loss: 0.2609 - val_acc: 0.8636\n",
            "Epoch 362/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2455 - acc: 0.8621 - val_loss: 0.2603 - val_acc: 0.8636\n",
            "Epoch 363/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2449 - acc: 0.8621 - val_loss: 0.2597 - val_acc: 0.8636\n",
            "Epoch 364/500\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.2442 - acc: 0.8621 - val_loss: 0.2590 - val_acc: 0.8636\n",
            "Epoch 365/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2435 - acc: 0.8621 - val_loss: 0.2584 - val_acc: 0.8636\n",
            "Epoch 366/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2429 - acc: 0.8621 - val_loss: 0.2578 - val_acc: 0.8636\n",
            "Epoch 367/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2422 - acc: 0.8621 - val_loss: 0.2571 - val_acc: 0.8636\n",
            "Epoch 368/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2415 - acc: 0.8621 - val_loss: 0.2564 - val_acc: 0.8636\n",
            "Epoch 369/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2408 - acc: 0.8621 - val_loss: 0.2558 - val_acc: 0.8636\n",
            "Epoch 370/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2401 - acc: 0.8621 - val_loss: 0.2552 - val_acc: 0.8636\n",
            "Epoch 371/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.2395 - acc: 0.8621 - val_loss: 0.2545 - val_acc: 0.8636\n",
            "Epoch 372/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2388 - acc: 0.8621 - val_loss: 0.2539 - val_acc: 0.8636\n",
            "Epoch 373/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2381 - acc: 0.8621 - val_loss: 0.2533 - val_acc: 0.8636\n",
            "Epoch 374/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2374 - acc: 0.8621 - val_loss: 0.2526 - val_acc: 0.8636\n",
            "Epoch 375/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2368 - acc: 0.8621 - val_loss: 0.2520 - val_acc: 0.8636\n",
            "Epoch 376/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2361 - acc: 0.8621 - val_loss: 0.2513 - val_acc: 0.8636\n",
            "Epoch 377/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2354 - acc: 0.8621 - val_loss: 0.2507 - val_acc: 0.8636\n",
            "Epoch 378/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2348 - acc: 0.8621 - val_loss: 0.2501 - val_acc: 0.8636\n",
            "Epoch 379/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2341 - acc: 0.8621 - val_loss: 0.2494 - val_acc: 0.8636\n",
            "Epoch 380/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2334 - acc: 0.8621 - val_loss: 0.2488 - val_acc: 0.8636\n",
            "Epoch 381/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2327 - acc: 0.8621 - val_loss: 0.2482 - val_acc: 0.8636\n",
            "Epoch 382/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2321 - acc: 0.8621 - val_loss: 0.2475 - val_acc: 0.8636\n",
            "Epoch 383/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2314 - acc: 0.8621 - val_loss: 0.2469 - val_acc: 0.8636\n",
            "Epoch 384/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2308 - acc: 0.8621 - val_loss: 0.2463 - val_acc: 0.8636\n",
            "Epoch 385/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2301 - acc: 0.8621 - val_loss: 0.2457 - val_acc: 0.8636\n",
            "Epoch 386/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2294 - acc: 0.8621 - val_loss: 0.2451 - val_acc: 0.8636\n",
            "Epoch 387/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2287 - acc: 0.8736 - val_loss: 0.2444 - val_acc: 0.8636\n",
            "Epoch 388/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.2281 - acc: 0.8736 - val_loss: 0.2438 - val_acc: 0.8636\n",
            "Epoch 389/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2274 - acc: 0.8736 - val_loss: 0.2431 - val_acc: 0.8636\n",
            "Epoch 390/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2267 - acc: 0.8851 - val_loss: 0.2426 - val_acc: 0.8636\n",
            "Epoch 391/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2261 - acc: 0.8851 - val_loss: 0.2420 - val_acc: 0.8636\n",
            "Epoch 392/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2254 - acc: 0.8966 - val_loss: 0.2413 - val_acc: 0.8636\n",
            "Epoch 393/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2247 - acc: 0.8966 - val_loss: 0.2406 - val_acc: 0.8636\n",
            "Epoch 394/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2241 - acc: 0.8966 - val_loss: 0.2400 - val_acc: 0.8636\n",
            "Epoch 395/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2234 - acc: 0.8966 - val_loss: 0.2394 - val_acc: 0.8636\n",
            "Epoch 396/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2227 - acc: 0.9080 - val_loss: 0.2388 - val_acc: 0.8636\n",
            "Epoch 397/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.2220 - acc: 0.9195 - val_loss: 0.2381 - val_acc: 0.8636\n",
            "Epoch 398/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2213 - acc: 0.9195 - val_loss: 0.2375 - val_acc: 0.8636\n",
            "Epoch 399/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2207 - acc: 0.9195 - val_loss: 0.2369 - val_acc: 0.8636\n",
            "Epoch 400/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2200 - acc: 0.9195 - val_loss: 0.2363 - val_acc: 0.8636\n",
            "Epoch 401/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2194 - acc: 0.9195 - val_loss: 0.2356 - val_acc: 0.8636\n",
            "Epoch 402/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2187 - acc: 0.9195 - val_loss: 0.2351 - val_acc: 0.8864\n",
            "Epoch 403/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2180 - acc: 0.9195 - val_loss: 0.2344 - val_acc: 0.9091\n",
            "Epoch 404/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.2174 - acc: 0.9195 - val_loss: 0.2338 - val_acc: 0.9091\n",
            "Epoch 405/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2167 - acc: 0.9195 - val_loss: 0.2332 - val_acc: 0.9091\n",
            "Epoch 406/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2160 - acc: 0.9195 - val_loss: 0.2326 - val_acc: 0.9091\n",
            "Epoch 407/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2153 - acc: 0.9195 - val_loss: 0.2320 - val_acc: 0.9091\n",
            "Epoch 408/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2147 - acc: 0.9310 - val_loss: 0.2313 - val_acc: 0.9091\n",
            "Epoch 409/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2140 - acc: 0.9425 - val_loss: 0.2307 - val_acc: 0.9091\n",
            "Epoch 410/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.2133 - acc: 0.9425 - val_loss: 0.2301 - val_acc: 0.9091\n",
            "Epoch 411/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2126 - acc: 0.9425 - val_loss: 0.2295 - val_acc: 0.9091\n",
            "Epoch 412/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2120 - acc: 0.9425 - val_loss: 0.2289 - val_acc: 0.9091\n",
            "Epoch 413/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2113 - acc: 0.9425 - val_loss: 0.2282 - val_acc: 0.9091\n",
            "Epoch 414/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2106 - acc: 0.9425 - val_loss: 0.2276 - val_acc: 0.9091\n",
            "Epoch 415/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2100 - acc: 0.9425 - val_loss: 0.2270 - val_acc: 0.9091\n",
            "Epoch 416/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.2093 - acc: 0.9425 - val_loss: 0.2263 - val_acc: 0.9318\n",
            "Epoch 417/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2086 - acc: 0.9425 - val_loss: 0.2257 - val_acc: 0.9318\n",
            "Epoch 418/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2080 - acc: 0.9425 - val_loss: 0.2251 - val_acc: 0.9318\n",
            "Epoch 419/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2073 - acc: 0.9425 - val_loss: 0.2245 - val_acc: 0.9318\n",
            "Epoch 420/500\n",
            "87/87 [==============================] - 0s 307us/step - loss: 0.2066 - acc: 0.9540 - val_loss: 0.2239 - val_acc: 0.9318\n",
            "Epoch 421/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2059 - acc: 0.9540 - val_loss: 0.2233 - val_acc: 0.9318\n",
            "Epoch 422/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2053 - acc: 0.9540 - val_loss: 0.2226 - val_acc: 0.9318\n",
            "Epoch 423/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2046 - acc: 0.9540 - val_loss: 0.2220 - val_acc: 0.9318\n",
            "Epoch 424/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.2039 - acc: 0.9540 - val_loss: 0.2214 - val_acc: 0.9318\n",
            "Epoch 425/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2032 - acc: 0.9540 - val_loss: 0.2207 - val_acc: 0.9318\n",
            "Epoch 426/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2026 - acc: 0.9540 - val_loss: 0.2201 - val_acc: 0.9318\n",
            "Epoch 427/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2019 - acc: 0.9540 - val_loss: 0.2195 - val_acc: 0.9318\n",
            "Epoch 428/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2012 - acc: 0.9540 - val_loss: 0.2189 - val_acc: 0.9318\n",
            "Epoch 429/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2005 - acc: 0.9540 - val_loss: 0.2183 - val_acc: 0.9318\n",
            "Epoch 430/500\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.1999 - acc: 0.9540 - val_loss: 0.2176 - val_acc: 0.9318\n",
            "Epoch 431/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1992 - acc: 0.9540 - val_loss: 0.2170 - val_acc: 0.9318\n",
            "Epoch 432/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1985 - acc: 0.9540 - val_loss: 0.2164 - val_acc: 0.9318\n",
            "Epoch 433/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1978 - acc: 0.9540 - val_loss: 0.2157 - val_acc: 0.9318\n",
            "Epoch 434/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1972 - acc: 0.9540 - val_loss: 0.2151 - val_acc: 0.9318\n",
            "Epoch 435/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.1965 - acc: 0.9540 - val_loss: 0.2145 - val_acc: 0.9318\n",
            "Epoch 436/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1958 - acc: 0.9540 - val_loss: 0.2139 - val_acc: 0.9318\n",
            "Epoch 437/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1951 - acc: 0.9540 - val_loss: 0.2133 - val_acc: 0.9318\n",
            "Epoch 438/500\n",
            "87/87 [==============================] - 0s 303us/step - loss: 0.1945 - acc: 0.9540 - val_loss: 0.2126 - val_acc: 0.9318\n",
            "Epoch 439/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1938 - acc: 0.9540 - val_loss: 0.2120 - val_acc: 0.9318\n",
            "Epoch 440/500\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.1931 - acc: 0.9540 - val_loss: 0.2114 - val_acc: 0.9318\n",
            "Epoch 441/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1924 - acc: 0.9540 - val_loss: 0.2108 - val_acc: 0.9318\n",
            "Epoch 442/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1917 - acc: 0.9655 - val_loss: 0.2102 - val_acc: 0.9318\n",
            "Epoch 443/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.1911 - acc: 0.9655 - val_loss: 0.2095 - val_acc: 0.9318\n",
            "Epoch 444/500\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.1904 - acc: 0.9655 - val_loss: 0.2089 - val_acc: 0.9318\n",
            "Epoch 445/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.1897 - acc: 0.9655 - val_loss: 0.2083 - val_acc: 0.9318\n",
            "Epoch 446/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1890 - acc: 0.9655 - val_loss: 0.2076 - val_acc: 0.9318\n",
            "Epoch 447/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1884 - acc: 0.9655 - val_loss: 0.2070 - val_acc: 0.9318\n",
            "Epoch 448/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1877 - acc: 0.9655 - val_loss: 0.2064 - val_acc: 0.9318\n",
            "Epoch 449/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1870 - acc: 0.9655 - val_loss: 0.2058 - val_acc: 0.9318\n",
            "Epoch 450/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.1864 - acc: 0.9655 - val_loss: 0.2052 - val_acc: 0.9318\n",
            "Epoch 451/500\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.1857 - acc: 0.9655 - val_loss: 0.2045 - val_acc: 0.9318\n",
            "Epoch 452/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1850 - acc: 0.9655 - val_loss: 0.2039 - val_acc: 0.9318\n",
            "Epoch 453/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1843 - acc: 0.9655 - val_loss: 0.2033 - val_acc: 0.9318\n",
            "Epoch 454/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.1836 - acc: 0.9655 - val_loss: 0.2027 - val_acc: 0.9318\n",
            "Epoch 455/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1830 - acc: 0.9655 - val_loss: 0.2020 - val_acc: 0.9318\n",
            "Epoch 456/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1823 - acc: 0.9655 - val_loss: 0.2014 - val_acc: 0.9318\n",
            "Epoch 457/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1816 - acc: 0.9655 - val_loss: 0.2008 - val_acc: 0.9318\n",
            "Epoch 458/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1809 - acc: 0.9655 - val_loss: 0.2002 - val_acc: 0.9318\n",
            "Epoch 459/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1803 - acc: 0.9655 - val_loss: 0.1995 - val_acc: 0.9318\n",
            "Epoch 460/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1796 - acc: 0.9655 - val_loss: 0.1989 - val_acc: 0.9318\n",
            "Epoch 461/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.1789 - acc: 0.9655 - val_loss: 0.1982 - val_acc: 0.9318\n",
            "Epoch 462/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1782 - acc: 0.9655 - val_loss: 0.1976 - val_acc: 0.9318\n",
            "Epoch 463/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1776 - acc: 0.9655 - val_loss: 0.1970 - val_acc: 0.9318\n",
            "Epoch 464/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1769 - acc: 0.9655 - val_loss: 0.1964 - val_acc: 0.9318\n",
            "Epoch 465/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1762 - acc: 0.9655 - val_loss: 0.1957 - val_acc: 0.9318\n",
            "Epoch 466/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1755 - acc: 0.9655 - val_loss: 0.1951 - val_acc: 0.9318\n",
            "Epoch 467/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1748 - acc: 0.9655 - val_loss: 0.1945 - val_acc: 0.9318\n",
            "Epoch 468/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1742 - acc: 0.9655 - val_loss: 0.1939 - val_acc: 0.9318\n",
            "Epoch 469/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1735 - acc: 0.9655 - val_loss: 0.1932 - val_acc: 0.9318\n",
            "Epoch 470/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.1728 - acc: 0.9655 - val_loss: 0.1926 - val_acc: 0.9318\n",
            "Epoch 471/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1721 - acc: 0.9655 - val_loss: 0.1920 - val_acc: 0.9318\n",
            "Epoch 472/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1715 - acc: 0.9655 - val_loss: 0.1914 - val_acc: 0.9318\n",
            "Epoch 473/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1708 - acc: 0.9655 - val_loss: 0.1907 - val_acc: 0.9318\n",
            "Epoch 474/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1701 - acc: 0.9655 - val_loss: 0.1901 - val_acc: 0.9318\n",
            "Epoch 475/500\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.1695 - acc: 0.9655 - val_loss: 0.1895 - val_acc: 0.9318\n",
            "Epoch 476/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1688 - acc: 0.9655 - val_loss: 0.1889 - val_acc: 0.9318\n",
            "Epoch 477/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.1681 - acc: 0.9655 - val_loss: 0.1883 - val_acc: 0.9318\n",
            "Epoch 478/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1675 - acc: 0.9655 - val_loss: 0.1876 - val_acc: 0.9318\n",
            "Epoch 479/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1668 - acc: 0.9655 - val_loss: 0.1870 - val_acc: 0.9318\n",
            "Epoch 480/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.1661 - acc: 0.9655 - val_loss: 0.1864 - val_acc: 0.9545\n",
            "Epoch 481/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.1655 - acc: 0.9655 - val_loss: 0.1858 - val_acc: 0.9545\n",
            "Epoch 482/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1648 - acc: 0.9655 - val_loss: 0.1852 - val_acc: 0.9545\n",
            "Epoch 483/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1642 - acc: 0.9655 - val_loss: 0.1846 - val_acc: 0.9545\n",
            "Epoch 484/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1635 - acc: 0.9655 - val_loss: 0.1840 - val_acc: 0.9545\n",
            "Epoch 485/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1629 - acc: 0.9655 - val_loss: 0.1834 - val_acc: 0.9545\n",
            "Epoch 486/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1622 - acc: 0.9655 - val_loss: 0.1828 - val_acc: 0.9773\n",
            "Epoch 487/500\n",
            "87/87 [==============================] - 0s 194us/step - loss: 0.1615 - acc: 0.9655 - val_loss: 0.1821 - val_acc: 0.9773\n",
            "Epoch 488/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1609 - acc: 0.9655 - val_loss: 0.1816 - val_acc: 0.9773\n",
            "Epoch 489/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1602 - acc: 0.9655 - val_loss: 0.1809 - val_acc: 0.9773\n",
            "Epoch 490/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.1596 - acc: 0.9655 - val_loss: 0.1803 - val_acc: 0.9773\n",
            "Epoch 491/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.1589 - acc: 0.9655 - val_loss: 0.1797 - val_acc: 0.9773\n",
            "Epoch 492/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1583 - acc: 0.9655 - val_loss: 0.1792 - val_acc: 0.9773\n",
            "Epoch 493/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1576 - acc: 0.9655 - val_loss: 0.1786 - val_acc: 0.9773\n",
            "Epoch 494/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.1570 - acc: 0.9655 - val_loss: 0.1780 - val_acc: 0.9773\n",
            "Epoch 495/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.1563 - acc: 0.9655 - val_loss: 0.1773 - val_acc: 0.9773\n",
            "Epoch 496/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1557 - acc: 0.9770 - val_loss: 0.1768 - val_acc: 0.9773\n",
            "Epoch 497/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.1550 - acc: 0.9770 - val_loss: 0.1762 - val_acc: 0.9773\n",
            "Epoch 498/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.1544 - acc: 0.9770 - val_loss: 0.1756 - val_acc: 0.9773\n",
            "Epoch 499/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1537 - acc: 0.9770 - val_loss: 0.1750 - val_acc: 0.9773\n",
            "Epoch 500/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1532 - acc: 0.9770 - val_loss: 0.1744 - val_acc: 0.9773\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/500\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 0.9991 - acc: 0.8182 - val_loss: 0.9851 - val_acc: 0.9302\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.9909 - acc: 0.8864 - val_loss: 0.9759 - val_acc: 0.9767\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.9828 - acc: 0.9091 - val_loss: 0.9675 - val_acc: 0.9767\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.9751 - acc: 0.9318 - val_loss: 0.9588 - val_acc: 0.9767\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.9676 - acc: 0.9432 - val_loss: 0.9506 - val_acc: 0.9767\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9602 - acc: 0.9773 - val_loss: 0.9429 - val_acc: 1.0000\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.9533 - acc: 0.9773 - val_loss: 0.9357 - val_acc: 1.0000\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9466 - acc: 0.9773 - val_loss: 0.9288 - val_acc: 1.0000\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.9401 - acc: 0.9886 - val_loss: 0.9215 - val_acc: 1.0000\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.9338 - acc: 0.9886 - val_loss: 0.9145 - val_acc: 1.0000\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.9275 - acc: 0.9886 - val_loss: 0.9078 - val_acc: 1.0000\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.9214 - acc: 0.9886 - val_loss: 0.9015 - val_acc: 1.0000\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9155 - acc: 0.9886 - val_loss: 0.8953 - val_acc: 1.0000\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.9097 - acc: 0.9886 - val_loss: 0.8892 - val_acc: 1.0000\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9039 - acc: 0.9886 - val_loss: 0.8832 - val_acc: 1.0000\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8983 - acc: 0.9886 - val_loss: 0.8774 - val_acc: 1.0000\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8929 - acc: 0.9886 - val_loss: 0.8716 - val_acc: 0.9767\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.8875 - acc: 0.9886 - val_loss: 0.8659 - val_acc: 0.9767\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8821 - acc: 0.9886 - val_loss: 0.8604 - val_acc: 0.9767\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8770 - acc: 0.9886 - val_loss: 0.8550 - val_acc: 0.9767\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8718 - acc: 0.9886 - val_loss: 0.8496 - val_acc: 0.9767\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 0s 290us/step - loss: 0.8668 - acc: 0.9886 - val_loss: 0.8442 - val_acc: 0.9767\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.8618 - acc: 0.9886 - val_loss: 0.8392 - val_acc: 0.9767\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8570 - acc: 0.9886 - val_loss: 0.8341 - val_acc: 0.9767\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8522 - acc: 0.9886 - val_loss: 0.8290 - val_acc: 0.9767\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8474 - acc: 0.9886 - val_loss: 0.8241 - val_acc: 0.9767\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8428 - acc: 0.9886 - val_loss: 0.8193 - val_acc: 0.9535\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8382 - acc: 0.9886 - val_loss: 0.8145 - val_acc: 0.9535\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8337 - acc: 0.9886 - val_loss: 0.8097 - val_acc: 0.9535\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8293 - acc: 0.9886 - val_loss: 0.8050 - val_acc: 0.9535\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8248 - acc: 0.9886 - val_loss: 0.8004 - val_acc: 0.9535\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.8204 - acc: 0.9886 - val_loss: 0.7957 - val_acc: 0.9302\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8160 - acc: 0.9886 - val_loss: 0.7912 - val_acc: 0.9302\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8118 - acc: 0.9773 - val_loss: 0.7868 - val_acc: 0.9302\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8075 - acc: 0.9773 - val_loss: 0.7824 - val_acc: 0.9302\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8033 - acc: 0.9659 - val_loss: 0.7780 - val_acc: 0.9302\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.7991 - acc: 0.9659 - val_loss: 0.7737 - val_acc: 0.9302\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7950 - acc: 0.9659 - val_loss: 0.7694 - val_acc: 0.9302\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7909 - acc: 0.9659 - val_loss: 0.7651 - val_acc: 0.9302\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7869 - acc: 0.9659 - val_loss: 0.7609 - val_acc: 0.9302\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7829 - acc: 0.9659 - val_loss: 0.7568 - val_acc: 0.9302\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7789 - acc: 0.9545 - val_loss: 0.7527 - val_acc: 0.9302\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7750 - acc: 0.9545 - val_loss: 0.7487 - val_acc: 0.9302\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.7712 - acc: 0.9545 - val_loss: 0.7446 - val_acc: 0.9302\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7672 - acc: 0.9545 - val_loss: 0.7407 - val_acc: 0.9302\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.7634 - acc: 0.9545 - val_loss: 0.7369 - val_acc: 0.9302\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7596 - acc: 0.9545 - val_loss: 0.7329 - val_acc: 0.9302\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.7559 - acc: 0.9545 - val_loss: 0.7291 - val_acc: 0.9302\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.7521 - acc: 0.9545 - val_loss: 0.7254 - val_acc: 0.9302\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.7485 - acc: 0.9545 - val_loss: 0.7217 - val_acc: 0.9302\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.7449 - acc: 0.9545 - val_loss: 0.7179 - val_acc: 0.9302\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.7413 - acc: 0.9545 - val_loss: 0.7143 - val_acc: 0.9302\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.7377 - acc: 0.9545 - val_loss: 0.7107 - val_acc: 0.9302\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.7342 - acc: 0.9545 - val_loss: 0.7071 - val_acc: 0.9302\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7307 - acc: 0.9545 - val_loss: 0.7036 - val_acc: 0.9302\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.7273 - acc: 0.9545 - val_loss: 0.7001 - val_acc: 0.9302\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7239 - acc: 0.9545 - val_loss: 0.6967 - val_acc: 0.9302\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7205 - acc: 0.9545 - val_loss: 0.6932 - val_acc: 0.9302\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.7172 - acc: 0.9545 - val_loss: 0.6899 - val_acc: 0.9302\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.7139 - acc: 0.9545 - val_loss: 0.6865 - val_acc: 0.9302\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7106 - acc: 0.9545 - val_loss: 0.6832 - val_acc: 0.9302\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.7074 - acc: 0.9545 - val_loss: 0.6799 - val_acc: 0.9302\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.7042 - acc: 0.9545 - val_loss: 0.6767 - val_acc: 0.9302\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7010 - acc: 0.9545 - val_loss: 0.6735 - val_acc: 0.9302\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.6978 - acc: 0.9545 - val_loss: 0.6703 - val_acc: 0.9302\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6947 - acc: 0.9545 - val_loss: 0.6672 - val_acc: 0.9302\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6916 - acc: 0.9545 - val_loss: 0.6640 - val_acc: 0.9302\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.6885 - acc: 0.9545 - val_loss: 0.6609 - val_acc: 0.9302\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.6855 - acc: 0.9545 - val_loss: 0.6578 - val_acc: 0.9302\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6824 - acc: 0.9545 - val_loss: 0.6548 - val_acc: 0.9302\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.6794 - acc: 0.9545 - val_loss: 0.6518 - val_acc: 0.9302\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.6765 - acc: 0.9545 - val_loss: 0.6488 - val_acc: 0.9302\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6735 - acc: 0.9545 - val_loss: 0.6458 - val_acc: 0.9302\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.6705 - acc: 0.9545 - val_loss: 0.6429 - val_acc: 0.9302\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.6676 - acc: 0.9545 - val_loss: 0.6399 - val_acc: 0.9302\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.6647 - acc: 0.9545 - val_loss: 0.6371 - val_acc: 0.9302\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6617 - acc: 0.9545 - val_loss: 0.6342 - val_acc: 0.9302\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.6589 - acc: 0.9545 - val_loss: 0.6313 - val_acc: 0.9302\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.6560 - acc: 0.9545 - val_loss: 0.6284 - val_acc: 0.9302\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.6531 - acc: 0.9545 - val_loss: 0.6256 - val_acc: 0.9302\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.6503 - acc: 0.9545 - val_loss: 0.6228 - val_acc: 0.9302\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.6475 - acc: 0.9545 - val_loss: 0.6200 - val_acc: 0.9302\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.6447 - acc: 0.9545 - val_loss: 0.6172 - val_acc: 0.9302\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.6419 - acc: 0.9545 - val_loss: 0.6144 - val_acc: 0.9302\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.6392 - acc: 0.9545 - val_loss: 0.6117 - val_acc: 0.9302\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.6364 - acc: 0.9545 - val_loss: 0.6089 - val_acc: 0.9302\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.6336 - acc: 0.9545 - val_loss: 0.6062 - val_acc: 0.9302\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6310 - acc: 0.9545 - val_loss: 0.6035 - val_acc: 0.9302\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.6282 - acc: 0.9659 - val_loss: 0.6008 - val_acc: 0.9302\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.6256 - acc: 0.9545 - val_loss: 0.5981 - val_acc: 0.9302\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.6229 - acc: 0.9659 - val_loss: 0.5955 - val_acc: 0.9302\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.6202 - acc: 0.9659 - val_loss: 0.5928 - val_acc: 0.9302\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.6175 - acc: 0.9659 - val_loss: 0.5901 - val_acc: 0.9302\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.6149 - acc: 0.9659 - val_loss: 0.5875 - val_acc: 0.9302\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.6123 - acc: 0.9659 - val_loss: 0.5849 - val_acc: 0.9302\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.6097 - acc: 0.9659 - val_loss: 0.5823 - val_acc: 0.9302\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.6070 - acc: 0.9659 - val_loss: 0.5797 - val_acc: 0.9302\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.6045 - acc: 0.9659 - val_loss: 0.5771 - val_acc: 0.9302\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.6019 - acc: 0.9659 - val_loss: 0.5746 - val_acc: 0.9302\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5993 - acc: 0.9659 - val_loss: 0.5720 - val_acc: 0.9302\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5968 - acc: 0.9773 - val_loss: 0.5695 - val_acc: 0.9302\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.5943 - acc: 0.9773 - val_loss: 0.5670 - val_acc: 0.9302\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.5917 - acc: 0.9773 - val_loss: 0.5645 - val_acc: 0.9302\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.5892 - acc: 0.9773 - val_loss: 0.5620 - val_acc: 0.9302\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.5867 - acc: 0.9773 - val_loss: 0.5595 - val_acc: 0.9302\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.5842 - acc: 0.9886 - val_loss: 0.5571 - val_acc: 0.9302\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5818 - acc: 0.9886 - val_loss: 0.5546 - val_acc: 0.9535\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5793 - acc: 0.9886 - val_loss: 0.5522 - val_acc: 0.9535\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.5769 - acc: 0.9886 - val_loss: 0.5498 - val_acc: 0.9535\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5745 - acc: 0.9886 - val_loss: 0.5474 - val_acc: 0.9535\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.5721 - acc: 0.9886 - val_loss: 0.5450 - val_acc: 0.9535\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.5697 - acc: 0.9886 - val_loss: 0.5426 - val_acc: 0.9535\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.5673 - acc: 0.9886 - val_loss: 0.5402 - val_acc: 0.9535\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.5649 - acc: 0.9886 - val_loss: 0.5379 - val_acc: 0.9535\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.5625 - acc: 0.9886 - val_loss: 0.5355 - val_acc: 0.9535\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.5602 - acc: 0.9886 - val_loss: 0.5332 - val_acc: 0.9535\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5579 - acc: 0.9886 - val_loss: 0.5308 - val_acc: 0.9535\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.5555 - acc: 0.9886 - val_loss: 0.5285 - val_acc: 0.9535\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.5532 - acc: 0.9886 - val_loss: 0.5262 - val_acc: 0.9535\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.5508 - acc: 0.9886 - val_loss: 0.5238 - val_acc: 0.9535\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.5485 - acc: 0.9886 - val_loss: 0.5215 - val_acc: 0.9535\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.5462 - acc: 0.9886 - val_loss: 0.5191 - val_acc: 0.9535\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.5438 - acc: 0.9886 - val_loss: 0.5168 - val_acc: 0.9535\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.5415 - acc: 0.9886 - val_loss: 0.5145 - val_acc: 0.9535\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5392 - acc: 0.9886 - val_loss: 0.5122 - val_acc: 0.9535\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.5369 - acc: 0.9886 - val_loss: 0.5099 - val_acc: 0.9535\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.5346 - acc: 0.9886 - val_loss: 0.5077 - val_acc: 0.9535\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.5324 - acc: 0.9886 - val_loss: 0.5054 - val_acc: 0.9535\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.5302 - acc: 0.9886 - val_loss: 0.5032 - val_acc: 0.9535\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 0s 286us/step - loss: 0.5279 - acc: 1.0000 - val_loss: 0.5009 - val_acc: 0.9535\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.5256 - acc: 1.0000 - val_loss: 0.4986 - val_acc: 0.9535\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.5233 - acc: 1.0000 - val_loss: 0.4964 - val_acc: 0.9535\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.5210 - acc: 1.0000 - val_loss: 0.4941 - val_acc: 0.9535\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.5188 - acc: 1.0000 - val_loss: 0.4917 - val_acc: 0.9535\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.5164 - acc: 1.0000 - val_loss: 0.4894 - val_acc: 0.9767\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.5141 - acc: 1.0000 - val_loss: 0.4871 - val_acc: 0.9767\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.5119 - acc: 1.0000 - val_loss: 0.4849 - val_acc: 0.9767\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.5096 - acc: 1.0000 - val_loss: 0.4824 - val_acc: 0.9767\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.5073 - acc: 1.0000 - val_loss: 0.4800 - val_acc: 0.9767\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.5049 - acc: 1.0000 - val_loss: 0.4776 - val_acc: 0.9767\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.5027 - acc: 1.0000 - val_loss: 0.4752 - val_acc: 0.9767\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.5004 - acc: 1.0000 - val_loss: 0.4729 - val_acc: 0.9767\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.4980 - acc: 1.0000 - val_loss: 0.4705 - val_acc: 0.9767\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 0s 284us/step - loss: 0.4958 - acc: 1.0000 - val_loss: 0.4681 - val_acc: 0.9767\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.4934 - acc: 1.0000 - val_loss: 0.4657 - val_acc: 0.9767\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.4911 - acc: 1.0000 - val_loss: 0.4632 - val_acc: 0.9767\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.4888 - acc: 1.0000 - val_loss: 0.4607 - val_acc: 0.9767\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.4865 - acc: 1.0000 - val_loss: 0.4583 - val_acc: 0.9767\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.4842 - acc: 1.0000 - val_loss: 0.4559 - val_acc: 0.9767\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.4819 - acc: 1.0000 - val_loss: 0.4534 - val_acc: 0.9767\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.4796 - acc: 1.0000 - val_loss: 0.4511 - val_acc: 0.9767\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.4773 - acc: 1.0000 - val_loss: 0.4487 - val_acc: 0.9767\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.4751 - acc: 1.0000 - val_loss: 0.4463 - val_acc: 0.9767\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.4729 - acc: 1.0000 - val_loss: 0.4440 - val_acc: 0.9767\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.4706 - acc: 1.0000 - val_loss: 0.4416 - val_acc: 0.9767\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.4684 - acc: 1.0000 - val_loss: 0.4393 - val_acc: 0.9767\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.4662 - acc: 1.0000 - val_loss: 0.4370 - val_acc: 0.9767\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.4640 - acc: 1.0000 - val_loss: 0.4347 - val_acc: 0.9767\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 0s 292us/step - loss: 0.4617 - acc: 1.0000 - val_loss: 0.4324 - val_acc: 0.9767\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.4596 - acc: 1.0000 - val_loss: 0.4301 - val_acc: 0.9767\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.4574 - acc: 1.0000 - val_loss: 0.4279 - val_acc: 0.9767\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.4552 - acc: 1.0000 - val_loss: 0.4257 - val_acc: 0.9767\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4530 - acc: 1.0000 - val_loss: 0.4235 - val_acc: 0.9767\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.4508 - acc: 1.0000 - val_loss: 0.4212 - val_acc: 0.9767\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.4487 - acc: 1.0000 - val_loss: 0.4190 - val_acc: 0.9767\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.4465 - acc: 1.0000 - val_loss: 0.4168 - val_acc: 0.9767\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.4443 - acc: 1.0000 - val_loss: 0.4146 - val_acc: 0.9767\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.4422 - acc: 1.0000 - val_loss: 0.4124 - val_acc: 0.9767\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.4401 - acc: 1.0000 - val_loss: 0.4102 - val_acc: 0.9767\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4380 - acc: 1.0000 - val_loss: 0.4080 - val_acc: 0.9767\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.4359 - acc: 1.0000 - val_loss: 0.4059 - val_acc: 0.9767\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.4339 - acc: 1.0000 - val_loss: 0.4037 - val_acc: 0.9767\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4318 - acc: 1.0000 - val_loss: 0.4016 - val_acc: 0.9767\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4297 - acc: 1.0000 - val_loss: 0.3995 - val_acc: 0.9767\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.4278 - acc: 1.0000 - val_loss: 0.3974 - val_acc: 0.9767\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4258 - acc: 1.0000 - val_loss: 0.3953 - val_acc: 0.9767\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.4238 - acc: 1.0000 - val_loss: 0.3932 - val_acc: 0.9767\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.4218 - acc: 1.0000 - val_loss: 0.3911 - val_acc: 0.9767\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.4198 - acc: 1.0000 - val_loss: 0.3891 - val_acc: 0.9767\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.4178 - acc: 1.0000 - val_loss: 0.3871 - val_acc: 0.9767\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4158 - acc: 1.0000 - val_loss: 0.3850 - val_acc: 0.9767\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.4139 - acc: 1.0000 - val_loss: 0.3830 - val_acc: 0.9767\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.4120 - acc: 1.0000 - val_loss: 0.3811 - val_acc: 0.9767\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4100 - acc: 1.0000 - val_loss: 0.3791 - val_acc: 0.9767\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.4081 - acc: 1.0000 - val_loss: 0.3771 - val_acc: 0.9767\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.4062 - acc: 1.0000 - val_loss: 0.3751 - val_acc: 0.9767\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.4043 - acc: 1.0000 - val_loss: 0.3732 - val_acc: 0.9767\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.4024 - acc: 1.0000 - val_loss: 0.3713 - val_acc: 0.9767\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.4006 - acc: 1.0000 - val_loss: 0.3694 - val_acc: 0.9767\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.3987 - acc: 1.0000 - val_loss: 0.3675 - val_acc: 0.9767\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.3969 - acc: 1.0000 - val_loss: 0.3656 - val_acc: 0.9767\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.3950 - acc: 1.0000 - val_loss: 0.3637 - val_acc: 0.9767\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3931 - acc: 1.0000 - val_loss: 0.3618 - val_acc: 0.9767\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.3912 - acc: 1.0000 - val_loss: 0.3599 - val_acc: 0.9767\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.3894 - acc: 1.0000 - val_loss: 0.3581 - val_acc: 0.9767\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.3876 - acc: 1.0000 - val_loss: 0.3562 - val_acc: 0.9767\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3857 - acc: 1.0000 - val_loss: 0.3544 - val_acc: 0.9767\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.3840 - acc: 1.0000 - val_loss: 0.3526 - val_acc: 0.9767\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.3821 - acc: 1.0000 - val_loss: 0.3508 - val_acc: 0.9767\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.3803 - acc: 1.0000 - val_loss: 0.3490 - val_acc: 0.9767\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.3786 - acc: 1.0000 - val_loss: 0.3472 - val_acc: 0.9767\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.3768 - acc: 1.0000 - val_loss: 0.3455 - val_acc: 0.9767\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.3751 - acc: 1.0000 - val_loss: 0.3437 - val_acc: 0.9767\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.3734 - acc: 1.0000 - val_loss: 0.3419 - val_acc: 0.9767\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.3716 - acc: 1.0000 - val_loss: 0.3402 - val_acc: 0.9767\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.3700 - acc: 1.0000 - val_loss: 0.3385 - val_acc: 0.9767\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.3682 - acc: 1.0000 - val_loss: 0.3368 - val_acc: 0.9767\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.3665 - acc: 1.0000 - val_loss: 0.3351 - val_acc: 0.9767\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.3648 - acc: 1.0000 - val_loss: 0.3334 - val_acc: 0.9767\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3632 - acc: 1.0000 - val_loss: 0.3318 - val_acc: 0.9767\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.3616 - acc: 1.0000 - val_loss: 0.3301 - val_acc: 0.9767\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3599 - acc: 1.0000 - val_loss: 0.3285 - val_acc: 0.9767\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.3583 - acc: 1.0000 - val_loss: 0.3268 - val_acc: 0.9767\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3567 - acc: 1.0000 - val_loss: 0.3252 - val_acc: 0.9767\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.3551 - acc: 1.0000 - val_loss: 0.3236 - val_acc: 0.9767\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.3536 - acc: 1.0000 - val_loss: 0.3220 - val_acc: 0.9767\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.3520 - acc: 1.0000 - val_loss: 0.3204 - val_acc: 0.9767\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.3504 - acc: 1.0000 - val_loss: 0.3188 - val_acc: 0.9767\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.3488 - acc: 1.0000 - val_loss: 0.3173 - val_acc: 0.9767\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.3472 - acc: 1.0000 - val_loss: 0.3157 - val_acc: 0.9767\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.3457 - acc: 1.0000 - val_loss: 0.3141 - val_acc: 0.9767\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.3442 - acc: 1.0000 - val_loss: 0.3126 - val_acc: 0.9767\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.3426 - acc: 1.0000 - val_loss: 0.3111 - val_acc: 0.9767\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.3411 - acc: 1.0000 - val_loss: 0.3096 - val_acc: 0.9767\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.3396 - acc: 1.0000 - val_loss: 0.3081 - val_acc: 0.9767\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.3381 - acc: 1.0000 - val_loss: 0.3066 - val_acc: 0.9767\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.3367 - acc: 1.0000 - val_loss: 0.3051 - val_acc: 0.9767\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.3351 - acc: 1.0000 - val_loss: 0.3036 - val_acc: 0.9767\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.3337 - acc: 1.0000 - val_loss: 0.3021 - val_acc: 0.9767\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.3322 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 0.9767\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.3308 - acc: 1.0000 - val_loss: 0.2992 - val_acc: 0.9767\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.3293 - acc: 1.0000 - val_loss: 0.2977 - val_acc: 0.9767\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.3279 - acc: 1.0000 - val_loss: 0.2963 - val_acc: 0.9767\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.3265 - acc: 1.0000 - val_loss: 0.2949 - val_acc: 0.9767\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.3251 - acc: 1.0000 - val_loss: 0.2934 - val_acc: 0.9767\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.3236 - acc: 1.0000 - val_loss: 0.2920 - val_acc: 0.9767\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.3222 - acc: 1.0000 - val_loss: 0.2906 - val_acc: 0.9767\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.3208 - acc: 1.0000 - val_loss: 0.2892 - val_acc: 1.0000\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.3194 - acc: 1.0000 - val_loss: 0.2878 - val_acc: 1.0000\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.3180 - acc: 1.0000 - val_loss: 0.2864 - val_acc: 1.0000\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.3166 - acc: 1.0000 - val_loss: 0.2851 - val_acc: 1.0000\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.3153 - acc: 1.0000 - val_loss: 0.2837 - val_acc: 1.0000\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.3139 - acc: 1.0000 - val_loss: 0.2823 - val_acc: 1.0000\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.3125 - acc: 1.0000 - val_loss: 0.2810 - val_acc: 1.0000\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3112 - acc: 1.0000 - val_loss: 0.2796 - val_acc: 1.0000\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.3099 - acc: 1.0000 - val_loss: 0.2783 - val_acc: 1.0000\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.3086 - acc: 1.0000 - val_loss: 0.2770 - val_acc: 1.0000\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3072 - acc: 1.0000 - val_loss: 0.2757 - val_acc: 1.0000\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.3059 - acc: 1.0000 - val_loss: 0.2743 - val_acc: 1.0000\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.3046 - acc: 1.0000 - val_loss: 0.2730 - val_acc: 1.0000\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.3033 - acc: 1.0000 - val_loss: 0.2717 - val_acc: 1.0000\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.3020 - acc: 1.0000 - val_loss: 0.2704 - val_acc: 1.0000\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.3007 - acc: 1.0000 - val_loss: 0.2691 - val_acc: 1.0000\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.2994 - acc: 1.0000 - val_loss: 0.2679 - val_acc: 1.0000\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.2982 - acc: 1.0000 - val_loss: 0.2666 - val_acc: 1.0000\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 0s 322us/step - loss: 0.2969 - acc: 1.0000 - val_loss: 0.2654 - val_acc: 1.0000\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.2957 - acc: 1.0000 - val_loss: 0.2641 - val_acc: 1.0000\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.2944 - acc: 1.0000 - val_loss: 0.2629 - val_acc: 1.0000\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.2932 - acc: 1.0000 - val_loss: 0.2616 - val_acc: 1.0000\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.2919 - acc: 1.0000 - val_loss: 0.2604 - val_acc: 1.0000\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.2907 - acc: 1.0000 - val_loss: 0.2591 - val_acc: 1.0000\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.2895 - acc: 1.0000 - val_loss: 0.2579 - val_acc: 1.0000\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.2882 - acc: 1.0000 - val_loss: 0.2567 - val_acc: 1.0000\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2871 - acc: 1.0000 - val_loss: 0.2555 - val_acc: 1.0000\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.2858 - acc: 1.0000 - val_loss: 0.2543 - val_acc: 1.0000\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.2847 - acc: 1.0000 - val_loss: 0.2531 - val_acc: 1.0000\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2834 - acc: 1.0000 - val_loss: 0.2519 - val_acc: 1.0000\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2822 - acc: 1.0000 - val_loss: 0.2507 - val_acc: 1.0000\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.2811 - acc: 1.0000 - val_loss: 0.2496 - val_acc: 1.0000\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2799 - acc: 1.0000 - val_loss: 0.2484 - val_acc: 1.0000\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.2787 - acc: 1.0000 - val_loss: 0.2472 - val_acc: 1.0000\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.2776 - acc: 1.0000 - val_loss: 0.2461 - val_acc: 1.0000\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.2764 - acc: 1.0000 - val_loss: 0.2449 - val_acc: 1.0000\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2753 - acc: 1.0000 - val_loss: 0.2438 - val_acc: 1.0000\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2741 - acc: 1.0000 - val_loss: 0.2426 - val_acc: 1.0000\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.2730 - acc: 1.0000 - val_loss: 0.2415 - val_acc: 1.0000\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.2718 - acc: 1.0000 - val_loss: 0.2404 - val_acc: 1.0000\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.2707 - acc: 1.0000 - val_loss: 0.2393 - val_acc: 1.0000\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.2696 - acc: 1.0000 - val_loss: 0.2382 - val_acc: 1.0000\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.2685 - acc: 1.0000 - val_loss: 0.2370 - val_acc: 1.0000\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.2674 - acc: 1.0000 - val_loss: 0.2359 - val_acc: 1.0000\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.2662 - acc: 1.0000 - val_loss: 0.2349 - val_acc: 1.0000\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.2652 - acc: 1.0000 - val_loss: 0.2338 - val_acc: 1.0000\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.2641 - acc: 1.0000 - val_loss: 0.2327 - val_acc: 1.0000\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2630 - acc: 1.0000 - val_loss: 0.2316 - val_acc: 1.0000\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2619 - acc: 1.0000 - val_loss: 0.2305 - val_acc: 1.0000\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2608 - acc: 1.0000 - val_loss: 0.2295 - val_acc: 1.0000\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.2597 - acc: 1.0000 - val_loss: 0.2284 - val_acc: 1.0000\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.2587 - acc: 1.0000 - val_loss: 0.2274 - val_acc: 1.0000\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.2576 - acc: 1.0000 - val_loss: 0.2263 - val_acc: 1.0000\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2565 - acc: 1.0000 - val_loss: 0.2253 - val_acc: 1.0000\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.2555 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 1.0000\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 0s 299us/step - loss: 0.2545 - acc: 1.0000 - val_loss: 0.2232 - val_acc: 1.0000\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2534 - acc: 1.0000 - val_loss: 0.2222 - val_acc: 1.0000\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2524 - acc: 1.0000 - val_loss: 0.2212 - val_acc: 1.0000\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.2514 - acc: 1.0000 - val_loss: 0.2202 - val_acc: 1.0000\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.2503 - acc: 1.0000 - val_loss: 0.2192 - val_acc: 1.0000\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2493 - acc: 1.0000 - val_loss: 0.2181 - val_acc: 1.0000\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.2483 - acc: 0.9886 - val_loss: 0.2171 - val_acc: 1.0000\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.2473 - acc: 0.9886 - val_loss: 0.2161 - val_acc: 1.0000\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2463 - acc: 0.9886 - val_loss: 0.2152 - val_acc: 1.0000\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.2453 - acc: 0.9886 - val_loss: 0.2142 - val_acc: 1.0000\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2443 - acc: 0.9886 - val_loss: 0.2132 - val_acc: 1.0000\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.2433 - acc: 0.9886 - val_loss: 0.2122 - val_acc: 1.0000\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.2423 - acc: 0.9886 - val_loss: 0.2113 - val_acc: 1.0000\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.2414 - acc: 0.9886 - val_loss: 0.2103 - val_acc: 1.0000\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2404 - acc: 0.9886 - val_loss: 0.2093 - val_acc: 1.0000\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.2394 - acc: 0.9886 - val_loss: 0.2084 - val_acc: 1.0000\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2384 - acc: 0.9886 - val_loss: 0.2074 - val_acc: 1.0000\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.2375 - acc: 0.9886 - val_loss: 0.2065 - val_acc: 1.0000\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2365 - acc: 0.9886 - val_loss: 0.2056 - val_acc: 1.0000\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.2356 - acc: 0.9886 - val_loss: 0.2046 - val_acc: 1.0000\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.2346 - acc: 0.9886 - val_loss: 0.2037 - val_acc: 1.0000\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.2337 - acc: 0.9886 - val_loss: 0.2028 - val_acc: 1.0000\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.2328 - acc: 0.9886 - val_loss: 0.2018 - val_acc: 1.0000\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.2318 - acc: 0.9886 - val_loss: 0.2009 - val_acc: 1.0000\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2309 - acc: 0.9886 - val_loss: 0.2000 - val_acc: 1.0000\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.2300 - acc: 0.9886 - val_loss: 0.1991 - val_acc: 1.0000\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2290 - acc: 0.9886 - val_loss: 0.1982 - val_acc: 1.0000\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.2282 - acc: 0.9886 - val_loss: 0.1974 - val_acc: 1.0000\n",
            "Epoch 321/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2272 - acc: 0.9886 - val_loss: 0.1965 - val_acc: 1.0000\n",
            "Epoch 322/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.2263 - acc: 0.9886 - val_loss: 0.1956 - val_acc: 1.0000\n",
            "Epoch 323/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.2255 - acc: 0.9886 - val_loss: 0.1947 - val_acc: 1.0000\n",
            "Epoch 324/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.2246 - acc: 0.9886 - val_loss: 0.1938 - val_acc: 1.0000\n",
            "Epoch 325/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.2237 - acc: 0.9886 - val_loss: 0.1930 - val_acc: 1.0000\n",
            "Epoch 326/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.2228 - acc: 0.9886 - val_loss: 0.1921 - val_acc: 1.0000\n",
            "Epoch 327/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2219 - acc: 0.9886 - val_loss: 0.1913 - val_acc: 1.0000\n",
            "Epoch 328/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.2210 - acc: 0.9886 - val_loss: 0.1904 - val_acc: 1.0000\n",
            "Epoch 329/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.2201 - acc: 0.9886 - val_loss: 0.1896 - val_acc: 1.0000\n",
            "Epoch 330/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.2193 - acc: 0.9886 - val_loss: 0.1887 - val_acc: 1.0000\n",
            "Epoch 331/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.2184 - acc: 0.9886 - val_loss: 0.1879 - val_acc: 1.0000\n",
            "Epoch 332/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.2176 - acc: 0.9886 - val_loss: 0.1870 - val_acc: 1.0000\n",
            "Epoch 333/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.2167 - acc: 0.9886 - val_loss: 0.1862 - val_acc: 1.0000\n",
            "Epoch 334/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.2159 - acc: 0.9886 - val_loss: 0.1854 - val_acc: 1.0000\n",
            "Epoch 335/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.2150 - acc: 0.9886 - val_loss: 0.1846 - val_acc: 1.0000\n",
            "Epoch 336/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.2142 - acc: 0.9886 - val_loss: 0.1838 - val_acc: 1.0000\n",
            "Epoch 337/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2133 - acc: 0.9886 - val_loss: 0.1829 - val_acc: 1.0000\n",
            "Epoch 338/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.2125 - acc: 0.9886 - val_loss: 0.1821 - val_acc: 1.0000\n",
            "Epoch 339/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.2117 - acc: 0.9886 - val_loss: 0.1813 - val_acc: 1.0000\n",
            "Epoch 340/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2109 - acc: 0.9886 - val_loss: 0.1805 - val_acc: 1.0000\n",
            "Epoch 341/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2101 - acc: 0.9886 - val_loss: 0.1797 - val_acc: 1.0000\n",
            "Epoch 342/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.2092 - acc: 0.9886 - val_loss: 0.1789 - val_acc: 1.0000\n",
            "Epoch 343/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.2084 - acc: 0.9886 - val_loss: 0.1782 - val_acc: 1.0000\n",
            "Epoch 344/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2076 - acc: 0.9886 - val_loss: 0.1774 - val_acc: 1.0000\n",
            "Epoch 345/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2068 - acc: 0.9886 - val_loss: 0.1766 - val_acc: 1.0000\n",
            "Epoch 346/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2060 - acc: 0.9886 - val_loss: 0.1758 - val_acc: 1.0000\n",
            "Epoch 347/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.2052 - acc: 0.9886 - val_loss: 0.1751 - val_acc: 1.0000\n",
            "Epoch 348/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.2044 - acc: 0.9886 - val_loss: 0.1743 - val_acc: 1.0000\n",
            "Epoch 349/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.2036 - acc: 0.9886 - val_loss: 0.1735 - val_acc: 1.0000\n",
            "Epoch 350/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.2028 - acc: 0.9886 - val_loss: 0.1728 - val_acc: 1.0000\n",
            "Epoch 351/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.2021 - acc: 0.9886 - val_loss: 0.1720 - val_acc: 1.0000\n",
            "Epoch 352/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2013 - acc: 0.9886 - val_loss: 0.1713 - val_acc: 1.0000\n",
            "Epoch 353/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.2005 - acc: 0.9886 - val_loss: 0.1705 - val_acc: 1.0000\n",
            "Epoch 354/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1998 - acc: 0.9886 - val_loss: 0.1698 - val_acc: 1.0000\n",
            "Epoch 355/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1990 - acc: 0.9886 - val_loss: 0.1690 - val_acc: 1.0000\n",
            "Epoch 356/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.1982 - acc: 0.9886 - val_loss: 0.1683 - val_acc: 1.0000\n",
            "Epoch 357/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.1975 - acc: 0.9886 - val_loss: 0.1676 - val_acc: 1.0000\n",
            "Epoch 358/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.1967 - acc: 0.9886 - val_loss: 0.1668 - val_acc: 1.0000\n",
            "Epoch 359/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1960 - acc: 0.9886 - val_loss: 0.1661 - val_acc: 1.0000\n",
            "Epoch 360/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1952 - acc: 0.9886 - val_loss: 0.1654 - val_acc: 1.0000\n",
            "Epoch 361/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1945 - acc: 0.9886 - val_loss: 0.1647 - val_acc: 1.0000\n",
            "Epoch 362/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1938 - acc: 0.9886 - val_loss: 0.1640 - val_acc: 1.0000\n",
            "Epoch 363/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1930 - acc: 0.9886 - val_loss: 0.1633 - val_acc: 1.0000\n",
            "Epoch 364/500\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.1923 - acc: 0.9886 - val_loss: 0.1626 - val_acc: 1.0000\n",
            "Epoch 365/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1916 - acc: 0.9886 - val_loss: 0.1619 - val_acc: 1.0000\n",
            "Epoch 366/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.1908 - acc: 0.9886 - val_loss: 0.1612 - val_acc: 1.0000\n",
            "Epoch 367/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1901 - acc: 0.9886 - val_loss: 0.1605 - val_acc: 1.0000\n",
            "Epoch 368/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1894 - acc: 0.9886 - val_loss: 0.1598 - val_acc: 1.0000\n",
            "Epoch 369/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1887 - acc: 0.9886 - val_loss: 0.1591 - val_acc: 1.0000\n",
            "Epoch 370/500\n",
            "88/88 [==============================] - 0s 293us/step - loss: 0.1880 - acc: 0.9886 - val_loss: 0.1584 - val_acc: 1.0000\n",
            "Epoch 371/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1873 - acc: 0.9886 - val_loss: 0.1577 - val_acc: 1.0000\n",
            "Epoch 372/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.1866 - acc: 0.9886 - val_loss: 0.1571 - val_acc: 1.0000\n",
            "Epoch 373/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1859 - acc: 0.9886 - val_loss: 0.1564 - val_acc: 1.0000\n",
            "Epoch 374/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1852 - acc: 0.9886 - val_loss: 0.1557 - val_acc: 1.0000\n",
            "Epoch 375/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.1845 - acc: 0.9886 - val_loss: 0.1551 - val_acc: 1.0000\n",
            "Epoch 376/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.1838 - acc: 0.9886 - val_loss: 0.1544 - val_acc: 1.0000\n",
            "Epoch 377/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1832 - acc: 0.9886 - val_loss: 0.1538 - val_acc: 1.0000\n",
            "Epoch 378/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.1825 - acc: 0.9886 - val_loss: 0.1531 - val_acc: 1.0000\n",
            "Epoch 379/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1818 - acc: 0.9886 - val_loss: 0.1525 - val_acc: 1.0000\n",
            "Epoch 380/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1811 - acc: 0.9886 - val_loss: 0.1518 - val_acc: 1.0000\n",
            "Epoch 381/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1804 - acc: 0.9886 - val_loss: 0.1512 - val_acc: 1.0000\n",
            "Epoch 382/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1798 - acc: 0.9886 - val_loss: 0.1505 - val_acc: 1.0000\n",
            "Epoch 383/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1791 - acc: 0.9886 - val_loss: 0.1499 - val_acc: 1.0000\n",
            "Epoch 384/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.1785 - acc: 0.9886 - val_loss: 0.1493 - val_acc: 1.0000\n",
            "Epoch 385/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.1778 - acc: 0.9886 - val_loss: 0.1486 - val_acc: 1.0000\n",
            "Epoch 386/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.1772 - acc: 0.9886 - val_loss: 0.1480 - val_acc: 1.0000\n",
            "Epoch 387/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.1765 - acc: 0.9886 - val_loss: 0.1474 - val_acc: 1.0000\n",
            "Epoch 388/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.1759 - acc: 0.9886 - val_loss: 0.1468 - val_acc: 1.0000\n",
            "Epoch 389/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.1752 - acc: 0.9886 - val_loss: 0.1462 - val_acc: 1.0000\n",
            "Epoch 390/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1746 - acc: 0.9886 - val_loss: 0.1455 - val_acc: 1.0000\n",
            "Epoch 391/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1740 - acc: 0.9886 - val_loss: 0.1449 - val_acc: 1.0000\n",
            "Epoch 392/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1733 - acc: 0.9886 - val_loss: 0.1443 - val_acc: 1.0000\n",
            "Epoch 393/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1727 - acc: 0.9886 - val_loss: 0.1437 - val_acc: 1.0000\n",
            "Epoch 394/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1721 - acc: 0.9886 - val_loss: 0.1431 - val_acc: 1.0000\n",
            "Epoch 395/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.1715 - acc: 0.9886 - val_loss: 0.1425 - val_acc: 1.0000\n",
            "Epoch 396/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1709 - acc: 0.9886 - val_loss: 0.1420 - val_acc: 1.0000\n",
            "Epoch 397/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1702 - acc: 0.9886 - val_loss: 0.1414 - val_acc: 1.0000\n",
            "Epoch 398/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.1696 - acc: 0.9886 - val_loss: 0.1408 - val_acc: 1.0000\n",
            "Epoch 399/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1690 - acc: 0.9886 - val_loss: 0.1402 - val_acc: 1.0000\n",
            "Epoch 400/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.1684 - acc: 0.9886 - val_loss: 0.1396 - val_acc: 1.0000\n",
            "Epoch 401/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.1678 - acc: 0.9886 - val_loss: 0.1391 - val_acc: 1.0000\n",
            "Epoch 402/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.1672 - acc: 0.9886 - val_loss: 0.1385 - val_acc: 1.0000\n",
            "Epoch 403/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.1666 - acc: 0.9886 - val_loss: 0.1379 - val_acc: 1.0000\n",
            "Epoch 404/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1660 - acc: 0.9886 - val_loss: 0.1374 - val_acc: 1.0000\n",
            "Epoch 405/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.1654 - acc: 0.9886 - val_loss: 0.1368 - val_acc: 1.0000\n",
            "Epoch 406/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.1648 - acc: 0.9886 - val_loss: 0.1362 - val_acc: 1.0000\n",
            "Epoch 407/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.1643 - acc: 0.9886 - val_loss: 0.1357 - val_acc: 1.0000\n",
            "Epoch 408/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1637 - acc: 0.9886 - val_loss: 0.1351 - val_acc: 1.0000\n",
            "Epoch 409/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.1631 - acc: 0.9886 - val_loss: 0.1346 - val_acc: 1.0000\n",
            "Epoch 410/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1625 - acc: 0.9886 - val_loss: 0.1340 - val_acc: 1.0000\n",
            "Epoch 411/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.1620 - acc: 0.9886 - val_loss: 0.1335 - val_acc: 1.0000\n",
            "Epoch 412/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.1614 - acc: 0.9886 - val_loss: 0.1330 - val_acc: 1.0000\n",
            "Epoch 413/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.1608 - acc: 0.9886 - val_loss: 0.1324 - val_acc: 1.0000\n",
            "Epoch 414/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.1603 - acc: 0.9886 - val_loss: 0.1319 - val_acc: 1.0000\n",
            "Epoch 415/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1597 - acc: 0.9886 - val_loss: 0.1314 - val_acc: 1.0000\n",
            "Epoch 416/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.1592 - acc: 0.9886 - val_loss: 0.1308 - val_acc: 1.0000\n",
            "Epoch 417/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1586 - acc: 0.9886 - val_loss: 0.1303 - val_acc: 1.0000\n",
            "Epoch 418/500\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.1581 - acc: 0.9886 - val_loss: 0.1298 - val_acc: 1.0000\n",
            "Epoch 419/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.1575 - acc: 0.9886 - val_loss: 0.1292 - val_acc: 1.0000\n",
            "Epoch 420/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1570 - acc: 0.9886 - val_loss: 0.1287 - val_acc: 1.0000\n",
            "Epoch 421/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1564 - acc: 0.9886 - val_loss: 0.1282 - val_acc: 1.0000\n",
            "Epoch 422/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.1559 - acc: 0.9886 - val_loss: 0.1277 - val_acc: 1.0000\n",
            "Epoch 423/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1554 - acc: 0.9886 - val_loss: 0.1272 - val_acc: 1.0000\n",
            "Epoch 424/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1548 - acc: 0.9886 - val_loss: 0.1267 - val_acc: 1.0000\n",
            "Epoch 425/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1543 - acc: 0.9886 - val_loss: 0.1262 - val_acc: 1.0000\n",
            "Epoch 426/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.1538 - acc: 0.9886 - val_loss: 0.1257 - val_acc: 1.0000\n",
            "Epoch 427/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1533 - acc: 0.9886 - val_loss: 0.1252 - val_acc: 1.0000\n",
            "Epoch 428/500\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.1527 - acc: 0.9886 - val_loss: 0.1247 - val_acc: 1.0000\n",
            "Epoch 429/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.1522 - acc: 0.9886 - val_loss: 0.1242 - val_acc: 1.0000\n",
            "Epoch 430/500\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.1517 - acc: 0.9886 - val_loss: 0.1237 - val_acc: 1.0000\n",
            "Epoch 431/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.1512 - acc: 0.9886 - val_loss: 0.1232 - val_acc: 1.0000\n",
            "Epoch 432/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.1507 - acc: 0.9886 - val_loss: 0.1227 - val_acc: 1.0000\n",
            "Epoch 433/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.1502 - acc: 0.9886 - val_loss: 0.1223 - val_acc: 1.0000\n",
            "Epoch 434/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1496 - acc: 0.9886 - val_loss: 0.1218 - val_acc: 1.0000\n",
            "Epoch 435/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.1491 - acc: 0.9886 - val_loss: 0.1213 - val_acc: 1.0000\n",
            "Epoch 436/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.1487 - acc: 0.9886 - val_loss: 0.1208 - val_acc: 1.0000\n",
            "Epoch 437/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1482 - acc: 0.9886 - val_loss: 0.1204 - val_acc: 1.0000\n",
            "Epoch 438/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.1476 - acc: 0.9886 - val_loss: 0.1199 - val_acc: 1.0000\n",
            "Epoch 439/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.1472 - acc: 0.9886 - val_loss: 0.1194 - val_acc: 1.0000\n",
            "Epoch 440/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1467 - acc: 0.9886 - val_loss: 0.1190 - val_acc: 1.0000\n",
            "Epoch 441/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.1462 - acc: 0.9886 - val_loss: 0.1185 - val_acc: 1.0000\n",
            "Epoch 442/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1457 - acc: 0.9886 - val_loss: 0.1180 - val_acc: 1.0000\n",
            "Epoch 443/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.1452 - acc: 0.9886 - val_loss: 0.1176 - val_acc: 1.0000\n",
            "Epoch 444/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1447 - acc: 0.9886 - val_loss: 0.1171 - val_acc: 1.0000\n",
            "Epoch 445/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.1443 - acc: 0.9886 - val_loss: 0.1167 - val_acc: 1.0000\n",
            "Epoch 446/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.1438 - acc: 0.9886 - val_loss: 0.1162 - val_acc: 1.0000\n",
            "Epoch 447/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.1433 - acc: 0.9886 - val_loss: 0.1158 - val_acc: 1.0000\n",
            "Epoch 448/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.1428 - acc: 0.9886 - val_loss: 0.1153 - val_acc: 1.0000\n",
            "Epoch 449/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.1424 - acc: 0.9886 - val_loss: 0.1149 - val_acc: 1.0000\n",
            "Epoch 450/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1419 - acc: 0.9886 - val_loss: 0.1145 - val_acc: 1.0000\n",
            "Epoch 451/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1414 - acc: 0.9886 - val_loss: 0.1140 - val_acc: 1.0000\n",
            "Epoch 452/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1410 - acc: 0.9886 - val_loss: 0.1136 - val_acc: 1.0000\n",
            "Epoch 453/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.1405 - acc: 0.9886 - val_loss: 0.1132 - val_acc: 1.0000\n",
            "Epoch 454/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.1401 - acc: 0.9886 - val_loss: 0.1127 - val_acc: 1.0000\n",
            "Epoch 455/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1396 - acc: 0.9886 - val_loss: 0.1123 - val_acc: 1.0000\n",
            "Epoch 456/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.1392 - acc: 0.9886 - val_loss: 0.1119 - val_acc: 1.0000\n",
            "Epoch 457/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.1387 - acc: 0.9886 - val_loss: 0.1114 - val_acc: 1.0000\n",
            "Epoch 458/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1383 - acc: 0.9886 - val_loss: 0.1110 - val_acc: 1.0000\n",
            "Epoch 459/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1378 - acc: 0.9886 - val_loss: 0.1106 - val_acc: 1.0000\n",
            "Epoch 460/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.1374 - acc: 0.9886 - val_loss: 0.1102 - val_acc: 1.0000\n",
            "Epoch 461/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1369 - acc: 0.9886 - val_loss: 0.1098 - val_acc: 1.0000\n",
            "Epoch 462/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.1365 - acc: 0.9886 - val_loss: 0.1094 - val_acc: 1.0000\n",
            "Epoch 463/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1361 - acc: 0.9886 - val_loss: 0.1090 - val_acc: 1.0000\n",
            "Epoch 464/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1356 - acc: 0.9886 - val_loss: 0.1086 - val_acc: 1.0000\n",
            "Epoch 465/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.1352 - acc: 0.9886 - val_loss: 0.1081 - val_acc: 1.0000\n",
            "Epoch 466/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1348 - acc: 0.9886 - val_loss: 0.1077 - val_acc: 1.0000\n",
            "Epoch 467/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1343 - acc: 0.9886 - val_loss: 0.1073 - val_acc: 1.0000\n",
            "Epoch 468/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1339 - acc: 0.9886 - val_loss: 0.1069 - val_acc: 1.0000\n",
            "Epoch 469/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1335 - acc: 0.9886 - val_loss: 0.1066 - val_acc: 1.0000\n",
            "Epoch 470/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.1331 - acc: 0.9886 - val_loss: 0.1062 - val_acc: 1.0000\n",
            "Epoch 471/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1327 - acc: 0.9886 - val_loss: 0.1058 - val_acc: 1.0000\n",
            "Epoch 472/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.1323 - acc: 0.9886 - val_loss: 0.1054 - val_acc: 1.0000\n",
            "Epoch 473/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1318 - acc: 0.9886 - val_loss: 0.1050 - val_acc: 1.0000\n",
            "Epoch 474/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.1314 - acc: 0.9886 - val_loss: 0.1046 - val_acc: 1.0000\n",
            "Epoch 475/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1310 - acc: 0.9886 - val_loss: 0.1042 - val_acc: 1.0000\n",
            "Epoch 476/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.1306 - acc: 0.9886 - val_loss: 0.1038 - val_acc: 1.0000\n",
            "Epoch 477/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1302 - acc: 0.9886 - val_loss: 0.1035 - val_acc: 1.0000\n",
            "Epoch 478/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1298 - acc: 0.9886 - val_loss: 0.1031 - val_acc: 1.0000\n",
            "Epoch 479/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1294 - acc: 0.9886 - val_loss: 0.1027 - val_acc: 1.0000\n",
            "Epoch 480/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.1290 - acc: 0.9886 - val_loss: 0.1023 - val_acc: 1.0000\n",
            "Epoch 481/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.1286 - acc: 0.9886 - val_loss: 0.1020 - val_acc: 1.0000\n",
            "Epoch 482/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1282 - acc: 0.9886 - val_loss: 0.1016 - val_acc: 1.0000\n",
            "Epoch 483/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1278 - acc: 0.9886 - val_loss: 0.1012 - val_acc: 1.0000\n",
            "Epoch 484/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1274 - acc: 0.9886 - val_loss: 0.1009 - val_acc: 1.0000\n",
            "Epoch 485/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.1270 - acc: 0.9886 - val_loss: 0.1005 - val_acc: 1.0000\n",
            "Epoch 486/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1266 - acc: 0.9886 - val_loss: 0.1001 - val_acc: 1.0000\n",
            "Epoch 487/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1263 - acc: 0.9886 - val_loss: 0.0998 - val_acc: 1.0000\n",
            "Epoch 488/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.1259 - acc: 0.9886 - val_loss: 0.0994 - val_acc: 1.0000\n",
            "Epoch 489/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1255 - acc: 0.9886 - val_loss: 0.0991 - val_acc: 1.0000\n",
            "Epoch 490/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.1251 - acc: 0.9886 - val_loss: 0.0987 - val_acc: 1.0000\n",
            "Epoch 491/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1247 - acc: 0.9886 - val_loss: 0.0984 - val_acc: 1.0000\n",
            "Epoch 492/500\n",
            "88/88 [==============================] - 0s 303us/step - loss: 0.1244 - acc: 0.9886 - val_loss: 0.0980 - val_acc: 1.0000\n",
            "Epoch 493/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.1240 - acc: 0.9886 - val_loss: 0.0977 - val_acc: 1.0000\n",
            "Epoch 494/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.1236 - acc: 0.9886 - val_loss: 0.0973 - val_acc: 1.0000\n",
            "Epoch 495/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.1233 - acc: 0.9886 - val_loss: 0.0970 - val_acc: 1.0000\n",
            "Epoch 496/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1229 - acc: 0.9886 - val_loss: 0.0966 - val_acc: 1.0000\n",
            "Epoch 497/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.1225 - acc: 0.9886 - val_loss: 0.0963 - val_acc: 1.0000\n",
            "Epoch 498/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1222 - acc: 0.9886 - val_loss: 0.0959 - val_acc: 1.0000\n",
            "Epoch 499/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1218 - acc: 0.9886 - val_loss: 0.0956 - val_acc: 1.0000\n",
            "Epoch 500/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1214 - acc: 0.9886 - val_loss: 0.0953 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "7d0333fa-d593-4785-8b4a-1633696248c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.05997689,  1.78480965],\n",
              "       [-1.70377632,  2.62178086],\n",
              "       [-0.88024938,  1.10677205],\n",
              "       [-1.6221035 ,  2.53797672],\n",
              "       [-1.20438162,  1.42505216],\n",
              "       [-0.72831295, -0.54994201],\n",
              "       [-0.71544696, -1.05313817],\n",
              "       [-0.78016566, -0.87184816],\n",
              "       [-0.66616332, -1.04551976],\n",
              "       [-0.80814457, -1.1534213 ],\n",
              "       [-0.18117691, -0.5752502 ],\n",
              "       [-0.5797643 , -0.90332856],\n",
              "       [-0.75946851, -0.80650801],\n",
              "       [-0.47381325, -0.94095818],\n",
              "       [-0.51651934, -0.3343159 ],\n",
              "       [-0.9078223 , -0.51343018],\n",
              "       [-0.83040609, -0.52280685],\n",
              "       [-0.8364583 , -1.1044959 ],\n",
              "       [-1.06491561, -0.23177315],\n",
              "       [-1.09349754, -1.34108765],\n",
              "       [-1.44273181, -1.36811384],\n",
              "       [-0.39121396, -1.41060042],\n",
              "       [-0.96660223, -1.52141472],\n",
              "       [-0.29851281, -1.2365264 ],\n",
              "       [-0.66466281, -0.89831149],\n",
              "       [ 1.12079905,  0.10919806],\n",
              "       [ 0.41839628,  0.46087105],\n",
              "       [ 0.81585568,  0.3499919 ],\n",
              "       [ 0.6952246 , -0.22961609],\n",
              "       [ 0.78712685,  0.6056401 ],\n",
              "       [ 0.98738836,  0.40085179],\n",
              "       [ 1.18764219,  0.36694321],\n",
              "       [ 1.70546993,  0.15254344],\n",
              "       [ 1.84248269, -0.39151261],\n",
              "       [ 1.42391516,  0.0626175 ],\n",
              "       [ 1.3927998 ,  0.12226364],\n",
              "       [ 1.6269367 ,  0.7386622 ],\n",
              "       [ 1.87133538,  1.64617693],\n",
              "       [ 1.27102475, -0.06582761],\n",
              "       [ 0.9978794 ,  0.55048648],\n",
              "       [ 0.59853173,  0.27469505],\n",
              "       [ 1.40628344,  0.21564038],\n",
              "       [ 1.07460331,  0.42032253]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "81c393e5-d306-4e82-d195-f8092691166f",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "70e9f17b-3cca-471d-fad0-43bb4c7ded00",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories_lda[2])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b95a5638-0501-4040-fa1b-c2c17cf3ac82",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history_lda)"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "75db8c3f-b044-40be-f856-d1d1def8d9de",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6937ecf908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "56a33c09-e98a-49a5-d1e8-d24656920c39",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6937e3f828>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RI6ZW6_iHkz",
        "colab_type": "text"
      },
      "source": [
        "QUESTI RISULTATI SONO COSÌ BUONI PERCHÈ SI È FATTO Z SCORE ED LDA DI TUTTO IL TRAIN+VALIDATION SET. ANDAVA INVECE FATTA SOLO SUL TRAIN SET ED APPLICATA AL VALIDATION SET."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1466e0b8-4ac9-4766-816e-67693ab5074b",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "131/131 [==============================] - 0s 1ms/step - loss: 1.1483 - acc: 0.2672\n",
            "Epoch 2/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1220 - acc: 0.3130\n",
            "Epoch 3/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 1.0969 - acc: 0.4122\n",
            "Epoch 4/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0735 - acc: 0.4580\n",
            "Epoch 5/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0509 - acc: 0.4809\n",
            "Epoch 6/500\n",
            "131/131 [==============================] - 0s 197us/step - loss: 1.0291 - acc: 0.5344\n",
            "Epoch 7/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.0080 - acc: 0.5725\n",
            "Epoch 8/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9882 - acc: 0.6107\n",
            "Epoch 9/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9693 - acc: 0.6412\n",
            "Epoch 10/500\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9512 - acc: 0.6565\n",
            "Epoch 11/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9338 - acc: 0.6870\n",
            "Epoch 12/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9172 - acc: 0.7099\n",
            "Epoch 13/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9011 - acc: 0.7252\n",
            "Epoch 14/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8857 - acc: 0.7481\n",
            "Epoch 15/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8711 - acc: 0.7557\n",
            "Epoch 16/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8566 - acc: 0.7557\n",
            "Epoch 17/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8426 - acc: 0.7634\n",
            "Epoch 18/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8290 - acc: 0.7710\n",
            "Epoch 19/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.8157 - acc: 0.7710\n",
            "Epoch 20/500\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.8031 - acc: 0.7786\n",
            "Epoch 21/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.7909 - acc: 0.7786\n",
            "Epoch 22/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.7789 - acc: 0.8092\n",
            "Epoch 23/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.7673 - acc: 0.8092\n",
            "Epoch 24/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.7559 - acc: 0.8168\n",
            "Epoch 25/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.7449 - acc: 0.8168\n",
            "Epoch 26/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.7341 - acc: 0.8321\n",
            "Epoch 27/500\n",
            "131/131 [==============================] - 0s 136us/step - loss: 0.7236 - acc: 0.8321\n",
            "Epoch 28/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7133 - acc: 0.8397\n",
            "Epoch 29/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.7032 - acc: 0.8397\n",
            "Epoch 30/500\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.6934 - acc: 0.8397\n",
            "Epoch 31/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.6838 - acc: 0.8397\n",
            "Epoch 32/500\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.6743 - acc: 0.8473\n",
            "Epoch 33/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.6651 - acc: 0.8473\n",
            "Epoch 34/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.6560 - acc: 0.8550\n",
            "Epoch 35/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.6472 - acc: 0.8626\n",
            "Epoch 36/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6385 - acc: 0.8626\n",
            "Epoch 37/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.6300 - acc: 0.8626\n",
            "Epoch 38/500\n",
            "131/131 [==============================] - 0s 134us/step - loss: 0.6215 - acc: 0.8626\n",
            "Epoch 39/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6131 - acc: 0.8626\n",
            "Epoch 40/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.6050 - acc: 0.8626\n",
            "Epoch 41/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.5969 - acc: 0.8626\n",
            "Epoch 42/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.5889 - acc: 0.8626\n",
            "Epoch 43/500\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.5813 - acc: 0.8626\n",
            "Epoch 44/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.5736 - acc: 0.8626\n",
            "Epoch 45/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.5660 - acc: 0.8626\n",
            "Epoch 46/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.5586 - acc: 0.8626\n",
            "Epoch 47/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5514 - acc: 0.8626\n",
            "Epoch 48/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.5442 - acc: 0.8626\n",
            "Epoch 49/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.5371 - acc: 0.8626\n",
            "Epoch 50/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.5302 - acc: 0.8626\n",
            "Epoch 51/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.5233 - acc: 0.8626\n",
            "Epoch 52/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5166 - acc: 0.8626\n",
            "Epoch 53/500\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.5100 - acc: 0.8626\n",
            "Epoch 54/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5036 - acc: 0.8626\n",
            "Epoch 55/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4971 - acc: 0.8626\n",
            "Epoch 56/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4907 - acc: 0.8626\n",
            "Epoch 57/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4846 - acc: 0.8626\n",
            "Epoch 58/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4784 - acc: 0.8626\n",
            "Epoch 59/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4725 - acc: 0.8626\n",
            "Epoch 60/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.4665 - acc: 0.8626\n",
            "Epoch 61/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.4608 - acc: 0.8626\n",
            "Epoch 62/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.4551 - acc: 0.8626\n",
            "Epoch 63/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4494 - acc: 0.8626\n",
            "Epoch 64/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.4439 - acc: 0.8626\n",
            "Epoch 65/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4383 - acc: 0.8626\n",
            "Epoch 66/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.4329 - acc: 0.8626\n",
            "Epoch 67/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4276 - acc: 0.8626\n",
            "Epoch 68/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.4223 - acc: 0.8626\n",
            "Epoch 69/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.4171 - acc: 0.8626\n",
            "Epoch 70/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.4121 - acc: 0.8855\n",
            "Epoch 71/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4071 - acc: 0.9008\n",
            "Epoch 72/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4021 - acc: 0.9389\n",
            "Epoch 73/500\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.3971 - acc: 0.9389\n",
            "Epoch 74/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.3923 - acc: 0.9542\n",
            "Epoch 75/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.3876 - acc: 0.9542\n",
            "Epoch 76/500\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.3829 - acc: 0.9618\n",
            "Epoch 77/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.3783 - acc: 0.9618\n",
            "Epoch 78/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3738 - acc: 0.9618\n",
            "Epoch 79/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.3693 - acc: 0.9618\n",
            "Epoch 80/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3649 - acc: 0.9618\n",
            "Epoch 81/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.3606 - acc: 0.9618\n",
            "Epoch 82/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3563 - acc: 0.9695\n",
            "Epoch 83/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.3521 - acc: 0.9695\n",
            "Epoch 84/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3480 - acc: 0.9695\n",
            "Epoch 85/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3439 - acc: 0.9695\n",
            "Epoch 86/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.3399 - acc: 0.9695\n",
            "Epoch 87/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3359 - acc: 0.9695\n",
            "Epoch 88/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.3320 - acc: 0.9695\n",
            "Epoch 89/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.3280 - acc: 0.9695\n",
            "Epoch 90/500\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3243 - acc: 0.9771\n",
            "Epoch 91/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.3205 - acc: 0.9771\n",
            "Epoch 92/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.3168 - acc: 0.9771\n",
            "Epoch 93/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.3131 - acc: 0.9771\n",
            "Epoch 94/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.3094 - acc: 0.9771\n",
            "Epoch 95/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.3058 - acc: 0.9771\n",
            "Epoch 96/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.3023 - acc: 0.9771\n",
            "Epoch 97/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.2987 - acc: 0.9847\n",
            "Epoch 98/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2953 - acc: 0.9847\n",
            "Epoch 99/500\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.2919 - acc: 0.9847\n",
            "Epoch 100/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2887 - acc: 0.9924\n",
            "Epoch 101/500\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.2854 - acc: 0.9924\n",
            "Epoch 102/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.2822 - acc: 0.9924\n",
            "Epoch 103/500\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.2791 - acc: 0.9924\n",
            "Epoch 104/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2759 - acc: 0.9924\n",
            "Epoch 105/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2727 - acc: 0.9924\n",
            "Epoch 106/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2697 - acc: 0.9924\n",
            "Epoch 107/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2667 - acc: 0.9924\n",
            "Epoch 108/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2638 - acc: 0.9924\n",
            "Epoch 109/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.2608 - acc: 0.9924\n",
            "Epoch 110/500\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.2579 - acc: 0.9924\n",
            "Epoch 111/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.2551 - acc: 0.9924\n",
            "Epoch 112/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.2524 - acc: 0.9924\n",
            "Epoch 113/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.2496 - acc: 0.9924\n",
            "Epoch 114/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.2469 - acc: 0.9924\n",
            "Epoch 115/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.2442 - acc: 0.9924\n",
            "Epoch 116/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2415 - acc: 0.9924\n",
            "Epoch 117/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2389 - acc: 0.9924\n",
            "Epoch 118/500\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.2363 - acc: 0.9924\n",
            "Epoch 119/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.2338 - acc: 0.9924\n",
            "Epoch 120/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.2313 - acc: 0.9924\n",
            "Epoch 121/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.2289 - acc: 0.9924\n",
            "Epoch 122/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.2265 - acc: 0.9924\n",
            "Epoch 123/500\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.2241 - acc: 0.9924\n",
            "Epoch 124/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.2218 - acc: 0.9924\n",
            "Epoch 125/500\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.2195 - acc: 0.9924\n",
            "Epoch 126/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2171 - acc: 0.9924\n",
            "Epoch 127/500\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.2148 - acc: 0.9924\n",
            "Epoch 128/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.2126 - acc: 0.9924\n",
            "Epoch 129/500\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.2104 - acc: 0.9924\n",
            "Epoch 130/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2083 - acc: 0.9924\n",
            "Epoch 131/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2062 - acc: 0.9924\n",
            "Epoch 132/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.2041 - acc: 0.9924\n",
            "Epoch 133/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2021 - acc: 0.9924\n",
            "Epoch 134/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2001 - acc: 0.9924\n",
            "Epoch 135/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1981 - acc: 0.9924\n",
            "Epoch 136/500\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.1961 - acc: 0.9924\n",
            "Epoch 137/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.1942 - acc: 0.9924\n",
            "Epoch 138/500\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.1923 - acc: 0.9924\n",
            "Epoch 139/500\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.1904 - acc: 0.9924\n",
            "Epoch 140/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.1886 - acc: 0.9924\n",
            "Epoch 141/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.1867 - acc: 0.9924\n",
            "Epoch 142/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.1849 - acc: 0.9924\n",
            "Epoch 143/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.1831 - acc: 0.9924\n",
            "Epoch 144/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.1814 - acc: 0.9924\n",
            "Epoch 145/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.1797 - acc: 0.9924\n",
            "Epoch 146/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.1780 - acc: 0.9924\n",
            "Epoch 147/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.1763 - acc: 0.9924\n",
            "Epoch 148/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.1746 - acc: 0.9924\n",
            "Epoch 149/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.1730 - acc: 0.9924\n",
            "Epoch 150/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1713 - acc: 0.9924\n",
            "Epoch 151/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1697 - acc: 0.9924\n",
            "Epoch 152/500\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.1682 - acc: 0.9924\n",
            "Epoch 153/500\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.1667 - acc: 0.9924\n",
            "Epoch 154/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1652 - acc: 0.9924\n",
            "Epoch 155/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.1637 - acc: 0.9924\n",
            "Epoch 156/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1622 - acc: 0.9924\n",
            "Epoch 157/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1608 - acc: 0.9924\n",
            "Epoch 158/500\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.1594 - acc: 0.9924\n",
            "Epoch 159/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1580 - acc: 0.9924\n",
            "Epoch 160/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.1566 - acc: 0.9924\n",
            "Epoch 161/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.1553 - acc: 0.9924\n",
            "Epoch 162/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.1540 - acc: 0.9924\n",
            "Epoch 163/500\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.1526 - acc: 0.9924\n",
            "Epoch 164/500\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.1513 - acc: 0.9924\n",
            "Epoch 165/500\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.1500 - acc: 0.9924\n",
            "Epoch 166/500\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.1488 - acc: 0.9924\n",
            "Epoch 167/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1475 - acc: 0.9924\n",
            "Epoch 168/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1463 - acc: 0.9924\n",
            "Epoch 169/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.1451 - acc: 0.9924\n",
            "Epoch 170/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1440 - acc: 0.9924\n",
            "Epoch 171/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.1428 - acc: 0.9924\n",
            "Epoch 172/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1417 - acc: 0.9924\n",
            "Epoch 173/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.1405 - acc: 0.9924\n",
            "Epoch 174/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.1394 - acc: 0.9924\n",
            "Epoch 175/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1383 - acc: 0.9924\n",
            "Epoch 176/500\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.1372 - acc: 0.9924\n",
            "Epoch 177/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.1362 - acc: 0.9924\n",
            "Epoch 178/500\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.1351 - acc: 0.9924\n",
            "Epoch 179/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1341 - acc: 0.9924\n",
            "Epoch 180/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.1331 - acc: 0.9924\n",
            "Epoch 181/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1321 - acc: 0.9924\n",
            "Epoch 182/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.1311 - acc: 0.9924\n",
            "Epoch 183/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.1301 - acc: 0.9924\n",
            "Epoch 184/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1291 - acc: 0.9924\n",
            "Epoch 185/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1281 - acc: 0.9924\n",
            "Epoch 186/500\n",
            "131/131 [==============================] - 0s 134us/step - loss: 0.1272 - acc: 0.9924\n",
            "Epoch 187/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1263 - acc: 0.9924\n",
            "Epoch 188/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1253 - acc: 0.9924\n",
            "Epoch 189/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.1244 - acc: 0.9924\n",
            "Epoch 190/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.1236 - acc: 0.9924\n",
            "Epoch 191/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.1227 - acc: 0.9924\n",
            "Epoch 192/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1218 - acc: 0.9924\n",
            "Epoch 193/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1210 - acc: 0.9924\n",
            "Epoch 194/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.1201 - acc: 0.9924\n",
            "Epoch 195/500\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.1193 - acc: 0.9924\n",
            "Epoch 196/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.1185 - acc: 0.9924\n",
            "Epoch 197/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.1177 - acc: 0.9924\n",
            "Epoch 198/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1169 - acc: 0.9924\n",
            "Epoch 199/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1160 - acc: 0.9924\n",
            "Epoch 200/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.1153 - acc: 0.9924\n",
            "Epoch 201/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.1145 - acc: 0.9924\n",
            "Epoch 202/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1137 - acc: 0.9924\n",
            "Epoch 203/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.1130 - acc: 0.9924\n",
            "Epoch 204/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.1122 - acc: 0.9924\n",
            "Epoch 205/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.1115 - acc: 0.9924\n",
            "Epoch 206/500\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.1108 - acc: 0.9924\n",
            "Epoch 207/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.1100 - acc: 0.9924\n",
            "Epoch 208/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1093 - acc: 0.9924\n",
            "Epoch 209/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1086 - acc: 0.9924\n",
            "Epoch 210/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.1079 - acc: 0.9924\n",
            "Epoch 211/500\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.1073 - acc: 0.9924\n",
            "Epoch 212/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1066 - acc: 0.9924\n",
            "Epoch 213/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.1060 - acc: 0.9924\n",
            "Epoch 214/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1053 - acc: 0.9924\n",
            "Epoch 215/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1047 - acc: 0.9924\n",
            "Epoch 216/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.1041 - acc: 0.9924\n",
            "Epoch 217/500\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.1035 - acc: 0.9924\n",
            "Epoch 218/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.1029 - acc: 0.9924\n",
            "Epoch 219/500\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1023 - acc: 0.9924\n",
            "Epoch 220/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1017 - acc: 0.9924\n",
            "Epoch 221/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.1011 - acc: 0.9924\n",
            "Epoch 222/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.1005 - acc: 0.9924\n",
            "Epoch 223/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.0999 - acc: 0.9924\n",
            "Epoch 224/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0994 - acc: 0.9924\n",
            "Epoch 225/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0988 - acc: 0.9924\n",
            "Epoch 226/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.0982 - acc: 0.9924\n",
            "Epoch 227/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0977 - acc: 0.9924\n",
            "Epoch 228/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.0972 - acc: 0.9924\n",
            "Epoch 229/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.0966 - acc: 0.9924\n",
            "Epoch 230/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0961 - acc: 0.9924\n",
            "Epoch 231/500\n",
            "131/131 [==============================] - 0s 133us/step - loss: 0.0956 - acc: 0.9924\n",
            "Epoch 232/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0951 - acc: 0.9924\n",
            "Epoch 233/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0946 - acc: 0.9924\n",
            "Epoch 234/500\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.0941 - acc: 0.9924\n",
            "Epoch 235/500\n",
            "131/131 [==============================] - 0s 132us/step - loss: 0.0936 - acc: 0.9924\n",
            "Epoch 236/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0931 - acc: 0.9924\n",
            "Epoch 237/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0926 - acc: 0.9924\n",
            "Epoch 238/500\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.0921 - acc: 0.9924\n",
            "Epoch 239/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0916 - acc: 0.9924\n",
            "Epoch 240/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0912 - acc: 0.9924\n",
            "Epoch 241/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0907 - acc: 0.9924\n",
            "Epoch 242/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0903 - acc: 0.9924\n",
            "Epoch 243/500\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.0898 - acc: 0.9924\n",
            "Epoch 244/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0894 - acc: 0.9924\n",
            "Epoch 245/500\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.0889 - acc: 0.9924\n",
            "Epoch 246/500\n",
            "131/131 [==============================] - 0s 137us/step - loss: 0.0885 - acc: 0.9924\n",
            "Epoch 247/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0880 - acc: 0.9924\n",
            "Epoch 248/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0876 - acc: 0.9924\n",
            "Epoch 249/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.0872 - acc: 0.9924\n",
            "Epoch 250/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0868 - acc: 0.9924\n",
            "Epoch 251/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.0863 - acc: 0.9924\n",
            "Epoch 252/500\n",
            "131/131 [==============================] - 0s 133us/step - loss: 0.0859 - acc: 0.9924\n",
            "Epoch 253/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.0855 - acc: 0.9924\n",
            "Epoch 254/500\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.0851 - acc: 0.9924\n",
            "Epoch 255/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.0847 - acc: 0.9924\n",
            "Epoch 256/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0843 - acc: 0.9924\n",
            "Epoch 257/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.0839 - acc: 0.9924\n",
            "Epoch 258/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0836 - acc: 0.9924\n",
            "Epoch 259/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0832 - acc: 0.9924\n",
            "Epoch 260/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0828 - acc: 0.9924\n",
            "Epoch 261/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.0824 - acc: 0.9924\n",
            "Epoch 262/500\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.0821 - acc: 0.9924\n",
            "Epoch 263/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.0817 - acc: 0.9924\n",
            "Epoch 264/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.0813 - acc: 0.9924\n",
            "Epoch 265/500\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.0810 - acc: 0.9924\n",
            "Epoch 266/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0806 - acc: 0.9924\n",
            "Epoch 267/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.0803 - acc: 0.9924\n",
            "Epoch 268/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.0799 - acc: 0.9924\n",
            "Epoch 269/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.0796 - acc: 0.9924\n",
            "Epoch 270/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.0792 - acc: 0.9924\n",
            "Epoch 271/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0789 - acc: 0.9924\n",
            "Epoch 272/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0786 - acc: 0.9924\n",
            "Epoch 273/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.0782 - acc: 0.9924\n",
            "Epoch 274/500\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.0779 - acc: 0.9924\n",
            "Epoch 275/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.0776 - acc: 0.9924\n",
            "Epoch 276/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0773 - acc: 0.9924\n",
            "Epoch 277/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.0770 - acc: 0.9924\n",
            "Epoch 278/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0766 - acc: 0.9924\n",
            "Epoch 279/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0763 - acc: 0.9924\n",
            "Epoch 280/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0760 - acc: 0.9924\n",
            "Epoch 281/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.0757 - acc: 0.9924\n",
            "Epoch 282/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0754 - acc: 0.9924\n",
            "Epoch 283/500\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.0751 - acc: 0.9924\n",
            "Epoch 284/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0749 - acc: 0.9924\n",
            "Epoch 285/500\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.0746 - acc: 0.9924\n",
            "Epoch 286/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0743 - acc: 0.9924\n",
            "Epoch 287/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0740 - acc: 0.9924\n",
            "Epoch 288/500\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.0737 - acc: 0.9924\n",
            "Epoch 289/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0734 - acc: 0.9924\n",
            "Epoch 290/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.0731 - acc: 0.9924\n",
            "Epoch 291/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0729 - acc: 0.9924\n",
            "Epoch 292/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0726 - acc: 0.9924\n",
            "Epoch 293/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.0723 - acc: 0.9924\n",
            "Epoch 294/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.0721 - acc: 0.9924\n",
            "Epoch 295/500\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.0718 - acc: 0.9924\n",
            "Epoch 296/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0716 - acc: 0.9924\n",
            "Epoch 297/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.0713 - acc: 0.9924\n",
            "Epoch 298/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0710 - acc: 0.9924\n",
            "Epoch 299/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0708 - acc: 0.9924\n",
            "Epoch 300/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0705 - acc: 0.9924\n",
            "Epoch 301/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0703 - acc: 0.9924\n",
            "Epoch 302/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.0700 - acc: 0.9924\n",
            "Epoch 303/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0698 - acc: 0.9924\n",
            "Epoch 304/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0695 - acc: 0.9924\n",
            "Epoch 305/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.0693 - acc: 0.9924\n",
            "Epoch 306/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0690 - acc: 0.9924\n",
            "Epoch 307/500\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.0688 - acc: 0.9924\n",
            "Epoch 308/500\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.0686 - acc: 0.9924\n",
            "Epoch 309/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0683 - acc: 0.9924\n",
            "Epoch 310/500\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.0681 - acc: 0.9924\n",
            "Epoch 311/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0679 - acc: 0.9924\n",
            "Epoch 312/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0676 - acc: 0.9924\n",
            "Epoch 313/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0674 - acc: 0.9924\n",
            "Epoch 314/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.0672 - acc: 0.9924\n",
            "Epoch 315/500\n",
            "131/131 [==============================] - 0s 131us/step - loss: 0.0670 - acc: 0.9924\n",
            "Epoch 316/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0667 - acc: 0.9924\n",
            "Epoch 317/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0665 - acc: 0.9924\n",
            "Epoch 318/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.0663 - acc: 0.9924\n",
            "Epoch 319/500\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.0661 - acc: 0.9924\n",
            "Epoch 320/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.0659 - acc: 0.9924\n",
            "Epoch 321/500\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.0657 - acc: 0.9924\n",
            "Epoch 322/500\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.0655 - acc: 0.9924\n",
            "Epoch 323/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0653 - acc: 0.9924\n",
            "Epoch 324/500\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.0651 - acc: 0.9924\n",
            "Epoch 325/500\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.0649 - acc: 0.9924\n",
            "Epoch 326/500\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.0646 - acc: 0.9924\n",
            "Epoch 327/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0644 - acc: 0.9924\n",
            "Epoch 328/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0642 - acc: 0.9924\n",
            "Epoch 329/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.0641 - acc: 0.9924\n",
            "Epoch 330/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.0639 - acc: 0.9924\n",
            "Epoch 331/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0637 - acc: 0.9924\n",
            "Epoch 332/500\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.0635 - acc: 0.9924\n",
            "Epoch 333/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0633 - acc: 0.9924\n",
            "Epoch 334/500\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.0631 - acc: 0.9924\n",
            "Epoch 335/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0629 - acc: 0.9924\n",
            "Epoch 336/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0627 - acc: 0.9924\n",
            "Epoch 337/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0625 - acc: 0.9924\n",
            "Epoch 338/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0624 - acc: 0.9924\n",
            "Epoch 339/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0622 - acc: 0.9924\n",
            "Epoch 340/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.0620 - acc: 0.9924\n",
            "Epoch 341/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0618 - acc: 0.9924\n",
            "Epoch 342/500\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.0616 - acc: 0.9924\n",
            "Epoch 343/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0615 - acc: 0.9924\n",
            "Epoch 344/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0613 - acc: 0.9924\n",
            "Epoch 345/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.0611 - acc: 0.9924\n",
            "Epoch 346/500\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.0610 - acc: 0.9924\n",
            "Epoch 347/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0608 - acc: 0.9924\n",
            "Epoch 348/500\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.0606 - acc: 0.9924\n",
            "Epoch 349/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.0604 - acc: 0.9924\n",
            "Epoch 350/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0603 - acc: 0.9924\n",
            "Epoch 351/500\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.0601 - acc: 0.9924\n",
            "Epoch 352/500\n",
            "131/131 [==============================] - 0s 136us/step - loss: 0.0599 - acc: 0.9924\n",
            "Epoch 353/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.0598 - acc: 0.9924\n",
            "Epoch 354/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.0596 - acc: 0.9924\n",
            "Epoch 355/500\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.0594 - acc: 0.9924\n",
            "Epoch 356/500\n",
            "131/131 [==============================] - 0s 132us/step - loss: 0.0593 - acc: 0.9924\n",
            "Epoch 357/500\n",
            "131/131 [==============================] - 0s 135us/step - loss: 0.0591 - acc: 0.9924\n",
            "Epoch 358/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0590 - acc: 0.9924\n",
            "Epoch 359/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0588 - acc: 0.9924\n",
            "Epoch 360/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.0587 - acc: 0.9924\n",
            "Epoch 361/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0585 - acc: 0.9924\n",
            "Epoch 362/500\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.0584 - acc: 0.9924\n",
            "Epoch 363/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.0582 - acc: 0.9924\n",
            "Epoch 364/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.0581 - acc: 0.9924\n",
            "Epoch 365/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0579 - acc: 0.9924\n",
            "Epoch 366/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0578 - acc: 0.9924\n",
            "Epoch 367/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.0576 - acc: 0.9924\n",
            "Epoch 368/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0575 - acc: 0.9924\n",
            "Epoch 369/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.0573 - acc: 0.9924\n",
            "Epoch 370/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.0572 - acc: 0.9924\n",
            "Epoch 371/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0570 - acc: 0.9924\n",
            "Epoch 372/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0569 - acc: 0.9924\n",
            "Epoch 373/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0567 - acc: 0.9924\n",
            "Epoch 374/500\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.0566 - acc: 0.9924\n",
            "Epoch 375/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0565 - acc: 0.9924\n",
            "Epoch 376/500\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.0563 - acc: 0.9924\n",
            "Epoch 377/500\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.0562 - acc: 0.9924\n",
            "Epoch 378/500\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.0560 - acc: 0.9924\n",
            "Epoch 379/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0559 - acc: 0.9924\n",
            "Epoch 380/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0558 - acc: 0.9924\n",
            "Epoch 381/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0556 - acc: 0.9924\n",
            "Epoch 382/500\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.0555 - acc: 0.9924\n",
            "Epoch 383/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0554 - acc: 0.9924\n",
            "Epoch 384/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0552 - acc: 0.9924\n",
            "Epoch 385/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0551 - acc: 0.9924\n",
            "Epoch 386/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0550 - acc: 0.9924\n",
            "Epoch 387/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0548 - acc: 0.9924\n",
            "Epoch 388/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0547 - acc: 0.9924\n",
            "Epoch 389/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.0546 - acc: 0.9924\n",
            "Epoch 390/500\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.0545 - acc: 0.9924\n",
            "Epoch 391/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0543 - acc: 0.9924\n",
            "Epoch 392/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0542 - acc: 0.9924\n",
            "Epoch 393/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0541 - acc: 0.9924\n",
            "Epoch 394/500\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.0540 - acc: 0.9924\n",
            "Epoch 395/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0538 - acc: 0.9924\n",
            "Epoch 396/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0537 - acc: 0.9924\n",
            "Epoch 397/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0536 - acc: 0.9924\n",
            "Epoch 398/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0535 - acc: 0.9924\n",
            "Epoch 399/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0534 - acc: 0.9924\n",
            "Epoch 400/500\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.0532 - acc: 0.9924\n",
            "Epoch 401/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.0531 - acc: 0.9924\n",
            "Epoch 402/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0530 - acc: 0.9924\n",
            "Epoch 403/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0529 - acc: 0.9924\n",
            "Epoch 404/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0528 - acc: 0.9924\n",
            "Epoch 405/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0526 - acc: 0.9924\n",
            "Epoch 406/500\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.0525 - acc: 0.9924\n",
            "Epoch 407/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0524 - acc: 0.9924\n",
            "Epoch 408/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0523 - acc: 0.9924\n",
            "Epoch 409/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.0522 - acc: 0.9924\n",
            "Epoch 410/500\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.0521 - acc: 0.9924\n",
            "Epoch 411/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0519 - acc: 0.9924\n",
            "Epoch 412/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0518 - acc: 0.9924\n",
            "Epoch 413/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0517 - acc: 0.9924\n",
            "Epoch 414/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0516 - acc: 0.9924\n",
            "Epoch 415/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0515 - acc: 0.9924\n",
            "Epoch 416/500\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.0514 - acc: 0.9924\n",
            "Epoch 417/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0513 - acc: 0.9924\n",
            "Epoch 418/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0512 - acc: 0.9924\n",
            "Epoch 419/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0511 - acc: 0.9924\n",
            "Epoch 420/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0510 - acc: 0.9924\n",
            "Epoch 421/500\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.0509 - acc: 0.9924\n",
            "Epoch 422/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.0508 - acc: 0.9924\n",
            "Epoch 423/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0506 - acc: 0.9924\n",
            "Epoch 424/500\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.0505 - acc: 0.9924\n",
            "Epoch 425/500\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.0504 - acc: 0.9924\n",
            "Epoch 426/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.0503 - acc: 0.9924\n",
            "Epoch 427/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0502 - acc: 0.9924\n",
            "Epoch 428/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.0501 - acc: 0.9924\n",
            "Epoch 429/500\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.0500 - acc: 0.9924\n",
            "Epoch 430/500\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.0499 - acc: 0.9924\n",
            "Epoch 431/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0498 - acc: 0.9924\n",
            "Epoch 432/500\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.0497 - acc: 0.9924\n",
            "Epoch 433/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0496 - acc: 0.9924\n",
            "Epoch 434/500\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.0496 - acc: 0.9924\n",
            "Epoch 435/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0494 - acc: 0.9924\n",
            "Epoch 436/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0493 - acc: 0.9924\n",
            "Epoch 437/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0493 - acc: 0.9924\n",
            "Epoch 438/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0492 - acc: 0.9924\n",
            "Epoch 439/500\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.0491 - acc: 0.9924\n",
            "Epoch 440/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0490 - acc: 0.9924\n",
            "Epoch 441/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0489 - acc: 0.9924\n",
            "Epoch 442/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0488 - acc: 0.9924\n",
            "Epoch 443/500\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.0487 - acc: 0.9924\n",
            "Epoch 444/500\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.0486 - acc: 0.9924\n",
            "Epoch 445/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0485 - acc: 0.9924\n",
            "Epoch 446/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0484 - acc: 0.9924\n",
            "Epoch 447/500\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.0483 - acc: 0.9924\n",
            "Epoch 448/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0482 - acc: 0.9924\n",
            "Epoch 449/500\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.0481 - acc: 0.9924\n",
            "Epoch 450/500\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.0480 - acc: 0.9924\n",
            "Epoch 451/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0479 - acc: 0.9924\n",
            "Epoch 452/500\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.0479 - acc: 0.9924\n",
            "Epoch 453/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0478 - acc: 0.9924\n",
            "Epoch 454/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.0477 - acc: 0.9924\n",
            "Epoch 455/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.0476 - acc: 0.9924\n",
            "Epoch 456/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0475 - acc: 0.9924\n",
            "Epoch 457/500\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.0474 - acc: 0.9924\n",
            "Epoch 458/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0473 - acc: 0.9924\n",
            "Epoch 459/500\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.0472 - acc: 0.9924\n",
            "Epoch 460/500\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.0472 - acc: 0.9924\n",
            "Epoch 461/500\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.0471 - acc: 0.9924\n",
            "Epoch 462/500\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.0470 - acc: 0.9924\n",
            "Epoch 463/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.0469 - acc: 0.9924\n",
            "Epoch 464/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0468 - acc: 0.9924\n",
            "Epoch 465/500\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.0467 - acc: 0.9924\n",
            "Epoch 466/500\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.0467 - acc: 0.9924\n",
            "Epoch 467/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0466 - acc: 0.9924\n",
            "Epoch 468/500\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.0465 - acc: 0.9924\n",
            "Epoch 469/500\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.0464 - acc: 0.9924\n",
            "Epoch 470/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.0463 - acc: 0.9924\n",
            "Epoch 471/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0463 - acc: 0.9924\n",
            "Epoch 472/500\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.0462 - acc: 0.9924\n",
            "Epoch 473/500\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.0461 - acc: 0.9924\n",
            "Epoch 474/500\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.0460 - acc: 0.9924\n",
            "Epoch 475/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0460 - acc: 0.9924\n",
            "Epoch 476/500\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.0459 - acc: 0.9924\n",
            "Epoch 477/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0458 - acc: 0.9924\n",
            "Epoch 478/500\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.0457 - acc: 0.9924\n",
            "Epoch 479/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.0456 - acc: 0.9924\n",
            "Epoch 480/500\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.0456 - acc: 0.9924\n",
            "Epoch 481/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.0455 - acc: 0.9924\n",
            "Epoch 482/500\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.0454 - acc: 0.9924\n",
            "Epoch 483/500\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.0453 - acc: 0.9924\n",
            "Epoch 484/500\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.0452 - acc: 0.9924\n",
            "Epoch 485/500\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.0452 - acc: 0.9924\n",
            "Epoch 486/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0451 - acc: 0.9924\n",
            "Epoch 487/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.0450 - acc: 0.9924\n",
            "Epoch 488/500\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.0449 - acc: 0.9924\n",
            "Epoch 489/500\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.0449 - acc: 0.9924\n",
            "Epoch 490/500\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.0448 - acc: 0.9924\n",
            "Epoch 491/500\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.0447 - acc: 0.9924\n",
            "Epoch 492/500\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.0447 - acc: 0.9924\n",
            "Epoch 493/500\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.0446 - acc: 0.9924\n",
            "Epoch 494/500\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.0445 - acc: 0.9924\n",
            "Epoch 495/500\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.0445 - acc: 0.9924\n",
            "Epoch 496/500\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.0444 - acc: 0.9924\n",
            "Epoch 497/500\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.0443 - acc: 0.9924\n",
            "Epoch 498/500\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.0442 - acc: 0.9924\n",
            "Epoch 499/500\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.0442 - acc: 0.9924\n",
            "Epoch 500/500\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.0441 - acc: 0.9924\n",
            "34/34 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35e1976b-f813-4182-8842-6b839d99bf30",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8bf8aa8a-b09e-41b1-e882-773d767cb593",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14705882352941177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZN9I7JNNUyS",
        "colab_type": "text"
      },
      "source": [
        "# Prova remove correlated features with treshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wACff-R4Ib",
        "colab_type": "text"
      },
      "source": [
        "https://campus.datacamp.com/courses/dimensionality-reduction-in-python/feature-selection-i-selecting-for-feature-information?ex=14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNufM8B3NYs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a positive correlation matrix\n",
        "corr_df = train_data_stand.corr().abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8ShRUvN63m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create and apply mask\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DgfSECpOwPu",
        "colab_type": "code",
        "outputId": "54c24399-ad0a-4f0f-f1fe-ca0a6c9c14bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [False,  True,  True, ...,  True,  True,  True],\n",
              "       [False, False,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [False, False, False, ...,  True,  True,  True],\n",
              "       [False, False, False, ..., False,  True,  True],\n",
              "       [False, False, False, ..., False, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xQLptVOw6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri_df = corr_df.mask(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDvePEPO8mn",
        "colab_type": "code",
        "outputId": "573a9f08-c0eb-4cb5-8df8-9a6cb24987e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tri_df"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>VoxelVolume</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <td>0.821982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MeshVolume</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <td>0.785606</td>\n",
              "      <td>0.964760</td>\n",
              "      <td>0.785457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sphericity</th>\n",
              "      <td>0.329751</td>\n",
              "      <td>0.678628</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.694906</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coarseness</th>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.586992</td>\n",
              "      <td>0.401370</td>\n",
              "      <td>0.546184</td>\n",
              "      <td>0.356708</td>\n",
              "      <td>0.569919</td>\n",
              "      <td>0.048602</td>\n",
              "      <td>0.813595</td>\n",
              "      <td>0.614989</td>\n",
              "      <td>0.094676</td>\n",
              "      <td>0.471734</td>\n",
              "      <td>0.598815</td>\n",
              "      <td>0.567029</td>\n",
              "      <td>0.554227</td>\n",
              "      <td>0.192052</td>\n",
              "      <td>0.605729</td>\n",
              "      <td>0.246122</td>\n",
              "      <td>0.406714</td>\n",
              "      <td>0.351606</td>\n",
              "      <td>0.669357</td>\n",
              "      <td>0.183179</td>\n",
              "      <td>0.731964</td>\n",
              "      <td>0.417524</td>\n",
              "      <td>0.077465</td>\n",
              "      <td>0.379163</td>\n",
              "      <td>0.437111</td>\n",
              "      <td>0.787425</td>\n",
              "      <td>0.182358</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>0.119708</td>\n",
              "      <td>0.324753</td>\n",
              "      <td>0.661086</td>\n",
              "      <td>0.326758</td>\n",
              "      <td>0.468498</td>\n",
              "      <td>0.481677</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.319663</td>\n",
              "      <td>0.667850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350082</td>\n",
              "      <td>0.292163</td>\n",
              "      <td>0.539925</td>\n",
              "      <td>0.345026</td>\n",
              "      <td>0.148147</td>\n",
              "      <td>0.251303</td>\n",
              "      <td>0.399211</td>\n",
              "      <td>0.336092</td>\n",
              "      <td>0.360524</td>\n",
              "      <td>0.381037</td>\n",
              "      <td>0.556410</td>\n",
              "      <td>0.412333</td>\n",
              "      <td>0.521415</td>\n",
              "      <td>0.480184</td>\n",
              "      <td>0.512788</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.194296</td>\n",
              "      <td>0.595126</td>\n",
              "      <td>0.552550</td>\n",
              "      <td>0.201612</td>\n",
              "      <td>0.272599</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.374723</td>\n",
              "      <td>0.426885</td>\n",
              "      <td>0.417651</td>\n",
              "      <td>0.272696</td>\n",
              "      <td>0.501061</td>\n",
              "      <td>0.667822</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.511486</td>\n",
              "      <td>0.366279</td>\n",
              "      <td>0.704958</td>\n",
              "      <td>0.523589</td>\n",
              "      <td>0.754391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Complexity</th>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.257297</td>\n",
              "      <td>0.277025</td>\n",
              "      <td>0.229141</td>\n",
              "      <td>0.182615</td>\n",
              "      <td>0.268450</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.214902</td>\n",
              "      <td>0.256226</td>\n",
              "      <td>0.019975</td>\n",
              "      <td>0.283738</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>0.441669</td>\n",
              "      <td>0.213055</td>\n",
              "      <td>0.423394</td>\n",
              "      <td>0.392267</td>\n",
              "      <td>0.103854</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.422629</td>\n",
              "      <td>0.058114</td>\n",
              "      <td>0.198407</td>\n",
              "      <td>0.214767</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.125140</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.275899</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.322120</td>\n",
              "      <td>0.410309</td>\n",
              "      <td>0.276311</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>0.275068</td>\n",
              "      <td>0.221594</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.125930</td>\n",
              "      <td>0.298911</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267658</td>\n",
              "      <td>0.198372</td>\n",
              "      <td>0.116352</td>\n",
              "      <td>0.255976</td>\n",
              "      <td>0.464523</td>\n",
              "      <td>0.267090</td>\n",
              "      <td>0.228132</td>\n",
              "      <td>0.195523</td>\n",
              "      <td>0.139835</td>\n",
              "      <td>0.198482</td>\n",
              "      <td>0.296969</td>\n",
              "      <td>0.359280</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>0.168824</td>\n",
              "      <td>0.228273</td>\n",
              "      <td>0.355694</td>\n",
              "      <td>0.220089</td>\n",
              "      <td>0.151630</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.011797</td>\n",
              "      <td>0.469854</td>\n",
              "      <td>0.036549</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.274863</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.351102</td>\n",
              "      <td>0.063147</td>\n",
              "      <td>0.152285</td>\n",
              "      <td>0.074162</td>\n",
              "      <td>0.313890</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.083326</td>\n",
              "      <td>0.166795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strength</th>\n",
              "      <td>0.510155</td>\n",
              "      <td>0.642559</td>\n",
              "      <td>0.509976</td>\n",
              "      <td>0.607597</td>\n",
              "      <td>0.339595</td>\n",
              "      <td>0.645544</td>\n",
              "      <td>0.021965</td>\n",
              "      <td>0.758610</td>\n",
              "      <td>0.670268</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.562243</td>\n",
              "      <td>0.659201</td>\n",
              "      <td>0.624228</td>\n",
              "      <td>0.620794</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.388883</td>\n",
              "      <td>0.341710</td>\n",
              "      <td>0.538344</td>\n",
              "      <td>0.425689</td>\n",
              "      <td>0.458791</td>\n",
              "      <td>0.226763</td>\n",
              "      <td>0.486770</td>\n",
              "      <td>0.290047</td>\n",
              "      <td>0.085559</td>\n",
              "      <td>0.272032</td>\n",
              "      <td>0.325476</td>\n",
              "      <td>0.564383</td>\n",
              "      <td>0.104481</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.099738</td>\n",
              "      <td>0.184149</td>\n",
              "      <td>0.141523</td>\n",
              "      <td>0.432332</td>\n",
              "      <td>0.139576</td>\n",
              "      <td>0.296993</td>\n",
              "      <td>0.271914</td>\n",
              "      <td>0.332061</td>\n",
              "      <td>0.195143</td>\n",
              "      <td>0.422246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126145</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.283099</td>\n",
              "      <td>0.224183</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>0.155122</td>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.240018</td>\n",
              "      <td>0.438144</td>\n",
              "      <td>0.264077</td>\n",
              "      <td>0.333647</td>\n",
              "      <td>0.530074</td>\n",
              "      <td>0.343866</td>\n",
              "      <td>0.337177</td>\n",
              "      <td>0.347487</td>\n",
              "      <td>0.027798</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.372899</td>\n",
              "      <td>0.365016</td>\n",
              "      <td>0.083608</td>\n",
              "      <td>0.329544</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.390861</td>\n",
              "      <td>0.567592</td>\n",
              "      <td>0.552634</td>\n",
              "      <td>0.329531</td>\n",
              "      <td>0.225342</td>\n",
              "      <td>0.451164</td>\n",
              "      <td>0.206294</td>\n",
              "      <td>0.221538</td>\n",
              "      <td>0.253885</td>\n",
              "      <td>0.383804</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.544355</td>\n",
              "      <td>0.611316</td>\n",
              "      <td>0.746738</td>\n",
              "      <td>0.074615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contrast.1</th>\n",
              "      <td>0.458069</td>\n",
              "      <td>0.592542</td>\n",
              "      <td>0.457835</td>\n",
              "      <td>0.555373</td>\n",
              "      <td>0.380133</td>\n",
              "      <td>0.588236</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>0.639843</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.065998</td>\n",
              "      <td>0.515229</td>\n",
              "      <td>0.623406</td>\n",
              "      <td>0.576338</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.699380</td>\n",
              "      <td>0.654218</td>\n",
              "      <td>0.165224</td>\n",
              "      <td>0.399133</td>\n",
              "      <td>0.462206</td>\n",
              "      <td>0.847990</td>\n",
              "      <td>0.404501</td>\n",
              "      <td>0.807251</td>\n",
              "      <td>0.503620</td>\n",
              "      <td>0.040094</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>0.507856</td>\n",
              "      <td>0.811825</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.674231</td>\n",
              "      <td>0.184830</td>\n",
              "      <td>0.427215</td>\n",
              "      <td>0.958897</td>\n",
              "      <td>0.507007</td>\n",
              "      <td>0.941239</td>\n",
              "      <td>0.822280</td>\n",
              "      <td>0.836388</td>\n",
              "      <td>0.858476</td>\n",
              "      <td>0.916872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777488</td>\n",
              "      <td>0.592929</td>\n",
              "      <td>0.882098</td>\n",
              "      <td>0.561351</td>\n",
              "      <td>0.652744</td>\n",
              "      <td>0.457706</td>\n",
              "      <td>0.693751</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.473433</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.601669</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.679507</td>\n",
              "      <td>0.566054</td>\n",
              "      <td>0.635554</td>\n",
              "      <td>0.154862</td>\n",
              "      <td>0.617903</td>\n",
              "      <td>0.665488</td>\n",
              "      <td>0.705727</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.390378</td>\n",
              "      <td>0.398000</td>\n",
              "      <td>0.502327</td>\n",
              "      <td>0.354174</td>\n",
              "      <td>0.437122</td>\n",
              "      <td>0.390572</td>\n",
              "      <td>0.610009</td>\n",
              "      <td>0.852276</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.242269</td>\n",
              "      <td>0.640887</td>\n",
              "      <td>0.494642</td>\n",
              "      <td>0.794082</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.759051</td>\n",
              "      <td>0.551508</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.374697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Busyness</th>\n",
              "      <td>0.937894</td>\n",
              "      <td>0.810919</td>\n",
              "      <td>0.937881</td>\n",
              "      <td>0.773599</td>\n",
              "      <td>0.339451</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.682675</td>\n",
              "      <td>0.843581</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>0.913619</td>\n",
              "      <td>0.867644</td>\n",
              "      <td>0.805965</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>0.176933</td>\n",
              "      <td>0.469093</td>\n",
              "      <td>0.023465</td>\n",
              "      <td>0.890974</td>\n",
              "      <td>0.884836</td>\n",
              "      <td>0.556691</td>\n",
              "      <td>0.356571</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.253053</td>\n",
              "      <td>0.425932</td>\n",
              "      <td>0.366938</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.364459</td>\n",
              "      <td>0.187027</td>\n",
              "      <td>0.262478</td>\n",
              "      <td>0.474823</td>\n",
              "      <td>0.313668</td>\n",
              "      <td>0.411803</td>\n",
              "      <td>0.469366</td>\n",
              "      <td>0.490211</td>\n",
              "      <td>0.354087</td>\n",
              "      <td>0.516006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294360</td>\n",
              "      <td>0.307158</td>\n",
              "      <td>0.382443</td>\n",
              "      <td>0.108961</td>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>0.428333</td>\n",
              "      <td>0.320272</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.365808</td>\n",
              "      <td>0.400702</td>\n",
              "      <td>0.915079</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.448260</td>\n",
              "      <td>0.462777</td>\n",
              "      <td>0.126498</td>\n",
              "      <td>0.276150</td>\n",
              "      <td>0.466663</td>\n",
              "      <td>0.497241</td>\n",
              "      <td>0.130804</td>\n",
              "      <td>0.770513</td>\n",
              "      <td>0.135920</td>\n",
              "      <td>0.372307</td>\n",
              "      <td>0.858147</td>\n",
              "      <td>0.912460</td>\n",
              "      <td>0.770458</td>\n",
              "      <td>0.381146</td>\n",
              "      <td>0.557089</td>\n",
              "      <td>0.516059</td>\n",
              "      <td>0.592938</td>\n",
              "      <td>0.403923</td>\n",
              "      <td>0.370737</td>\n",
              "      <td>0.363137</td>\n",
              "      <td>0.403091</td>\n",
              "      <td>0.382465</td>\n",
              "      <td>0.434561</td>\n",
              "      <td>0.093528</td>\n",
              "      <td>0.602417</td>\n",
              "      <td>0.418697</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "VoxelVolume                NaN                NaN  ...         NaN       NaN\n",
              "Maximum3DDiameter     0.821982                NaN  ...         NaN       NaN\n",
              "MeshVolume            0.999999           0.821800  ...         NaN       NaN\n",
              "MajorAxisLength       0.785606           0.964760  ...         NaN       NaN\n",
              "Sphericity            0.329751           0.678628  ...         NaN       NaN\n",
              "...                        ...                ...  ...         ...       ...\n",
              "Coarseness            0.401634           0.586992  ...         NaN       NaN\n",
              "Complexity            0.277302           0.257297  ...         NaN       NaN\n",
              "Strength              0.510155           0.642559  ...         NaN       NaN\n",
              "Contrast.1            0.458069           0.592542  ...         NaN       NaN\n",
              "Busyness              0.937894           0.810919  ...    0.418697       NaN\n",
              "\n",
              "[107 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1MkWP2O9hU",
        "colab_type": "code",
        "outputId": "437e8615-21f4-446c-f7d9-80ff93397fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Find columns that meet threshold\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.70)]\n",
        "to_drop"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VoxelVolume',\n",
              " 'Maximum3DDiameter',\n",
              " 'MeshVolume',\n",
              " 'MajorAxisLength',\n",
              " 'LeastAxisLength',\n",
              " 'Elongation',\n",
              " 'SurfaceVolumeRatio',\n",
              " 'Maximum2DDiameterSlice',\n",
              " 'SurfaceArea',\n",
              " 'MinorAxisLength',\n",
              " 'Maximum2DDiameterColumn',\n",
              " 'Maximum2DDiameterRow',\n",
              " 'GrayLevelVariance',\n",
              " 'HighGrayLevelEmphasis',\n",
              " 'DependenceEntropy',\n",
              " 'DependenceNonUniformity',\n",
              " 'GrayLevelNonUniformity',\n",
              " 'SmallDependenceEmphasis',\n",
              " 'SmallDependenceHighGrayLevelEmphasis',\n",
              " 'DependenceNonUniformityNormalized',\n",
              " 'LargeDependenceEmphasis',\n",
              " 'LargeDependenceLowGrayLevelEmphasis',\n",
              " 'DependenceVariance',\n",
              " 'LargeDependenceHighGrayLevelEmphasis',\n",
              " 'SmallDependenceLowGrayLevelEmphasis',\n",
              " 'LowGrayLevelEmphasis',\n",
              " 'JointAverage',\n",
              " 'SumAverage',\n",
              " 'JointEntropy',\n",
              " 'ClusterShade',\n",
              " 'MaximumProbability',\n",
              " 'Idmn',\n",
              " 'JointEnergy',\n",
              " 'Contrast',\n",
              " 'DifferenceEntropy',\n",
              " 'InverseVariance',\n",
              " 'DifferenceVariance',\n",
              " 'Idn',\n",
              " 'Idm',\n",
              " 'Correlation',\n",
              " 'Autocorrelation',\n",
              " 'SumEntropy',\n",
              " 'SumSquares',\n",
              " 'ClusterProminence',\n",
              " 'Imc2',\n",
              " 'DifferenceAverage',\n",
              " 'Id',\n",
              " 'ClusterTendency',\n",
              " 'InterquartileRange',\n",
              " 'Skewness',\n",
              " 'Uniformity',\n",
              " 'Median',\n",
              " 'Energy',\n",
              " 'RobustMeanAbsoluteDeviation',\n",
              " 'MeanAbsoluteDeviation',\n",
              " 'Maximum',\n",
              " 'RootMeanSquared',\n",
              " 'Minimum',\n",
              " 'Entropy',\n",
              " 'Range',\n",
              " 'Variance',\n",
              " '10Percentile',\n",
              " 'Kurtosis',\n",
              " 'Mean',\n",
              " 'ShortRunLowGrayLevelEmphasis',\n",
              " 'GrayLevelVariance.1',\n",
              " 'LowGrayLevelRunEmphasis',\n",
              " 'GrayLevelNonUniformityNormalized',\n",
              " 'RunVariance',\n",
              " 'GrayLevelNonUniformity.1',\n",
              " 'LongRunEmphasis',\n",
              " 'ShortRunHighGrayLevelEmphasis',\n",
              " 'RunLengthNonUniformity',\n",
              " 'ShortRunEmphasis',\n",
              " 'LongRunHighGrayLevelEmphasis',\n",
              " 'RunPercentage',\n",
              " 'LongRunLowGrayLevelEmphasis',\n",
              " 'RunEntropy',\n",
              " 'HighGrayLevelRunEmphasis',\n",
              " 'RunLengthNonUniformityNormalized',\n",
              " 'ZoneVariance',\n",
              " 'SizeZoneNonUniformityNormalized',\n",
              " 'SizeZoneNonUniformity',\n",
              " 'GrayLevelNonUniformity.2',\n",
              " 'LargeAreaEmphasis',\n",
              " 'SmallAreaHighGrayLevelEmphasis',\n",
              " 'ZonePercentage',\n",
              " 'LowGrayLevelZoneEmphasis',\n",
              " 'SmallAreaLowGrayLevelEmphasis',\n",
              " 'Coarseness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKeereQoci",
        "colab_type": "text"
      },
      "source": [
        "The reason we used the mask to set half of the matrix to NA value is that we ewnt to avoid removing both features when thay have a strong correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R-tG-WPNXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "train_data_stand_reduced = train_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNUiUoZ0Vdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "test_data_stand_reduced = test_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-uuBifRO8G",
        "colab_type": "code",
        "outputId": "8244ca0a-8c64-4c58-cbf0-40609b8d190c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wuGcLV0acQ",
        "colab_type": "code",
        "outputId": "ace6ff64-29f0-47cd-d4b2-6bce4bea29ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data_stand_reduced.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1qR3re7QwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_reduced = train_data_stand_reduced.to_numpy()\n",
        "test_data_stand_reduced = test_data_stand_reduced.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUG3a0o5W1O",
        "colab_type": "code",
        "outputId": "548f83b8-d441-4ac1-f8fe-3cbf572b7e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8wkFES71GJ",
        "colab_type": "code",
        "outputId": "6368a651-7bf3-48f6-f888-6cf3d8aaa9e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljMaQeuSHe1",
        "colab_type": "text"
      },
      "source": [
        "funziona bene, però bisogna stare attenti a basarsi unicamente sul coefficiente di correlazione. Se y = x^2, x e y risulteranno scorrelate secondo il coeffiente di correlazione di Pearson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxuH4u741mLR",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(17,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmeGHBV1xs_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5hBUph6149E"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7NwNpEO149J"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MD1VB0S149O",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "830049c2-ea5e-4c09-abf4-a8d657d2e1bf",
        "id": "VkpRnrey149Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_reduced, train_labels_dec)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4f67a0f1-9785-4790-94ed-67082a6f2237",
        "id": "F3zK4E6o149g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b9be9e00-4e3f-4407-d312-6e74560804a0",
        "id": "BDX_0MCd149o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9QlfChU149v",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvCKZfjj1490",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bjrjhWo1497"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Du7DFfm1498",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qsz_jnVasri",
        "colab_type": "code",
        "outputId": "7d5d93d0-5463-4ad1-ce55-9ce4a26226ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "275eca15-8d04-420d-dc44-20c20e3d46cb",
        "id": "kHNpJxBL14-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "all_acc_histories_reduced = []\n",
        "all_loss_histories_reduced = []\n",
        "all_val_acc_histories_reduced = []\n",
        "all_val_loss_histories_reduced = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_reduced[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_reduced[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history_reduced = history.history['acc']\n",
        "  all_acc_histories_reduced.append(acc_history)\n",
        "\n",
        "  loss_history_reduced = history.history['loss']\n",
        "  all_loss_histories_reduced.append(loss_history)\n",
        "\n",
        "  acc_val_history_reduced = history.history['val_acc']\n",
        "  all_val_acc_histories_reduced.append(acc_val_history)\n",
        "\n",
        "  loss_val_history_reduced = history.history['val_loss']\n",
        "  all_val_loss_histories_reduced.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/500\n",
            "87/87 [==============================] - 0s 4ms/step - loss: 1.2339 - acc: 0.3678 - val_loss: 1.1038 - val_acc: 0.4545\n",
            "Epoch 2/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 1.1343 - acc: 0.3908 - val_loss: 1.0760 - val_acc: 0.4545\n",
            "Epoch 3/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 1.0648 - acc: 0.4598 - val_loss: 1.0616 - val_acc: 0.4545\n",
            "Epoch 4/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.0161 - acc: 0.5057 - val_loss: 1.0535 - val_acc: 0.4318\n",
            "Epoch 5/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.9779 - acc: 0.5172 - val_loss: 1.0485 - val_acc: 0.4545\n",
            "Epoch 6/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.9424 - acc: 0.5287 - val_loss: 1.0454 - val_acc: 0.5000\n",
            "Epoch 7/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9184 - acc: 0.5517 - val_loss: 1.0442 - val_acc: 0.5227\n",
            "Epoch 8/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.9002 - acc: 0.5747 - val_loss: 1.0448 - val_acc: 0.4773\n",
            "Epoch 9/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8829 - acc: 0.5862 - val_loss: 1.0447 - val_acc: 0.5000\n",
            "Epoch 10/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8692 - acc: 0.5977 - val_loss: 1.0458 - val_acc: 0.5000\n",
            "Epoch 11/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8581 - acc: 0.5977 - val_loss: 1.0472 - val_acc: 0.5227\n",
            "Epoch 12/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8420 - acc: 0.6207 - val_loss: 1.0498 - val_acc: 0.5000\n",
            "Epoch 13/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.8267 - acc: 0.6207 - val_loss: 1.0522 - val_acc: 0.4773\n",
            "Epoch 14/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8193 - acc: 0.6092 - val_loss: 1.0558 - val_acc: 0.4545\n",
            "Epoch 15/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8074 - acc: 0.6207 - val_loss: 1.0581 - val_acc: 0.4773\n",
            "Epoch 16/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8005 - acc: 0.6207 - val_loss: 1.0603 - val_acc: 0.4545\n",
            "Epoch 17/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.7917 - acc: 0.6322 - val_loss: 1.0636 - val_acc: 0.4773\n",
            "Epoch 18/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7867 - acc: 0.6437 - val_loss: 1.0670 - val_acc: 0.4773\n",
            "Epoch 19/500\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.7820 - acc: 0.6207 - val_loss: 1.0696 - val_acc: 0.4773\n",
            "Epoch 20/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.7721 - acc: 0.6437 - val_loss: 1.0709 - val_acc: 0.4773\n",
            "Epoch 21/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.7671 - acc: 0.6437 - val_loss: 1.0745 - val_acc: 0.5000\n",
            "Epoch 22/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7614 - acc: 0.6552 - val_loss: 1.0767 - val_acc: 0.5227\n",
            "Epoch 23/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.7596 - acc: 0.6322 - val_loss: 1.0810 - val_acc: 0.5000\n",
            "Epoch 24/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7507 - acc: 0.6322 - val_loss: 1.0836 - val_acc: 0.5000\n",
            "Epoch 25/500\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.7467 - acc: 0.6437 - val_loss: 1.0864 - val_acc: 0.5000\n",
            "Epoch 26/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.7426 - acc: 0.6552 - val_loss: 1.0895 - val_acc: 0.5000\n",
            "Epoch 27/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7392 - acc: 0.6667 - val_loss: 1.0929 - val_acc: 0.5000\n",
            "Epoch 28/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.7331 - acc: 0.6782 - val_loss: 1.0965 - val_acc: 0.5227\n",
            "Epoch 29/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.7295 - acc: 0.6782 - val_loss: 1.1003 - val_acc: 0.5000\n",
            "Epoch 30/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.7258 - acc: 0.6782 - val_loss: 1.1038 - val_acc: 0.5227\n",
            "Epoch 31/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.7236 - acc: 0.6782 - val_loss: 1.1065 - val_acc: 0.5227\n",
            "Epoch 32/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.7185 - acc: 0.6782 - val_loss: 1.1106 - val_acc: 0.5227\n",
            "Epoch 33/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7154 - acc: 0.7011 - val_loss: 1.1152 - val_acc: 0.5000\n",
            "Epoch 34/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.7108 - acc: 0.7011 - val_loss: 1.1183 - val_acc: 0.5000\n",
            "Epoch 35/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7081 - acc: 0.6897 - val_loss: 1.1217 - val_acc: 0.5000\n",
            "Epoch 36/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.7058 - acc: 0.7126 - val_loss: 1.1256 - val_acc: 0.5000\n",
            "Epoch 37/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7047 - acc: 0.6667 - val_loss: 1.1292 - val_acc: 0.5000\n",
            "Epoch 38/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.6982 - acc: 0.7011 - val_loss: 1.1326 - val_acc: 0.5000\n",
            "Epoch 39/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6968 - acc: 0.6897 - val_loss: 1.1354 - val_acc: 0.5000\n",
            "Epoch 40/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6913 - acc: 0.7011 - val_loss: 1.1375 - val_acc: 0.5000\n",
            "Epoch 41/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.6912 - acc: 0.7126 - val_loss: 1.1398 - val_acc: 0.5000\n",
            "Epoch 42/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.6849 - acc: 0.7241 - val_loss: 1.1421 - val_acc: 0.5000\n",
            "Epoch 43/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.6816 - acc: 0.7241 - val_loss: 1.1479 - val_acc: 0.5000\n",
            "Epoch 44/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.6789 - acc: 0.7126 - val_loss: 1.1515 - val_acc: 0.5000\n",
            "Epoch 45/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6802 - acc: 0.7126 - val_loss: 1.1539 - val_acc: 0.5000\n",
            "Epoch 46/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.6748 - acc: 0.7241 - val_loss: 1.1564 - val_acc: 0.5000\n",
            "Epoch 47/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6746 - acc: 0.7126 - val_loss: 1.1616 - val_acc: 0.5000\n",
            "Epoch 48/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.6699 - acc: 0.7126 - val_loss: 1.1664 - val_acc: 0.5000\n",
            "Epoch 49/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6669 - acc: 0.7356 - val_loss: 1.1700 - val_acc: 0.5000\n",
            "Epoch 50/500\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.6650 - acc: 0.7126 - val_loss: 1.1744 - val_acc: 0.5000\n",
            "Epoch 51/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.6631 - acc: 0.7241 - val_loss: 1.1775 - val_acc: 0.5000\n",
            "Epoch 52/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6596 - acc: 0.7356 - val_loss: 1.1841 - val_acc: 0.5000\n",
            "Epoch 53/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.6591 - acc: 0.7356 - val_loss: 1.1857 - val_acc: 0.5000\n",
            "Epoch 54/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6574 - acc: 0.7241 - val_loss: 1.1889 - val_acc: 0.5000\n",
            "Epoch 55/500\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.6544 - acc: 0.7241 - val_loss: 1.1908 - val_acc: 0.5000\n",
            "Epoch 56/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.6505 - acc: 0.7356 - val_loss: 1.1947 - val_acc: 0.5000\n",
            "Epoch 57/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.6509 - acc: 0.7356 - val_loss: 1.1956 - val_acc: 0.5000\n",
            "Epoch 58/500\n",
            "87/87 [==============================] - 0s 190us/step - loss: 0.6450 - acc: 0.7471 - val_loss: 1.2007 - val_acc: 0.5000\n",
            "Epoch 59/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.6446 - acc: 0.7356 - val_loss: 1.2034 - val_acc: 0.5000\n",
            "Epoch 60/500\n",
            "87/87 [==============================] - 0s 286us/step - loss: 0.6419 - acc: 0.7241 - val_loss: 1.2093 - val_acc: 0.5000\n",
            "Epoch 61/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6390 - acc: 0.7471 - val_loss: 1.2128 - val_acc: 0.5000\n",
            "Epoch 62/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.6373 - acc: 0.7356 - val_loss: 1.2184 - val_acc: 0.5000\n",
            "Epoch 63/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6345 - acc: 0.7356 - val_loss: 1.2203 - val_acc: 0.5000\n",
            "Epoch 64/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6329 - acc: 0.7471 - val_loss: 1.2220 - val_acc: 0.5000\n",
            "Epoch 65/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6314 - acc: 0.7471 - val_loss: 1.2254 - val_acc: 0.5000\n",
            "Epoch 66/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.6286 - acc: 0.7356 - val_loss: 1.2300 - val_acc: 0.5000\n",
            "Epoch 67/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.6233 - acc: 0.7356 - val_loss: 1.2292 - val_acc: 0.5000\n",
            "Epoch 68/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.6263 - acc: 0.7586 - val_loss: 1.2354 - val_acc: 0.5000\n",
            "Epoch 69/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.6230 - acc: 0.7471 - val_loss: 1.2379 - val_acc: 0.5000\n",
            "Epoch 70/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6221 - acc: 0.7356 - val_loss: 1.2414 - val_acc: 0.5000\n",
            "Epoch 71/500\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.6144 - acc: 0.7586 - val_loss: 1.2525 - val_acc: 0.5000\n",
            "Epoch 72/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6121 - acc: 0.7471 - val_loss: 1.2529 - val_acc: 0.5000\n",
            "Epoch 73/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.6122 - acc: 0.7701 - val_loss: 1.2578 - val_acc: 0.5000\n",
            "Epoch 74/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.6113 - acc: 0.7701 - val_loss: 1.2639 - val_acc: 0.5000\n",
            "Epoch 75/500\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.6094 - acc: 0.7701 - val_loss: 1.2699 - val_acc: 0.5227\n",
            "Epoch 76/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6055 - acc: 0.7586 - val_loss: 1.2729 - val_acc: 0.5227\n",
            "Epoch 77/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.6046 - acc: 0.7701 - val_loss: 1.2701 - val_acc: 0.5227\n",
            "Epoch 78/500\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.6008 - acc: 0.7701 - val_loss: 1.2778 - val_acc: 0.5227\n",
            "Epoch 79/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5977 - acc: 0.7701 - val_loss: 1.2867 - val_acc: 0.5227\n",
            "Epoch 80/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.5986 - acc: 0.7816 - val_loss: 1.2913 - val_acc: 0.5227\n",
            "Epoch 81/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5944 - acc: 0.7701 - val_loss: 1.2960 - val_acc: 0.5227\n",
            "Epoch 82/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.5909 - acc: 0.7701 - val_loss: 1.2967 - val_acc: 0.5227\n",
            "Epoch 83/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5911 - acc: 0.7816 - val_loss: 1.3007 - val_acc: 0.5227\n",
            "Epoch 84/500\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.5877 - acc: 0.7701 - val_loss: 1.3038 - val_acc: 0.5227\n",
            "Epoch 85/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5848 - acc: 0.7816 - val_loss: 1.3126 - val_acc: 0.5227\n",
            "Epoch 86/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.5828 - acc: 0.8161 - val_loss: 1.3141 - val_acc: 0.5227\n",
            "Epoch 87/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5791 - acc: 0.8161 - val_loss: 1.3213 - val_acc: 0.5227\n",
            "Epoch 88/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5795 - acc: 0.8046 - val_loss: 1.3293 - val_acc: 0.5227\n",
            "Epoch 89/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.5773 - acc: 0.8276 - val_loss: 1.3292 - val_acc: 0.5227\n",
            "Epoch 90/500\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.5745 - acc: 0.8161 - val_loss: 1.3348 - val_acc: 0.5227\n",
            "Epoch 91/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5738 - acc: 0.8276 - val_loss: 1.3399 - val_acc: 0.5000\n",
            "Epoch 92/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5709 - acc: 0.8161 - val_loss: 1.3417 - val_acc: 0.5227\n",
            "Epoch 93/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5672 - acc: 0.8276 - val_loss: 1.3481 - val_acc: 0.5227\n",
            "Epoch 94/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5663 - acc: 0.8276 - val_loss: 1.3546 - val_acc: 0.5227\n",
            "Epoch 95/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.5646 - acc: 0.8161 - val_loss: 1.3609 - val_acc: 0.5000\n",
            "Epoch 96/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5631 - acc: 0.8276 - val_loss: 1.3663 - val_acc: 0.5227\n",
            "Epoch 97/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5627 - acc: 0.8161 - val_loss: 1.3706 - val_acc: 0.5227\n",
            "Epoch 98/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5577 - acc: 0.8276 - val_loss: 1.3754 - val_acc: 0.5227\n",
            "Epoch 99/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5564 - acc: 0.8276 - val_loss: 1.3831 - val_acc: 0.5227\n",
            "Epoch 100/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5556 - acc: 0.8161 - val_loss: 1.3841 - val_acc: 0.5227\n",
            "Epoch 101/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.5522 - acc: 0.8391 - val_loss: 1.3868 - val_acc: 0.5227\n",
            "Epoch 102/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5479 - acc: 0.8391 - val_loss: 1.3940 - val_acc: 0.5227\n",
            "Epoch 103/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.5479 - acc: 0.8276 - val_loss: 1.4001 - val_acc: 0.5227\n",
            "Epoch 104/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.5460 - acc: 0.8391 - val_loss: 1.4063 - val_acc: 0.5227\n",
            "Epoch 105/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5445 - acc: 0.8391 - val_loss: 1.4108 - val_acc: 0.5227\n",
            "Epoch 106/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5423 - acc: 0.8276 - val_loss: 1.4129 - val_acc: 0.5227\n",
            "Epoch 107/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.5380 - acc: 0.8276 - val_loss: 1.4156 - val_acc: 0.5455\n",
            "Epoch 108/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.5366 - acc: 0.8621 - val_loss: 1.4237 - val_acc: 0.5227\n",
            "Epoch 109/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.5347 - acc: 0.8621 - val_loss: 1.4299 - val_acc: 0.5227\n",
            "Epoch 110/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5319 - acc: 0.8621 - val_loss: 1.4368 - val_acc: 0.5227\n",
            "Epoch 111/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5337 - acc: 0.8506 - val_loss: 1.4405 - val_acc: 0.5227\n",
            "Epoch 112/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5314 - acc: 0.8391 - val_loss: 1.4442 - val_acc: 0.5455\n",
            "Epoch 113/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5289 - acc: 0.8506 - val_loss: 1.4480 - val_acc: 0.5455\n",
            "Epoch 114/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.5263 - acc: 0.8621 - val_loss: 1.4495 - val_acc: 0.5682\n",
            "Epoch 115/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5220 - acc: 0.8621 - val_loss: 1.4539 - val_acc: 0.5682\n",
            "Epoch 116/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5204 - acc: 0.8506 - val_loss: 1.4619 - val_acc: 0.5455\n",
            "Epoch 117/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5165 - acc: 0.8621 - val_loss: 1.4669 - val_acc: 0.5227\n",
            "Epoch 118/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5135 - acc: 0.8621 - val_loss: 1.4710 - val_acc: 0.5455\n",
            "Epoch 119/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.5135 - acc: 0.8621 - val_loss: 1.4734 - val_acc: 0.5682\n",
            "Epoch 120/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.5092 - acc: 0.8506 - val_loss: 1.4762 - val_acc: 0.5455\n",
            "Epoch 121/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.5082 - acc: 0.8506 - val_loss: 1.4782 - val_acc: 0.5455\n",
            "Epoch 122/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5050 - acc: 0.8621 - val_loss: 1.4822 - val_acc: 0.5227\n",
            "Epoch 123/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.5042 - acc: 0.8621 - val_loss: 1.4897 - val_acc: 0.5227\n",
            "Epoch 124/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5006 - acc: 0.8621 - val_loss: 1.4889 - val_acc: 0.5000\n",
            "Epoch 125/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4997 - acc: 0.8506 - val_loss: 1.4995 - val_acc: 0.4773\n",
            "Epoch 126/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4969 - acc: 0.8621 - val_loss: 1.5003 - val_acc: 0.5227\n",
            "Epoch 127/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4949 - acc: 0.8621 - val_loss: 1.5080 - val_acc: 0.4773\n",
            "Epoch 128/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4958 - acc: 0.8506 - val_loss: 1.5064 - val_acc: 0.5000\n",
            "Epoch 129/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.4891 - acc: 0.8621 - val_loss: 1.5132 - val_acc: 0.4773\n",
            "Epoch 130/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4888 - acc: 0.8621 - val_loss: 1.5143 - val_acc: 0.5227\n",
            "Epoch 131/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4849 - acc: 0.8736 - val_loss: 1.5196 - val_acc: 0.5000\n",
            "Epoch 132/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4846 - acc: 0.8621 - val_loss: 1.5230 - val_acc: 0.5227\n",
            "Epoch 133/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.4808 - acc: 0.8736 - val_loss: 1.5315 - val_acc: 0.5000\n",
            "Epoch 134/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.4812 - acc: 0.8621 - val_loss: 1.5383 - val_acc: 0.5000\n",
            "Epoch 135/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4736 - acc: 0.8736 - val_loss: 1.5440 - val_acc: 0.5000\n",
            "Epoch 136/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4766 - acc: 0.8736 - val_loss: 1.5489 - val_acc: 0.5000\n",
            "Epoch 137/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4751 - acc: 0.8736 - val_loss: 1.5540 - val_acc: 0.5000\n",
            "Epoch 138/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.4725 - acc: 0.8736 - val_loss: 1.5524 - val_acc: 0.5000\n",
            "Epoch 139/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4708 - acc: 0.8736 - val_loss: 1.5599 - val_acc: 0.5000\n",
            "Epoch 140/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4681 - acc: 0.8851 - val_loss: 1.5631 - val_acc: 0.5000\n",
            "Epoch 141/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.4660 - acc: 0.8851 - val_loss: 1.5672 - val_acc: 0.5000\n",
            "Epoch 142/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4640 - acc: 0.8736 - val_loss: 1.5716 - val_acc: 0.5000\n",
            "Epoch 143/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.4647 - acc: 0.8736 - val_loss: 1.5770 - val_acc: 0.5000\n",
            "Epoch 144/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4628 - acc: 0.8851 - val_loss: 1.5850 - val_acc: 0.5000\n",
            "Epoch 145/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.4605 - acc: 0.8851 - val_loss: 1.5890 - val_acc: 0.5000\n",
            "Epoch 146/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.4588 - acc: 0.8736 - val_loss: 1.5888 - val_acc: 0.5000\n",
            "Epoch 147/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4565 - acc: 0.8736 - val_loss: 1.5950 - val_acc: 0.5000\n",
            "Epoch 148/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4532 - acc: 0.8851 - val_loss: 1.6048 - val_acc: 0.5227\n",
            "Epoch 149/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4558 - acc: 0.8736 - val_loss: 1.5990 - val_acc: 0.5000\n",
            "Epoch 150/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4513 - acc: 0.8736 - val_loss: 1.6101 - val_acc: 0.5000\n",
            "Epoch 151/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4514 - acc: 0.8736 - val_loss: 1.6123 - val_acc: 0.5000\n",
            "Epoch 152/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4477 - acc: 0.8736 - val_loss: 1.6230 - val_acc: 0.5000\n",
            "Epoch 153/500\n",
            "87/87 [==============================] - 0s 301us/step - loss: 0.4452 - acc: 0.8851 - val_loss: 1.6236 - val_acc: 0.5000\n",
            "Epoch 154/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.4443 - acc: 0.8851 - val_loss: 1.6250 - val_acc: 0.5227\n",
            "Epoch 155/500\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.4416 - acc: 0.8736 - val_loss: 1.6342 - val_acc: 0.5455\n",
            "Epoch 156/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4382 - acc: 0.8851 - val_loss: 1.6431 - val_acc: 0.5227\n",
            "Epoch 157/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4380 - acc: 0.8736 - val_loss: 1.6480 - val_acc: 0.5227\n",
            "Epoch 158/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.4354 - acc: 0.8851 - val_loss: 1.6541 - val_acc: 0.5227\n",
            "Epoch 159/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.4320 - acc: 0.8966 - val_loss: 1.6588 - val_acc: 0.5455\n",
            "Epoch 160/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.4310 - acc: 0.8966 - val_loss: 1.6652 - val_acc: 0.5227\n",
            "Epoch 161/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.4303 - acc: 0.8851 - val_loss: 1.6693 - val_acc: 0.5455\n",
            "Epoch 162/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4331 - acc: 0.8966 - val_loss: 1.6631 - val_acc: 0.5455\n",
            "Epoch 163/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.4290 - acc: 0.8851 - val_loss: 1.6730 - val_acc: 0.5455\n",
            "Epoch 164/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4265 - acc: 0.8966 - val_loss: 1.6827 - val_acc: 0.5455\n",
            "Epoch 165/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4262 - acc: 0.8966 - val_loss: 1.6884 - val_acc: 0.5227\n",
            "Epoch 166/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.4223 - acc: 0.8966 - val_loss: 1.6970 - val_acc: 0.5682\n",
            "Epoch 167/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.4200 - acc: 0.8966 - val_loss: 1.7063 - val_acc: 0.5455\n",
            "Epoch 168/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4212 - acc: 0.8966 - val_loss: 1.7019 - val_acc: 0.5455\n",
            "Epoch 169/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4167 - acc: 0.8851 - val_loss: 1.7083 - val_acc: 0.5455\n",
            "Epoch 170/500\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.4158 - acc: 0.8966 - val_loss: 1.7177 - val_acc: 0.5455\n",
            "Epoch 171/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4176 - acc: 0.8851 - val_loss: 1.7177 - val_acc: 0.5455\n",
            "Epoch 172/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.4156 - acc: 0.8966 - val_loss: 1.7297 - val_acc: 0.5682\n",
            "Epoch 173/500\n",
            "87/87 [==============================] - 0s 193us/step - loss: 0.4153 - acc: 0.8736 - val_loss: 1.7319 - val_acc: 0.5682\n",
            "Epoch 174/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4109 - acc: 0.8736 - val_loss: 1.7322 - val_acc: 0.5682\n",
            "Epoch 175/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4075 - acc: 0.8966 - val_loss: 1.7410 - val_acc: 0.5455\n",
            "Epoch 176/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4087 - acc: 0.8851 - val_loss: 1.7387 - val_acc: 0.5455\n",
            "Epoch 177/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4089 - acc: 0.8966 - val_loss: 1.7442 - val_acc: 0.5455\n",
            "Epoch 178/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4062 - acc: 0.8851 - val_loss: 1.7579 - val_acc: 0.5682\n",
            "Epoch 179/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4044 - acc: 0.8851 - val_loss: 1.7599 - val_acc: 0.5455\n",
            "Epoch 180/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3990 - acc: 0.8851 - val_loss: 1.7742 - val_acc: 0.5682\n",
            "Epoch 181/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4012 - acc: 0.8851 - val_loss: 1.7783 - val_acc: 0.5682\n",
            "Epoch 182/500\n",
            "87/87 [==============================] - 0s 189us/step - loss: 0.3976 - acc: 0.8966 - val_loss: 1.7780 - val_acc: 0.5682\n",
            "Epoch 183/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3952 - acc: 0.8851 - val_loss: 1.7903 - val_acc: 0.5682\n",
            "Epoch 184/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.3959 - acc: 0.8851 - val_loss: 1.7884 - val_acc: 0.5682\n",
            "Epoch 185/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3959 - acc: 0.8851 - val_loss: 1.7933 - val_acc: 0.5682\n",
            "Epoch 186/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3891 - acc: 0.8966 - val_loss: 1.8024 - val_acc: 0.5682\n",
            "Epoch 187/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3921 - acc: 0.8966 - val_loss: 1.8052 - val_acc: 0.5682\n",
            "Epoch 188/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.3886 - acc: 0.8851 - val_loss: 1.8090 - val_acc: 0.5682\n",
            "Epoch 189/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3880 - acc: 0.8851 - val_loss: 1.8158 - val_acc: 0.5682\n",
            "Epoch 190/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3876 - acc: 0.8851 - val_loss: 1.8211 - val_acc: 0.5682\n",
            "Epoch 191/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3840 - acc: 0.8851 - val_loss: 1.8341 - val_acc: 0.5682\n",
            "Epoch 192/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3815 - acc: 0.8851 - val_loss: 1.8433 - val_acc: 0.5682\n",
            "Epoch 193/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.3823 - acc: 0.8966 - val_loss: 1.8447 - val_acc: 0.5682\n",
            "Epoch 194/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3817 - acc: 0.8851 - val_loss: 1.8409 - val_acc: 0.5682\n",
            "Epoch 195/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3792 - acc: 0.8851 - val_loss: 1.8563 - val_acc: 0.5682\n",
            "Epoch 196/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3771 - acc: 0.8851 - val_loss: 1.8657 - val_acc: 0.5682\n",
            "Epoch 197/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.3783 - acc: 0.8851 - val_loss: 1.8602 - val_acc: 0.5682\n",
            "Epoch 198/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.3767 - acc: 0.8851 - val_loss: 1.8646 - val_acc: 0.5682\n",
            "Epoch 199/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3746 - acc: 0.8851 - val_loss: 1.8704 - val_acc: 0.5682\n",
            "Epoch 200/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.3729 - acc: 0.8851 - val_loss: 1.8774 - val_acc: 0.5682\n",
            "Epoch 201/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.3714 - acc: 0.8851 - val_loss: 1.8836 - val_acc: 0.5682\n",
            "Epoch 202/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3727 - acc: 0.8851 - val_loss: 1.8970 - val_acc: 0.5682\n",
            "Epoch 203/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.3695 - acc: 0.8966 - val_loss: 1.8992 - val_acc: 0.5682\n",
            "Epoch 204/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3692 - acc: 0.8851 - val_loss: 1.9054 - val_acc: 0.5682\n",
            "Epoch 205/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3680 - acc: 0.8851 - val_loss: 1.9123 - val_acc: 0.5682\n",
            "Epoch 206/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.3637 - acc: 0.8851 - val_loss: 1.9190 - val_acc: 0.5682\n",
            "Epoch 207/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.3608 - acc: 0.8851 - val_loss: 1.9220 - val_acc: 0.5682\n",
            "Epoch 208/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.3620 - acc: 0.8851 - val_loss: 1.9279 - val_acc: 0.5682\n",
            "Epoch 209/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3606 - acc: 0.8966 - val_loss: 1.9319 - val_acc: 0.5682\n",
            "Epoch 210/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3597 - acc: 0.8966 - val_loss: 1.9369 - val_acc: 0.5682\n",
            "Epoch 211/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.3591 - acc: 0.8966 - val_loss: 1.9410 - val_acc: 0.5682\n",
            "Epoch 212/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3582 - acc: 0.9080 - val_loss: 1.9474 - val_acc: 0.5682\n",
            "Epoch 213/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3528 - acc: 0.9080 - val_loss: 1.9511 - val_acc: 0.5682\n",
            "Epoch 214/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3542 - acc: 0.8966 - val_loss: 1.9565 - val_acc: 0.5682\n",
            "Epoch 215/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.3506 - acc: 0.8966 - val_loss: 1.9629 - val_acc: 0.5455\n",
            "Epoch 216/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3499 - acc: 0.8966 - val_loss: 1.9640 - val_acc: 0.5682\n",
            "Epoch 217/500\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.3517 - acc: 0.8966 - val_loss: 1.9694 - val_acc: 0.5682\n",
            "Epoch 218/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.3495 - acc: 0.9080 - val_loss: 1.9744 - val_acc: 0.5682\n",
            "Epoch 219/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.3500 - acc: 0.8966 - val_loss: 1.9774 - val_acc: 0.5682\n",
            "Epoch 220/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.3479 - acc: 0.8851 - val_loss: 1.9840 - val_acc: 0.5682\n",
            "Epoch 221/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.3419 - acc: 0.8966 - val_loss: 1.9888 - val_acc: 0.5682\n",
            "Epoch 222/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3433 - acc: 0.9080 - val_loss: 1.9920 - val_acc: 0.5682\n",
            "Epoch 223/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.3415 - acc: 0.9080 - val_loss: 1.9958 - val_acc: 0.5682\n",
            "Epoch 224/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.3415 - acc: 0.8966 - val_loss: 1.9998 - val_acc: 0.5682\n",
            "Epoch 225/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3387 - acc: 0.8966 - val_loss: 2.0054 - val_acc: 0.5682\n",
            "Epoch 226/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3402 - acc: 0.8966 - val_loss: 2.0103 - val_acc: 0.5682\n",
            "Epoch 227/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.3372 - acc: 0.8966 - val_loss: 2.0161 - val_acc: 0.5682\n",
            "Epoch 228/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.3373 - acc: 0.9080 - val_loss: 2.0166 - val_acc: 0.5682\n",
            "Epoch 229/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3351 - acc: 0.8966 - val_loss: 2.0193 - val_acc: 0.5682\n",
            "Epoch 230/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3309 - acc: 0.9080 - val_loss: 2.0274 - val_acc: 0.5682\n",
            "Epoch 231/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3328 - acc: 0.9080 - val_loss: 2.0285 - val_acc: 0.5909\n",
            "Epoch 232/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3336 - acc: 0.9080 - val_loss: 2.0368 - val_acc: 0.5682\n",
            "Epoch 233/500\n",
            "87/87 [==============================] - 0s 190us/step - loss: 0.3291 - acc: 0.9080 - val_loss: 2.0410 - val_acc: 0.5682\n",
            "Epoch 234/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3299 - acc: 0.9195 - val_loss: 2.0462 - val_acc: 0.5682\n",
            "Epoch 235/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.3249 - acc: 0.8966 - val_loss: 2.0493 - val_acc: 0.5909\n",
            "Epoch 236/500\n",
            "87/87 [==============================] - 0s 189us/step - loss: 0.3252 - acc: 0.8966 - val_loss: 2.0540 - val_acc: 0.5682\n",
            "Epoch 237/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3261 - acc: 0.8966 - val_loss: 2.0596 - val_acc: 0.5682\n",
            "Epoch 238/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3232 - acc: 0.9195 - val_loss: 2.0634 - val_acc: 0.5682\n",
            "Epoch 239/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.3233 - acc: 0.8966 - val_loss: 2.0691 - val_acc: 0.5682\n",
            "Epoch 240/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3191 - acc: 0.9080 - val_loss: 2.0756 - val_acc: 0.5909\n",
            "Epoch 241/500\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.3201 - acc: 0.9080 - val_loss: 2.0769 - val_acc: 0.5682\n",
            "Epoch 242/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.3167 - acc: 0.9080 - val_loss: 2.0840 - val_acc: 0.5909\n",
            "Epoch 243/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.3159 - acc: 0.9195 - val_loss: 2.0866 - val_acc: 0.5682\n",
            "Epoch 244/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3150 - acc: 0.8966 - val_loss: 2.0909 - val_acc: 0.5909\n",
            "Epoch 245/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.3142 - acc: 0.8966 - val_loss: 2.0945 - val_acc: 0.5909\n",
            "Epoch 246/500\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.3124 - acc: 0.9195 - val_loss: 2.1031 - val_acc: 0.5682\n",
            "Epoch 247/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3094 - acc: 0.8966 - val_loss: 2.1067 - val_acc: 0.5909\n",
            "Epoch 248/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.3153 - acc: 0.9080 - val_loss: 2.1070 - val_acc: 0.5682\n",
            "Epoch 249/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3097 - acc: 0.9080 - val_loss: 2.1101 - val_acc: 0.5682\n",
            "Epoch 250/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3088 - acc: 0.9195 - val_loss: 2.1198 - val_acc: 0.5682\n",
            "Epoch 251/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.3112 - acc: 0.9080 - val_loss: 2.1195 - val_acc: 0.5909\n",
            "Epoch 252/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.3081 - acc: 0.9080 - val_loss: 2.1258 - val_acc: 0.5682\n",
            "Epoch 253/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3019 - acc: 0.9195 - val_loss: 2.1309 - val_acc: 0.5909\n",
            "Epoch 254/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3034 - acc: 0.9080 - val_loss: 2.1372 - val_acc: 0.6136\n",
            "Epoch 255/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.3051 - acc: 0.8966 - val_loss: 2.1383 - val_acc: 0.5909\n",
            "Epoch 256/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.3023 - acc: 0.9195 - val_loss: 2.1425 - val_acc: 0.5682\n",
            "Epoch 257/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3040 - acc: 0.9080 - val_loss: 2.1470 - val_acc: 0.5682\n",
            "Epoch 258/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.3018 - acc: 0.9080 - val_loss: 2.1524 - val_acc: 0.5682\n",
            "Epoch 259/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.3003 - acc: 0.9080 - val_loss: 2.1563 - val_acc: 0.5682\n",
            "Epoch 260/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.2956 - acc: 0.9080 - val_loss: 2.1600 - val_acc: 0.5909\n",
            "Epoch 261/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.2972 - acc: 0.9080 - val_loss: 2.1639 - val_acc: 0.5909\n",
            "Epoch 262/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2971 - acc: 0.9080 - val_loss: 2.1706 - val_acc: 0.5682\n",
            "Epoch 263/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.2976 - acc: 0.9080 - val_loss: 2.1766 - val_acc: 0.5682\n",
            "Epoch 264/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.2954 - acc: 0.9080 - val_loss: 2.1786 - val_acc: 0.5909\n",
            "Epoch 265/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2911 - acc: 0.9080 - val_loss: 2.1847 - val_acc: 0.6136\n",
            "Epoch 266/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.2931 - acc: 0.9080 - val_loss: 2.1855 - val_acc: 0.5682\n",
            "Epoch 267/500\n",
            "87/87 [==============================] - 0s 194us/step - loss: 0.2877 - acc: 0.9195 - val_loss: 2.1877 - val_acc: 0.5682\n",
            "Epoch 268/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.2877 - acc: 0.9080 - val_loss: 2.1948 - val_acc: 0.5909\n",
            "Epoch 269/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2895 - acc: 0.8966 - val_loss: 2.1995 - val_acc: 0.6136\n",
            "Epoch 270/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2895 - acc: 0.9080 - val_loss: 2.2038 - val_acc: 0.5682\n",
            "Epoch 271/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2864 - acc: 0.8966 - val_loss: 2.2072 - val_acc: 0.5909\n",
            "Epoch 272/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.2850 - acc: 0.9195 - val_loss: 2.2134 - val_acc: 0.5909\n",
            "Epoch 273/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2838 - acc: 0.8966 - val_loss: 2.2171 - val_acc: 0.5682\n",
            "Epoch 274/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.2822 - acc: 0.9195 - val_loss: 2.2223 - val_acc: 0.6136\n",
            "Epoch 275/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2784 - acc: 0.9195 - val_loss: 2.2266 - val_acc: 0.5909\n",
            "Epoch 276/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2847 - acc: 0.8966 - val_loss: 2.2282 - val_acc: 0.5682\n",
            "Epoch 277/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2814 - acc: 0.9080 - val_loss: 2.2294 - val_acc: 0.5909\n",
            "Epoch 278/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2766 - acc: 0.9080 - val_loss: 2.2372 - val_acc: 0.5682\n",
            "Epoch 279/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.2777 - acc: 0.9080 - val_loss: 2.2401 - val_acc: 0.5682\n",
            "Epoch 280/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2756 - acc: 0.9080 - val_loss: 2.2445 - val_acc: 0.5909\n",
            "Epoch 281/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2752 - acc: 0.9195 - val_loss: 2.2489 - val_acc: 0.5909\n",
            "Epoch 282/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2753 - acc: 0.9080 - val_loss: 2.2543 - val_acc: 0.5909\n",
            "Epoch 283/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2736 - acc: 0.8966 - val_loss: 2.2589 - val_acc: 0.5909\n",
            "Epoch 284/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2715 - acc: 0.9195 - val_loss: 2.2605 - val_acc: 0.5682\n",
            "Epoch 285/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.2707 - acc: 0.9310 - val_loss: 2.2658 - val_acc: 0.5909\n",
            "Epoch 286/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2717 - acc: 0.9310 - val_loss: 2.2645 - val_acc: 0.5682\n",
            "Epoch 287/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2705 - acc: 0.9080 - val_loss: 2.2724 - val_acc: 0.5909\n",
            "Epoch 288/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.2663 - acc: 0.9310 - val_loss: 2.2796 - val_acc: 0.5909\n",
            "Epoch 289/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2704 - acc: 0.9195 - val_loss: 2.2828 - val_acc: 0.5682\n",
            "Epoch 290/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.2699 - acc: 0.9195 - val_loss: 2.2895 - val_acc: 0.5682\n",
            "Epoch 291/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2666 - acc: 0.9195 - val_loss: 2.2934 - val_acc: 0.5909\n",
            "Epoch 292/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2642 - acc: 0.9310 - val_loss: 2.2991 - val_acc: 0.5682\n",
            "Epoch 293/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2635 - acc: 0.9195 - val_loss: 2.3068 - val_acc: 0.5682\n",
            "Epoch 294/500\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.2616 - acc: 0.9195 - val_loss: 2.3104 - val_acc: 0.5909\n",
            "Epoch 295/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2664 - acc: 0.9195 - val_loss: 2.3145 - val_acc: 0.5682\n",
            "Epoch 296/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2632 - acc: 0.9195 - val_loss: 2.3196 - val_acc: 0.5909\n",
            "Epoch 297/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2597 - acc: 0.9195 - val_loss: 2.3231 - val_acc: 0.5909\n",
            "Epoch 298/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2584 - acc: 0.9080 - val_loss: 2.3286 - val_acc: 0.5909\n",
            "Epoch 299/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2585 - acc: 0.9310 - val_loss: 2.3304 - val_acc: 0.5909\n",
            "Epoch 300/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2543 - acc: 0.9310 - val_loss: 2.3354 - val_acc: 0.5909\n",
            "Epoch 301/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2545 - acc: 0.9310 - val_loss: 2.3418 - val_acc: 0.5909\n",
            "Epoch 302/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2516 - acc: 0.9310 - val_loss: 2.3434 - val_acc: 0.5909\n",
            "Epoch 303/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2528 - acc: 0.9310 - val_loss: 2.3529 - val_acc: 0.5909\n",
            "Epoch 304/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.2516 - acc: 0.9195 - val_loss: 2.3586 - val_acc: 0.5909\n",
            "Epoch 305/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2498 - acc: 0.9080 - val_loss: 2.3646 - val_acc: 0.5909\n",
            "Epoch 306/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.2493 - acc: 0.9310 - val_loss: 2.3727 - val_acc: 0.5909\n",
            "Epoch 307/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2487 - acc: 0.9310 - val_loss: 2.3743 - val_acc: 0.5909\n",
            "Epoch 308/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2450 - acc: 0.9310 - val_loss: 2.3820 - val_acc: 0.5909\n",
            "Epoch 309/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.2481 - acc: 0.9310 - val_loss: 2.3880 - val_acc: 0.5909\n",
            "Epoch 310/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.2459 - acc: 0.9195 - val_loss: 2.3924 - val_acc: 0.5909\n",
            "Epoch 311/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2417 - acc: 0.9310 - val_loss: 2.3948 - val_acc: 0.5909\n",
            "Epoch 312/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2423 - acc: 0.9310 - val_loss: 2.3981 - val_acc: 0.5909\n",
            "Epoch 313/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2393 - acc: 0.9195 - val_loss: 2.4025 - val_acc: 0.5909\n",
            "Epoch 314/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2416 - acc: 0.9310 - val_loss: 2.4091 - val_acc: 0.5909\n",
            "Epoch 315/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2401 - acc: 0.9195 - val_loss: 2.4104 - val_acc: 0.5909\n",
            "Epoch 316/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2399 - acc: 0.9310 - val_loss: 2.4166 - val_acc: 0.5909\n",
            "Epoch 317/500\n",
            "87/87 [==============================] - 0s 191us/step - loss: 0.2366 - acc: 0.9310 - val_loss: 2.4176 - val_acc: 0.5909\n",
            "Epoch 318/500\n",
            "87/87 [==============================] - 0s 192us/step - loss: 0.2362 - acc: 0.9310 - val_loss: 2.4228 - val_acc: 0.5909\n",
            "Epoch 319/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2365 - acc: 0.9310 - val_loss: 2.4266 - val_acc: 0.5909\n",
            "Epoch 320/500\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.2344 - acc: 0.9310 - val_loss: 2.4348 - val_acc: 0.5909\n",
            "Epoch 321/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2331 - acc: 0.9310 - val_loss: 2.4390 - val_acc: 0.5909\n",
            "Epoch 322/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2329 - acc: 0.9310 - val_loss: 2.4406 - val_acc: 0.5909\n",
            "Epoch 323/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2319 - acc: 0.9310 - val_loss: 2.4461 - val_acc: 0.5909\n",
            "Epoch 324/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.2309 - acc: 0.9425 - val_loss: 2.4470 - val_acc: 0.5909\n",
            "Epoch 325/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2309 - acc: 0.9310 - val_loss: 2.4543 - val_acc: 0.5909\n",
            "Epoch 326/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2315 - acc: 0.9310 - val_loss: 2.4546 - val_acc: 0.5909\n",
            "Epoch 327/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2284 - acc: 0.9425 - val_loss: 2.4569 - val_acc: 0.5909\n",
            "Epoch 328/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2284 - acc: 0.9425 - val_loss: 2.4577 - val_acc: 0.5909\n",
            "Epoch 329/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2280 - acc: 0.9310 - val_loss: 2.4717 - val_acc: 0.5909\n",
            "Epoch 330/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2253 - acc: 0.9425 - val_loss: 2.4733 - val_acc: 0.5909\n",
            "Epoch 331/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2256 - acc: 0.9425 - val_loss: 2.4735 - val_acc: 0.5909\n",
            "Epoch 332/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2266 - acc: 0.9425 - val_loss: 2.4836 - val_acc: 0.5909\n",
            "Epoch 333/500\n",
            "87/87 [==============================] - 0s 312us/step - loss: 0.2247 - acc: 0.9425 - val_loss: 2.4885 - val_acc: 0.5682\n",
            "Epoch 334/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.2248 - acc: 0.9425 - val_loss: 2.4841 - val_acc: 0.5682\n",
            "Epoch 335/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2214 - acc: 0.9425 - val_loss: 2.4868 - val_acc: 0.5682\n",
            "Epoch 336/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2211 - acc: 0.9425 - val_loss: 2.4887 - val_acc: 0.5682\n",
            "Epoch 337/500\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.2213 - acc: 0.9425 - val_loss: 2.4937 - val_acc: 0.5682\n",
            "Epoch 338/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2207 - acc: 0.9425 - val_loss: 2.5033 - val_acc: 0.5682\n",
            "Epoch 339/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.2202 - acc: 0.9425 - val_loss: 2.5037 - val_acc: 0.5682\n",
            "Epoch 340/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2181 - acc: 0.9425 - val_loss: 2.5057 - val_acc: 0.5682\n",
            "Epoch 341/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.2187 - acc: 0.9425 - val_loss: 2.5125 - val_acc: 0.5682\n",
            "Epoch 342/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.2172 - acc: 0.9310 - val_loss: 2.5116 - val_acc: 0.5682\n",
            "Epoch 343/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.2169 - acc: 0.9425 - val_loss: 2.5193 - val_acc: 0.5682\n",
            "Epoch 344/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2130 - acc: 0.9540 - val_loss: 2.5209 - val_acc: 0.5682\n",
            "Epoch 345/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2137 - acc: 0.9425 - val_loss: 2.5264 - val_acc: 0.5682\n",
            "Epoch 346/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2139 - acc: 0.9425 - val_loss: 2.5292 - val_acc: 0.5682\n",
            "Epoch 347/500\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.2129 - acc: 0.9540 - val_loss: 2.5316 - val_acc: 0.5682\n",
            "Epoch 348/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.2114 - acc: 0.9540 - val_loss: 2.5330 - val_acc: 0.5682\n",
            "Epoch 349/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2154 - acc: 0.9310 - val_loss: 2.5374 - val_acc: 0.5682\n",
            "Epoch 350/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2125 - acc: 0.9540 - val_loss: 2.5470 - val_acc: 0.5682\n",
            "Epoch 351/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2094 - acc: 0.9425 - val_loss: 2.5438 - val_acc: 0.5682\n",
            "Epoch 352/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2102 - acc: 0.9540 - val_loss: 2.5479 - val_acc: 0.5682\n",
            "Epoch 353/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2078 - acc: 0.9425 - val_loss: 2.5517 - val_acc: 0.5682\n",
            "Epoch 354/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2079 - acc: 0.9540 - val_loss: 2.5523 - val_acc: 0.5682\n",
            "Epoch 355/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2080 - acc: 0.9425 - val_loss: 2.5579 - val_acc: 0.5682\n",
            "Epoch 356/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2071 - acc: 0.9540 - val_loss: 2.5623 - val_acc: 0.5682\n",
            "Epoch 357/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.2065 - acc: 0.9425 - val_loss: 2.5636 - val_acc: 0.5682\n",
            "Epoch 358/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2064 - acc: 0.9425 - val_loss: 2.5706 - val_acc: 0.5682\n",
            "Epoch 359/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.2049 - acc: 0.9425 - val_loss: 2.5705 - val_acc: 0.5682\n",
            "Epoch 360/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.2041 - acc: 0.9425 - val_loss: 2.5744 - val_acc: 0.5682\n",
            "Epoch 361/500\n",
            "87/87 [==============================] - 0s 320us/step - loss: 0.2029 - acc: 0.9425 - val_loss: 2.5782 - val_acc: 0.5682\n",
            "Epoch 362/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.2027 - acc: 0.9540 - val_loss: 2.5773 - val_acc: 0.5682\n",
            "Epoch 363/500\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.2021 - acc: 0.9540 - val_loss: 2.5888 - val_acc: 0.5682\n",
            "Epoch 364/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2026 - acc: 0.9540 - val_loss: 2.5895 - val_acc: 0.5682\n",
            "Epoch 365/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2012 - acc: 0.9540 - val_loss: 2.5920 - val_acc: 0.5682\n",
            "Epoch 366/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2014 - acc: 0.9540 - val_loss: 2.5956 - val_acc: 0.5682\n",
            "Epoch 367/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2003 - acc: 0.9540 - val_loss: 2.6007 - val_acc: 0.5682\n",
            "Epoch 368/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1974 - acc: 0.9540 - val_loss: 2.5994 - val_acc: 0.5682\n",
            "Epoch 369/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.1979 - acc: 0.9540 - val_loss: 2.6018 - val_acc: 0.5682\n",
            "Epoch 370/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1968 - acc: 0.9540 - val_loss: 2.6054 - val_acc: 0.5682\n",
            "Epoch 371/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1957 - acc: 0.9540 - val_loss: 2.6078 - val_acc: 0.5682\n",
            "Epoch 372/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1970 - acc: 0.9540 - val_loss: 2.6088 - val_acc: 0.5682\n",
            "Epoch 373/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1954 - acc: 0.9540 - val_loss: 2.6108 - val_acc: 0.5682\n",
            "Epoch 374/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.1938 - acc: 0.9540 - val_loss: 2.6104 - val_acc: 0.5682\n",
            "Epoch 375/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1949 - acc: 0.9540 - val_loss: 2.6175 - val_acc: 0.5682\n",
            "Epoch 376/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.1939 - acc: 0.9540 - val_loss: 2.6145 - val_acc: 0.5682\n",
            "Epoch 377/500\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.1919 - acc: 0.9540 - val_loss: 2.6196 - val_acc: 0.5682\n",
            "Epoch 378/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.1946 - acc: 0.9540 - val_loss: 2.6201 - val_acc: 0.5682\n",
            "Epoch 379/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1912 - acc: 0.9540 - val_loss: 2.6223 - val_acc: 0.5682\n",
            "Epoch 380/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1906 - acc: 0.9540 - val_loss: 2.6271 - val_acc: 0.5682\n",
            "Epoch 381/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1890 - acc: 0.9655 - val_loss: 2.6267 - val_acc: 0.5682\n",
            "Epoch 382/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.1892 - acc: 0.9655 - val_loss: 2.6297 - val_acc: 0.5682\n",
            "Epoch 383/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.1894 - acc: 0.9540 - val_loss: 2.6350 - val_acc: 0.5682\n",
            "Epoch 384/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1881 - acc: 0.9655 - val_loss: 2.6356 - val_acc: 0.5682\n",
            "Epoch 385/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1870 - acc: 0.9540 - val_loss: 2.6439 - val_acc: 0.5682\n",
            "Epoch 386/500\n",
            "87/87 [==============================] - 0s 189us/step - loss: 0.1883 - acc: 0.9655 - val_loss: 2.6425 - val_acc: 0.5682\n",
            "Epoch 387/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1862 - acc: 0.9655 - val_loss: 2.6449 - val_acc: 0.5682\n",
            "Epoch 388/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1859 - acc: 0.9655 - val_loss: 2.6462 - val_acc: 0.5682\n",
            "Epoch 389/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1865 - acc: 0.9655 - val_loss: 2.6486 - val_acc: 0.5682\n",
            "Epoch 390/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1842 - acc: 0.9655 - val_loss: 2.6516 - val_acc: 0.5682\n",
            "Epoch 391/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1849 - acc: 0.9655 - val_loss: 2.6556 - val_acc: 0.5682\n",
            "Epoch 392/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1839 - acc: 0.9770 - val_loss: 2.6564 - val_acc: 0.5682\n",
            "Epoch 393/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1807 - acc: 0.9655 - val_loss: 2.6580 - val_acc: 0.5909\n",
            "Epoch 394/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1824 - acc: 0.9655 - val_loss: 2.6630 - val_acc: 0.5682\n",
            "Epoch 395/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1810 - acc: 0.9655 - val_loss: 2.6655 - val_acc: 0.5682\n",
            "Epoch 396/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1830 - acc: 0.9655 - val_loss: 2.6675 - val_acc: 0.5682\n",
            "Epoch 397/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.1792 - acc: 0.9655 - val_loss: 2.6677 - val_acc: 0.5682\n",
            "Epoch 398/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1799 - acc: 0.9770 - val_loss: 2.6721 - val_acc: 0.5909\n",
            "Epoch 399/500\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.1795 - acc: 0.9655 - val_loss: 2.6761 - val_acc: 0.5909\n",
            "Epoch 400/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.1786 - acc: 0.9655 - val_loss: 2.6739 - val_acc: 0.5909\n",
            "Epoch 401/500\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.1782 - acc: 0.9655 - val_loss: 2.6820 - val_acc: 0.5909\n",
            "Epoch 402/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1776 - acc: 0.9770 - val_loss: 2.6840 - val_acc: 0.5909\n",
            "Epoch 403/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1768 - acc: 0.9655 - val_loss: 2.6869 - val_acc: 0.5909\n",
            "Epoch 404/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1763 - acc: 0.9655 - val_loss: 2.6885 - val_acc: 0.5909\n",
            "Epoch 405/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1751 - acc: 0.9770 - val_loss: 2.6867 - val_acc: 0.5909\n",
            "Epoch 406/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.1756 - acc: 0.9770 - val_loss: 2.6905 - val_acc: 0.5909\n",
            "Epoch 407/500\n",
            "87/87 [==============================] - 0s 190us/step - loss: 0.1758 - acc: 0.9655 - val_loss: 2.6954 - val_acc: 0.5909\n",
            "Epoch 408/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1742 - acc: 0.9655 - val_loss: 2.6961 - val_acc: 0.5909\n",
            "Epoch 409/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.1741 - acc: 0.9770 - val_loss: 2.6989 - val_acc: 0.5909\n",
            "Epoch 410/500\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.1729 - acc: 0.9770 - val_loss: 2.6999 - val_acc: 0.5909\n",
            "Epoch 411/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1722 - acc: 0.9655 - val_loss: 2.7042 - val_acc: 0.5909\n",
            "Epoch 412/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1729 - acc: 0.9655 - val_loss: 2.7053 - val_acc: 0.5909\n",
            "Epoch 413/500\n",
            "87/87 [==============================] - 0s 193us/step - loss: 0.1736 - acc: 0.9770 - val_loss: 2.7148 - val_acc: 0.5909\n",
            "Epoch 414/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1717 - acc: 0.9770 - val_loss: 2.7107 - val_acc: 0.5909\n",
            "Epoch 415/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1724 - acc: 0.9770 - val_loss: 2.7148 - val_acc: 0.5909\n",
            "Epoch 416/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.1712 - acc: 0.9655 - val_loss: 2.7160 - val_acc: 0.5909\n",
            "Epoch 417/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1703 - acc: 0.9770 - val_loss: 2.7200 - val_acc: 0.5909\n",
            "Epoch 418/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1688 - acc: 0.9655 - val_loss: 2.7223 - val_acc: 0.5909\n",
            "Epoch 419/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1687 - acc: 0.9770 - val_loss: 2.7260 - val_acc: 0.5909\n",
            "Epoch 420/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1679 - acc: 0.9770 - val_loss: 2.7310 - val_acc: 0.5909\n",
            "Epoch 421/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.1670 - acc: 0.9770 - val_loss: 2.7311 - val_acc: 0.5909\n",
            "Epoch 422/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1674 - acc: 0.9770 - val_loss: 2.7359 - val_acc: 0.5909\n",
            "Epoch 423/500\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.1688 - acc: 0.9770 - val_loss: 2.7374 - val_acc: 0.5909\n",
            "Epoch 424/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1671 - acc: 0.9655 - val_loss: 2.7416 - val_acc: 0.5909\n",
            "Epoch 425/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.1666 - acc: 0.9770 - val_loss: 2.7430 - val_acc: 0.5682\n",
            "Epoch 426/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1659 - acc: 0.9770 - val_loss: 2.7473 - val_acc: 0.5909\n",
            "Epoch 427/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1645 - acc: 0.9770 - val_loss: 2.7486 - val_acc: 0.6136\n",
            "Epoch 428/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1643 - acc: 0.9770 - val_loss: 2.7542 - val_acc: 0.5909\n",
            "Epoch 429/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1639 - acc: 0.9770 - val_loss: 2.7564 - val_acc: 0.6136\n",
            "Epoch 430/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1635 - acc: 0.9770 - val_loss: 2.7567 - val_acc: 0.5909\n",
            "Epoch 431/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1636 - acc: 0.9770 - val_loss: 2.7587 - val_acc: 0.5909\n",
            "Epoch 432/500\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.1620 - acc: 0.9770 - val_loss: 2.7593 - val_acc: 0.5909\n",
            "Epoch 433/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1619 - acc: 0.9770 - val_loss: 2.7637 - val_acc: 0.5682\n",
            "Epoch 434/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1611 - acc: 0.9770 - val_loss: 2.7642 - val_acc: 0.5682\n",
            "Epoch 435/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.1606 - acc: 0.9770 - val_loss: 2.7726 - val_acc: 0.6136\n",
            "Epoch 436/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1594 - acc: 0.9770 - val_loss: 2.7718 - val_acc: 0.6136\n",
            "Epoch 437/500\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.1601 - acc: 0.9770 - val_loss: 2.7757 - val_acc: 0.5909\n",
            "Epoch 438/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1593 - acc: 0.9770 - val_loss: 2.7773 - val_acc: 0.6136\n",
            "Epoch 439/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1595 - acc: 0.9770 - val_loss: 2.7845 - val_acc: 0.5909\n",
            "Epoch 440/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.1591 - acc: 0.9770 - val_loss: 2.7886 - val_acc: 0.5909\n",
            "Epoch 441/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1604 - acc: 0.9770 - val_loss: 2.7886 - val_acc: 0.6136\n",
            "Epoch 442/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1600 - acc: 0.9770 - val_loss: 2.7898 - val_acc: 0.5909\n",
            "Epoch 443/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1571 - acc: 0.9770 - val_loss: 2.7961 - val_acc: 0.6136\n",
            "Epoch 444/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1563 - acc: 0.9770 - val_loss: 2.7976 - val_acc: 0.6136\n",
            "Epoch 445/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.1576 - acc: 0.9770 - val_loss: 2.7995 - val_acc: 0.5909\n",
            "Epoch 446/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.1562 - acc: 0.9770 - val_loss: 2.8002 - val_acc: 0.5909\n",
            "Epoch 447/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1558 - acc: 0.9770 - val_loss: 2.8032 - val_acc: 0.5909\n",
            "Epoch 448/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1548 - acc: 0.9770 - val_loss: 2.8066 - val_acc: 0.5909\n",
            "Epoch 449/500\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.1546 - acc: 0.9770 - val_loss: 2.8103 - val_acc: 0.5909\n",
            "Epoch 450/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1550 - acc: 0.9770 - val_loss: 2.8130 - val_acc: 0.5682\n",
            "Epoch 451/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1540 - acc: 0.9770 - val_loss: 2.8135 - val_acc: 0.5909\n",
            "Epoch 452/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1530 - acc: 0.9770 - val_loss: 2.8167 - val_acc: 0.5909\n",
            "Epoch 453/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.1527 - acc: 0.9770 - val_loss: 2.8196 - val_acc: 0.5909\n",
            "Epoch 454/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.1523 - acc: 0.9770 - val_loss: 2.8203 - val_acc: 0.5909\n",
            "Epoch 455/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1531 - acc: 0.9770 - val_loss: 2.8256 - val_acc: 0.5909\n",
            "Epoch 456/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1527 - acc: 0.9770 - val_loss: 2.8297 - val_acc: 0.5909\n",
            "Epoch 457/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1514 - acc: 0.9770 - val_loss: 2.8292 - val_acc: 0.5909\n",
            "Epoch 458/500\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.1515 - acc: 0.9770 - val_loss: 2.8323 - val_acc: 0.5909\n",
            "Epoch 459/500\n",
            "87/87 [==============================] - 0s 307us/step - loss: 0.1505 - acc: 0.9770 - val_loss: 2.8337 - val_acc: 0.5909\n",
            "Epoch 460/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.1497 - acc: 0.9770 - val_loss: 2.8379 - val_acc: 0.5909\n",
            "Epoch 461/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1499 - acc: 0.9770 - val_loss: 2.8419 - val_acc: 0.5909\n",
            "Epoch 462/500\n",
            "87/87 [==============================] - 0s 301us/step - loss: 0.1494 - acc: 0.9770 - val_loss: 2.8427 - val_acc: 0.5909\n",
            "Epoch 463/500\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.1486 - acc: 0.9770 - val_loss: 2.8446 - val_acc: 0.5909\n",
            "Epoch 464/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.1489 - acc: 0.9770 - val_loss: 2.8484 - val_acc: 0.5909\n",
            "Epoch 465/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1487 - acc: 0.9770 - val_loss: 2.8520 - val_acc: 0.5909\n",
            "Epoch 466/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.1478 - acc: 0.9770 - val_loss: 2.8535 - val_acc: 0.5909\n",
            "Epoch 467/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1480 - acc: 0.9770 - val_loss: 2.8520 - val_acc: 0.5909\n",
            "Epoch 468/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1466 - acc: 0.9770 - val_loss: 2.8567 - val_acc: 0.5909\n",
            "Epoch 469/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.1461 - acc: 0.9770 - val_loss: 2.8607 - val_acc: 0.5909\n",
            "Epoch 470/500\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.1467 - acc: 0.9770 - val_loss: 2.8612 - val_acc: 0.5909\n",
            "Epoch 471/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.1458 - acc: 0.9770 - val_loss: 2.8641 - val_acc: 0.5909\n",
            "Epoch 472/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1451 - acc: 0.9770 - val_loss: 2.8634 - val_acc: 0.5909\n",
            "Epoch 473/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1451 - acc: 0.9770 - val_loss: 2.8659 - val_acc: 0.5909\n",
            "Epoch 474/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1451 - acc: 0.9770 - val_loss: 2.8695 - val_acc: 0.5909\n",
            "Epoch 475/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.1451 - acc: 0.9770 - val_loss: 2.8754 - val_acc: 0.5909\n",
            "Epoch 476/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1449 - acc: 0.9770 - val_loss: 2.8758 - val_acc: 0.5909\n",
            "Epoch 477/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1442 - acc: 0.9770 - val_loss: 2.8767 - val_acc: 0.5909\n",
            "Epoch 478/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1431 - acc: 0.9770 - val_loss: 2.8811 - val_acc: 0.5909\n",
            "Epoch 479/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.1431 - acc: 0.9770 - val_loss: 2.8841 - val_acc: 0.5909\n",
            "Epoch 480/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1432 - acc: 0.9770 - val_loss: 2.8852 - val_acc: 0.5909\n",
            "Epoch 481/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1426 - acc: 0.9770 - val_loss: 2.8902 - val_acc: 0.5909\n",
            "Epoch 482/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1421 - acc: 0.9770 - val_loss: 2.8926 - val_acc: 0.5909\n",
            "Epoch 483/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.1415 - acc: 0.9770 - val_loss: 2.8903 - val_acc: 0.5909\n",
            "Epoch 484/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1413 - acc: 0.9770 - val_loss: 2.8930 - val_acc: 0.5909\n",
            "Epoch 485/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1407 - acc: 0.9770 - val_loss: 2.8985 - val_acc: 0.5909\n",
            "Epoch 486/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1401 - acc: 0.9770 - val_loss: 2.8986 - val_acc: 0.5909\n",
            "Epoch 487/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1403 - acc: 0.9770 - val_loss: 2.9010 - val_acc: 0.5909\n",
            "Epoch 488/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1402 - acc: 0.9770 - val_loss: 2.9040 - val_acc: 0.5909\n",
            "Epoch 489/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1396 - acc: 0.9770 - val_loss: 2.9078 - val_acc: 0.5909\n",
            "Epoch 490/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.1389 - acc: 0.9770 - val_loss: 2.9074 - val_acc: 0.5909\n",
            "Epoch 491/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.1389 - acc: 0.9770 - val_loss: 2.9112 - val_acc: 0.5909\n",
            "Epoch 492/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1385 - acc: 0.9770 - val_loss: 2.9161 - val_acc: 0.5909\n",
            "Epoch 493/500\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.1389 - acc: 0.9770 - val_loss: 2.9170 - val_acc: 0.5909\n",
            "Epoch 494/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.1375 - acc: 0.9770 - val_loss: 2.9198 - val_acc: 0.5909\n",
            "Epoch 495/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1369 - acc: 0.9770 - val_loss: 2.9218 - val_acc: 0.5909\n",
            "Epoch 496/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1376 - acc: 0.9770 - val_loss: 2.9260 - val_acc: 0.5909\n",
            "Epoch 497/500\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.1365 - acc: 0.9770 - val_loss: 2.9268 - val_acc: 0.5909\n",
            "Epoch 498/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1366 - acc: 0.9770 - val_loss: 2.9292 - val_acc: 0.5909\n",
            "Epoch 499/500\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.1356 - acc: 0.9770 - val_loss: 2.9322 - val_acc: 0.5909\n",
            "Epoch 500/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.1359 - acc: 0.9770 - val_loss: 2.9366 - val_acc: 0.5909\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/500\n",
            "87/87 [==============================] - 0s 4ms/step - loss: 1.2585 - acc: 0.2989 - val_loss: 1.0995 - val_acc: 0.3182\n",
            "Epoch 2/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 1.1488 - acc: 0.3908 - val_loss: 1.0443 - val_acc: 0.4318\n",
            "Epoch 3/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 1.0911 - acc: 0.4368 - val_loss: 1.0119 - val_acc: 0.4773\n",
            "Epoch 4/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 1.0528 - acc: 0.4023 - val_loss: 0.9894 - val_acc: 0.5000\n",
            "Epoch 5/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 1.0277 - acc: 0.4138 - val_loss: 0.9736 - val_acc: 0.5000\n",
            "Epoch 6/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 1.0105 - acc: 0.4483 - val_loss: 0.9614 - val_acc: 0.5455\n",
            "Epoch 7/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.9919 - acc: 0.4713 - val_loss: 0.9514 - val_acc: 0.5455\n",
            "Epoch 8/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9808 - acc: 0.4598 - val_loss: 0.9419 - val_acc: 0.5682\n",
            "Epoch 9/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.9670 - acc: 0.5057 - val_loss: 0.9349 - val_acc: 0.5455\n",
            "Epoch 10/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.9561 - acc: 0.5402 - val_loss: 0.9279 - val_acc: 0.5227\n",
            "Epoch 11/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.9518 - acc: 0.5517 - val_loss: 0.9231 - val_acc: 0.5455\n",
            "Epoch 12/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.9383 - acc: 0.5402 - val_loss: 0.9192 - val_acc: 0.5455\n",
            "Epoch 13/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.9338 - acc: 0.5402 - val_loss: 0.9115 - val_acc: 0.5682\n",
            "Epoch 14/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.9242 - acc: 0.5517 - val_loss: 0.9130 - val_acc: 0.5682\n",
            "Epoch 15/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.9187 - acc: 0.5632 - val_loss: 0.9069 - val_acc: 0.5909\n",
            "Epoch 16/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.9102 - acc: 0.5747 - val_loss: 0.9047 - val_acc: 0.5909\n",
            "Epoch 17/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.9065 - acc: 0.5517 - val_loss: 0.9021 - val_acc: 0.5909\n",
            "Epoch 18/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9009 - acc: 0.5747 - val_loss: 0.8985 - val_acc: 0.5909\n",
            "Epoch 19/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8952 - acc: 0.5747 - val_loss: 0.8946 - val_acc: 0.5682\n",
            "Epoch 20/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8886 - acc: 0.5862 - val_loss: 0.8940 - val_acc: 0.5682\n",
            "Epoch 21/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8876 - acc: 0.5862 - val_loss: 0.8916 - val_acc: 0.5909\n",
            "Epoch 22/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8814 - acc: 0.5977 - val_loss: 0.8894 - val_acc: 0.5909\n",
            "Epoch 23/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.8793 - acc: 0.6092 - val_loss: 0.8860 - val_acc: 0.5909\n",
            "Epoch 24/500\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.8735 - acc: 0.5977 - val_loss: 0.8874 - val_acc: 0.5909\n",
            "Epoch 25/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8699 - acc: 0.5977 - val_loss: 0.8848 - val_acc: 0.5909\n",
            "Epoch 26/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8670 - acc: 0.5862 - val_loss: 0.8846 - val_acc: 0.5909\n",
            "Epoch 27/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8626 - acc: 0.6092 - val_loss: 0.8837 - val_acc: 0.5909\n",
            "Epoch 28/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.8606 - acc: 0.6092 - val_loss: 0.8827 - val_acc: 0.6136\n",
            "Epoch 29/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8554 - acc: 0.6092 - val_loss: 0.8813 - val_acc: 0.6136\n",
            "Epoch 30/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8511 - acc: 0.6207 - val_loss: 0.8804 - val_acc: 0.5909\n",
            "Epoch 31/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8487 - acc: 0.6207 - val_loss: 0.8810 - val_acc: 0.5909\n",
            "Epoch 32/500\n",
            "87/87 [==============================] - 0s 308us/step - loss: 0.8454 - acc: 0.6092 - val_loss: 0.8804 - val_acc: 0.5909\n",
            "Epoch 33/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8433 - acc: 0.6092 - val_loss: 0.8799 - val_acc: 0.5909\n",
            "Epoch 34/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8405 - acc: 0.6207 - val_loss: 0.8785 - val_acc: 0.6136\n",
            "Epoch 35/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.8372 - acc: 0.6092 - val_loss: 0.8782 - val_acc: 0.5909\n",
            "Epoch 36/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8336 - acc: 0.6322 - val_loss: 0.8792 - val_acc: 0.5909\n",
            "Epoch 37/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.8325 - acc: 0.6322 - val_loss: 0.8779 - val_acc: 0.5909\n",
            "Epoch 38/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8294 - acc: 0.6207 - val_loss: 0.8789 - val_acc: 0.5909\n",
            "Epoch 39/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.8274 - acc: 0.6437 - val_loss: 0.8799 - val_acc: 0.5682\n",
            "Epoch 40/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8227 - acc: 0.6207 - val_loss: 0.8793 - val_acc: 0.5909\n",
            "Epoch 41/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8204 - acc: 0.6092 - val_loss: 0.8787 - val_acc: 0.5682\n",
            "Epoch 42/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8203 - acc: 0.6322 - val_loss: 0.8788 - val_acc: 0.5909\n",
            "Epoch 43/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8152 - acc: 0.6092 - val_loss: 0.8800 - val_acc: 0.5682\n",
            "Epoch 44/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.8131 - acc: 0.6322 - val_loss: 0.8793 - val_acc: 0.5682\n",
            "Epoch 45/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8085 - acc: 0.6092 - val_loss: 0.8808 - val_acc: 0.5682\n",
            "Epoch 46/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8070 - acc: 0.6092 - val_loss: 0.8830 - val_acc: 0.5682\n",
            "Epoch 47/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8057 - acc: 0.6092 - val_loss: 0.8836 - val_acc: 0.5909\n",
            "Epoch 48/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8026 - acc: 0.6092 - val_loss: 0.8837 - val_acc: 0.5909\n",
            "Epoch 49/500\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.7991 - acc: 0.6092 - val_loss: 0.8866 - val_acc: 0.5909\n",
            "Epoch 50/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7960 - acc: 0.6322 - val_loss: 0.8865 - val_acc: 0.5909\n",
            "Epoch 51/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7956 - acc: 0.6207 - val_loss: 0.8855 - val_acc: 0.5682\n",
            "Epoch 52/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.7914 - acc: 0.6207 - val_loss: 0.8873 - val_acc: 0.5682\n",
            "Epoch 53/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7885 - acc: 0.6322 - val_loss: 0.8876 - val_acc: 0.5909\n",
            "Epoch 54/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.7864 - acc: 0.6322 - val_loss: 0.8880 - val_acc: 0.5682\n",
            "Epoch 55/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.7845 - acc: 0.6207 - val_loss: 0.8894 - val_acc: 0.5682\n",
            "Epoch 56/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.7806 - acc: 0.6322 - val_loss: 0.8899 - val_acc: 0.5909\n",
            "Epoch 57/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.7792 - acc: 0.6322 - val_loss: 0.8910 - val_acc: 0.5909\n",
            "Epoch 58/500\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.7773 - acc: 0.6207 - val_loss: 0.8907 - val_acc: 0.5909\n",
            "Epoch 59/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7747 - acc: 0.6207 - val_loss: 0.8927 - val_acc: 0.5909\n",
            "Epoch 60/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.7723 - acc: 0.6322 - val_loss: 0.8936 - val_acc: 0.5682\n",
            "Epoch 61/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7681 - acc: 0.6322 - val_loss: 0.8947 - val_acc: 0.5682\n",
            "Epoch 62/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.7651 - acc: 0.6207 - val_loss: 0.8951 - val_acc: 0.5682\n",
            "Epoch 63/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7636 - acc: 0.6322 - val_loss: 0.8957 - val_acc: 0.5682\n",
            "Epoch 64/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.7621 - acc: 0.6437 - val_loss: 0.8970 - val_acc: 0.5909\n",
            "Epoch 65/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7593 - acc: 0.6207 - val_loss: 0.8967 - val_acc: 0.6136\n",
            "Epoch 66/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7563 - acc: 0.6437 - val_loss: 0.8987 - val_acc: 0.6136\n",
            "Epoch 67/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.7528 - acc: 0.6322 - val_loss: 0.8984 - val_acc: 0.6136\n",
            "Epoch 68/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.7513 - acc: 0.6322 - val_loss: 0.8990 - val_acc: 0.6136\n",
            "Epoch 69/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7491 - acc: 0.6207 - val_loss: 0.9012 - val_acc: 0.6136\n",
            "Epoch 70/500\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.7481 - acc: 0.6322 - val_loss: 0.8992 - val_acc: 0.6136\n",
            "Epoch 71/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.7427 - acc: 0.6667 - val_loss: 0.9048 - val_acc: 0.5909\n",
            "Epoch 72/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7421 - acc: 0.6437 - val_loss: 0.9041 - val_acc: 0.6136\n",
            "Epoch 73/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7395 - acc: 0.6437 - val_loss: 0.9075 - val_acc: 0.6136\n",
            "Epoch 74/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7365 - acc: 0.6552 - val_loss: 0.9060 - val_acc: 0.6136\n",
            "Epoch 75/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7364 - acc: 0.6207 - val_loss: 0.9101 - val_acc: 0.6136\n",
            "Epoch 76/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.7323 - acc: 0.6437 - val_loss: 0.9112 - val_acc: 0.6136\n",
            "Epoch 77/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.7290 - acc: 0.6437 - val_loss: 0.9130 - val_acc: 0.6136\n",
            "Epoch 78/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7249 - acc: 0.6552 - val_loss: 0.9150 - val_acc: 0.6136\n",
            "Epoch 79/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.7271 - acc: 0.6437 - val_loss: 0.9180 - val_acc: 0.6136\n",
            "Epoch 80/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.7195 - acc: 0.6667 - val_loss: 0.9175 - val_acc: 0.6136\n",
            "Epoch 81/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7228 - acc: 0.6667 - val_loss: 0.9202 - val_acc: 0.6136\n",
            "Epoch 82/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.7143 - acc: 0.6667 - val_loss: 0.9230 - val_acc: 0.5682\n",
            "Epoch 83/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7126 - acc: 0.6667 - val_loss: 0.9244 - val_acc: 0.5909\n",
            "Epoch 84/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7113 - acc: 0.6552 - val_loss: 0.9272 - val_acc: 0.5682\n",
            "Epoch 85/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.7119 - acc: 0.6667 - val_loss: 0.9299 - val_acc: 0.5682\n",
            "Epoch 86/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7063 - acc: 0.6667 - val_loss: 0.9305 - val_acc: 0.5909\n",
            "Epoch 87/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.7046 - acc: 0.6782 - val_loss: 0.9308 - val_acc: 0.6136\n",
            "Epoch 88/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.7012 - acc: 0.6897 - val_loss: 0.9338 - val_acc: 0.5909\n",
            "Epoch 89/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.6982 - acc: 0.6897 - val_loss: 0.9341 - val_acc: 0.5909\n",
            "Epoch 90/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6958 - acc: 0.7011 - val_loss: 0.9366 - val_acc: 0.5909\n",
            "Epoch 91/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6932 - acc: 0.6897 - val_loss: 0.9399 - val_acc: 0.5909\n",
            "Epoch 92/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.6901 - acc: 0.6897 - val_loss: 0.9427 - val_acc: 0.5909\n",
            "Epoch 93/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.6899 - acc: 0.7011 - val_loss: 0.9464 - val_acc: 0.6136\n",
            "Epoch 94/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6849 - acc: 0.7241 - val_loss: 0.9508 - val_acc: 0.6136\n",
            "Epoch 95/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6815 - acc: 0.7126 - val_loss: 0.9521 - val_acc: 0.6136\n",
            "Epoch 96/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.6825 - acc: 0.7011 - val_loss: 0.9516 - val_acc: 0.6136\n",
            "Epoch 97/500\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.6789 - acc: 0.7241 - val_loss: 0.9557 - val_acc: 0.6136\n",
            "Epoch 98/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6794 - acc: 0.7241 - val_loss: 0.9559 - val_acc: 0.6136\n",
            "Epoch 99/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.6727 - acc: 0.7241 - val_loss: 0.9595 - val_acc: 0.6136\n",
            "Epoch 100/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.6731 - acc: 0.7126 - val_loss: 0.9590 - val_acc: 0.6136\n",
            "Epoch 101/500\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.6687 - acc: 0.7356 - val_loss: 0.9637 - val_acc: 0.6136\n",
            "Epoch 102/500\n",
            "87/87 [==============================] - 0s 308us/step - loss: 0.6648 - acc: 0.7356 - val_loss: 0.9673 - val_acc: 0.6136\n",
            "Epoch 103/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6637 - acc: 0.7356 - val_loss: 0.9634 - val_acc: 0.6136\n",
            "Epoch 104/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.6621 - acc: 0.7356 - val_loss: 0.9693 - val_acc: 0.6136\n",
            "Epoch 105/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6610 - acc: 0.7241 - val_loss: 0.9712 - val_acc: 0.6136\n",
            "Epoch 106/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.6578 - acc: 0.7126 - val_loss: 0.9740 - val_acc: 0.6136\n",
            "Epoch 107/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6540 - acc: 0.7356 - val_loss: 0.9752 - val_acc: 0.6136\n",
            "Epoch 108/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.6532 - acc: 0.7356 - val_loss: 0.9751 - val_acc: 0.6136\n",
            "Epoch 109/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.6508 - acc: 0.7356 - val_loss: 0.9753 - val_acc: 0.6136\n",
            "Epoch 110/500\n",
            "87/87 [==============================] - 0s 191us/step - loss: 0.6500 - acc: 0.7356 - val_loss: 0.9826 - val_acc: 0.6136\n",
            "Epoch 111/500\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.6453 - acc: 0.7241 - val_loss: 0.9831 - val_acc: 0.6136\n",
            "Epoch 112/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.6450 - acc: 0.7356 - val_loss: 0.9843 - val_acc: 0.6136\n",
            "Epoch 113/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.6403 - acc: 0.7356 - val_loss: 0.9834 - val_acc: 0.6136\n",
            "Epoch 114/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.6373 - acc: 0.7356 - val_loss: 0.9881 - val_acc: 0.6136\n",
            "Epoch 115/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.6383 - acc: 0.7356 - val_loss: 0.9927 - val_acc: 0.6136\n",
            "Epoch 116/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.6362 - acc: 0.7241 - val_loss: 0.9981 - val_acc: 0.6136\n",
            "Epoch 117/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.6328 - acc: 0.7356 - val_loss: 1.0002 - val_acc: 0.6136\n",
            "Epoch 118/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6292 - acc: 0.7356 - val_loss: 1.0010 - val_acc: 0.6136\n",
            "Epoch 119/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.6278 - acc: 0.7356 - val_loss: 1.0043 - val_acc: 0.6136\n",
            "Epoch 120/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6258 - acc: 0.7356 - val_loss: 1.0048 - val_acc: 0.6136\n",
            "Epoch 121/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.6252 - acc: 0.7356 - val_loss: 1.0065 - val_acc: 0.6136\n",
            "Epoch 122/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.6197 - acc: 0.7356 - val_loss: 1.0058 - val_acc: 0.6136\n",
            "Epoch 123/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.6187 - acc: 0.7356 - val_loss: 1.0120 - val_acc: 0.6136\n",
            "Epoch 124/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.6149 - acc: 0.7356 - val_loss: 1.0148 - val_acc: 0.6136\n",
            "Epoch 125/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.6106 - acc: 0.7356 - val_loss: 1.0228 - val_acc: 0.6136\n",
            "Epoch 126/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6084 - acc: 0.7356 - val_loss: 1.0260 - val_acc: 0.6136\n",
            "Epoch 127/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.6052 - acc: 0.7356 - val_loss: 1.0249 - val_acc: 0.6136\n",
            "Epoch 128/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.6053 - acc: 0.7356 - val_loss: 1.0252 - val_acc: 0.6136\n",
            "Epoch 129/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6027 - acc: 0.7356 - val_loss: 1.0303 - val_acc: 0.6136\n",
            "Epoch 130/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6004 - acc: 0.7356 - val_loss: 1.0340 - val_acc: 0.6136\n",
            "Epoch 131/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6006 - acc: 0.7356 - val_loss: 1.0344 - val_acc: 0.6136\n",
            "Epoch 132/500\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.5964 - acc: 0.7356 - val_loss: 1.0406 - val_acc: 0.6136\n",
            "Epoch 133/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5935 - acc: 0.7356 - val_loss: 1.0442 - val_acc: 0.6136\n",
            "Epoch 134/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5879 - acc: 0.7356 - val_loss: 1.0451 - val_acc: 0.6364\n",
            "Epoch 135/500\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.5907 - acc: 0.7356 - val_loss: 1.0494 - val_acc: 0.6364\n",
            "Epoch 136/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5864 - acc: 0.7356 - val_loss: 1.0522 - val_acc: 0.6364\n",
            "Epoch 137/500\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.5804 - acc: 0.7471 - val_loss: 1.0518 - val_acc: 0.6364\n",
            "Epoch 138/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5792 - acc: 0.7356 - val_loss: 1.0558 - val_acc: 0.6364\n",
            "Epoch 139/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.5799 - acc: 0.7471 - val_loss: 1.0584 - val_acc: 0.6364\n",
            "Epoch 140/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5736 - acc: 0.7471 - val_loss: 1.0612 - val_acc: 0.6364\n",
            "Epoch 141/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.5747 - acc: 0.7471 - val_loss: 1.0682 - val_acc: 0.6136\n",
            "Epoch 142/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.5720 - acc: 0.7471 - val_loss: 1.0673 - val_acc: 0.6136\n",
            "Epoch 143/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.5698 - acc: 0.7471 - val_loss: 1.0698 - val_acc: 0.6136\n",
            "Epoch 144/500\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.5679 - acc: 0.7471 - val_loss: 1.0736 - val_acc: 0.6136\n",
            "Epoch 145/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5634 - acc: 0.7471 - val_loss: 1.0824 - val_acc: 0.6136\n",
            "Epoch 146/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5591 - acc: 0.7471 - val_loss: 1.0826 - val_acc: 0.6136\n",
            "Epoch 147/500\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.5571 - acc: 0.7471 - val_loss: 1.0806 - val_acc: 0.6136\n",
            "Epoch 148/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5565 - acc: 0.7471 - val_loss: 1.0844 - val_acc: 0.6136\n",
            "Epoch 149/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.5536 - acc: 0.7471 - val_loss: 1.0893 - val_acc: 0.6136\n",
            "Epoch 150/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.5505 - acc: 0.7471 - val_loss: 1.0910 - val_acc: 0.6136\n",
            "Epoch 151/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5481 - acc: 0.7471 - val_loss: 1.0942 - val_acc: 0.6136\n",
            "Epoch 152/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.5462 - acc: 0.7471 - val_loss: 1.0958 - val_acc: 0.6136\n",
            "Epoch 153/500\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5449 - acc: 0.7471 - val_loss: 1.1019 - val_acc: 0.6136\n",
            "Epoch 154/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5445 - acc: 0.7471 - val_loss: 1.1055 - val_acc: 0.6136\n",
            "Epoch 155/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5399 - acc: 0.7701 - val_loss: 1.1077 - val_acc: 0.6136\n",
            "Epoch 156/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5383 - acc: 0.7471 - val_loss: 1.1101 - val_acc: 0.6136\n",
            "Epoch 157/500\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.5372 - acc: 0.7701 - val_loss: 1.1160 - val_acc: 0.6136\n",
            "Epoch 158/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5361 - acc: 0.7701 - val_loss: 1.1150 - val_acc: 0.6136\n",
            "Epoch 159/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.5308 - acc: 0.7586 - val_loss: 1.1186 - val_acc: 0.6136\n",
            "Epoch 160/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5291 - acc: 0.7586 - val_loss: 1.1209 - val_acc: 0.6136\n",
            "Epoch 161/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.5284 - acc: 0.7586 - val_loss: 1.1225 - val_acc: 0.6136\n",
            "Epoch 162/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.5248 - acc: 0.7701 - val_loss: 1.1220 - val_acc: 0.6136\n",
            "Epoch 163/500\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.5248 - acc: 0.7586 - val_loss: 1.1267 - val_acc: 0.6136\n",
            "Epoch 164/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5211 - acc: 0.7701 - val_loss: 1.1294 - val_acc: 0.6136\n",
            "Epoch 165/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5217 - acc: 0.7586 - val_loss: 1.1303 - val_acc: 0.6136\n",
            "Epoch 166/500\n",
            "87/87 [==============================] - 0s 193us/step - loss: 0.5169 - acc: 0.7816 - val_loss: 1.1376 - val_acc: 0.6136\n",
            "Epoch 167/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.5172 - acc: 0.7816 - val_loss: 1.1367 - val_acc: 0.6136\n",
            "Epoch 168/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5143 - acc: 0.7701 - val_loss: 1.1443 - val_acc: 0.6136\n",
            "Epoch 169/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.5140 - acc: 0.7931 - val_loss: 1.1371 - val_acc: 0.6136\n",
            "Epoch 170/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5101 - acc: 0.7701 - val_loss: 1.1403 - val_acc: 0.6136\n",
            "Epoch 171/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5081 - acc: 0.7816 - val_loss: 1.1429 - val_acc: 0.6136\n",
            "Epoch 172/500\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.5058 - acc: 0.7701 - val_loss: 1.1484 - val_acc: 0.6136\n",
            "Epoch 173/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5034 - acc: 0.7701 - val_loss: 1.1542 - val_acc: 0.6136\n",
            "Epoch 174/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4992 - acc: 0.7816 - val_loss: 1.1578 - val_acc: 0.6136\n",
            "Epoch 175/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.4998 - acc: 0.8046 - val_loss: 1.1564 - val_acc: 0.6136\n",
            "Epoch 176/500\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.4979 - acc: 0.7816 - val_loss: 1.1681 - val_acc: 0.6136\n",
            "Epoch 177/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4967 - acc: 0.7816 - val_loss: 1.1632 - val_acc: 0.6136\n",
            "Epoch 178/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4951 - acc: 0.7701 - val_loss: 1.1640 - val_acc: 0.6136\n",
            "Epoch 179/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4941 - acc: 0.8046 - val_loss: 1.1672 - val_acc: 0.6136\n",
            "Epoch 180/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.4892 - acc: 0.7816 - val_loss: 1.1677 - val_acc: 0.6136\n",
            "Epoch 181/500\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4870 - acc: 0.8046 - val_loss: 1.1762 - val_acc: 0.6136\n",
            "Epoch 182/500\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.4842 - acc: 0.7816 - val_loss: 1.1802 - val_acc: 0.6136\n",
            "Epoch 183/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4815 - acc: 0.7931 - val_loss: 1.1843 - val_acc: 0.6136\n",
            "Epoch 184/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4840 - acc: 0.7931 - val_loss: 1.1872 - val_acc: 0.6136\n",
            "Epoch 185/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4818 - acc: 0.8046 - val_loss: 1.1902 - val_acc: 0.6136\n",
            "Epoch 186/500\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.4763 - acc: 0.8046 - val_loss: 1.1881 - val_acc: 0.6136\n",
            "Epoch 187/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4746 - acc: 0.8161 - val_loss: 1.1910 - val_acc: 0.6136\n",
            "Epoch 188/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4724 - acc: 0.8046 - val_loss: 1.1946 - val_acc: 0.6136\n",
            "Epoch 189/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4697 - acc: 0.8276 - val_loss: 1.2014 - val_acc: 0.6136\n",
            "Epoch 190/500\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4687 - acc: 0.8276 - val_loss: 1.1993 - val_acc: 0.6136\n",
            "Epoch 191/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4694 - acc: 0.8276 - val_loss: 1.2053 - val_acc: 0.6136\n",
            "Epoch 192/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.4656 - acc: 0.8161 - val_loss: 1.2047 - val_acc: 0.6364\n",
            "Epoch 193/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.4674 - acc: 0.8276 - val_loss: 1.2098 - val_acc: 0.6364\n",
            "Epoch 194/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.4619 - acc: 0.8046 - val_loss: 1.2097 - val_acc: 0.6364\n",
            "Epoch 195/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4609 - acc: 0.8276 - val_loss: 1.2082 - val_acc: 0.6136\n",
            "Epoch 196/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4564 - acc: 0.8276 - val_loss: 1.2091 - val_acc: 0.6364\n",
            "Epoch 197/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.4560 - acc: 0.8391 - val_loss: 1.2175 - val_acc: 0.6136\n",
            "Epoch 198/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4538 - acc: 0.8391 - val_loss: 1.2144 - val_acc: 0.6136\n",
            "Epoch 199/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4536 - acc: 0.8391 - val_loss: 1.2221 - val_acc: 0.6136\n",
            "Epoch 200/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4505 - acc: 0.8506 - val_loss: 1.2232 - val_acc: 0.6136\n",
            "Epoch 201/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4482 - acc: 0.8391 - val_loss: 1.2276 - val_acc: 0.6136\n",
            "Epoch 202/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.4466 - acc: 0.8506 - val_loss: 1.2355 - val_acc: 0.6364\n",
            "Epoch 203/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4448 - acc: 0.8391 - val_loss: 1.2302 - val_acc: 0.6136\n",
            "Epoch 204/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4450 - acc: 0.8391 - val_loss: 1.2344 - val_acc: 0.6136\n",
            "Epoch 205/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4419 - acc: 0.8506 - val_loss: 1.2372 - val_acc: 0.6136\n",
            "Epoch 206/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4401 - acc: 0.8391 - val_loss: 1.2455 - val_acc: 0.6136\n",
            "Epoch 207/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4403 - acc: 0.8391 - val_loss: 1.2413 - val_acc: 0.6136\n",
            "Epoch 208/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.4348 - acc: 0.8391 - val_loss: 1.2436 - val_acc: 0.6136\n",
            "Epoch 209/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4352 - acc: 0.8391 - val_loss: 1.2450 - val_acc: 0.6136\n",
            "Epoch 210/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4320 - acc: 0.8506 - val_loss: 1.2520 - val_acc: 0.6136\n",
            "Epoch 211/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4323 - acc: 0.8391 - val_loss: 1.2546 - val_acc: 0.6136\n",
            "Epoch 212/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.4304 - acc: 0.8621 - val_loss: 1.2577 - val_acc: 0.6136\n",
            "Epoch 213/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4256 - acc: 0.8506 - val_loss: 1.2634 - val_acc: 0.6136\n",
            "Epoch 214/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4240 - acc: 0.8506 - val_loss: 1.2569 - val_acc: 0.6136\n",
            "Epoch 215/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4243 - acc: 0.8391 - val_loss: 1.2576 - val_acc: 0.6136\n",
            "Epoch 216/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4230 - acc: 0.8391 - val_loss: 1.2613 - val_acc: 0.6136\n",
            "Epoch 217/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4188 - acc: 0.8506 - val_loss: 1.2678 - val_acc: 0.6136\n",
            "Epoch 218/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4176 - acc: 0.8506 - val_loss: 1.2663 - val_acc: 0.6136\n",
            "Epoch 219/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4152 - acc: 0.8621 - val_loss: 1.2725 - val_acc: 0.6136\n",
            "Epoch 220/500\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.4134 - acc: 0.8506 - val_loss: 1.2746 - val_acc: 0.6136\n",
            "Epoch 221/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4170 - acc: 0.8391 - val_loss: 1.2684 - val_acc: 0.6136\n",
            "Epoch 222/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4105 - acc: 0.8506 - val_loss: 1.2801 - val_acc: 0.6136\n",
            "Epoch 223/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4085 - acc: 0.8506 - val_loss: 1.2844 - val_acc: 0.6136\n",
            "Epoch 224/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4083 - acc: 0.8506 - val_loss: 1.2850 - val_acc: 0.6136\n",
            "Epoch 225/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4056 - acc: 0.8506 - val_loss: 1.2921 - val_acc: 0.6136\n",
            "Epoch 226/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4045 - acc: 0.8506 - val_loss: 1.2930 - val_acc: 0.6136\n",
            "Epoch 227/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4030 - acc: 0.8506 - val_loss: 1.2930 - val_acc: 0.6136\n",
            "Epoch 228/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4014 - acc: 0.8621 - val_loss: 1.2962 - val_acc: 0.6136\n",
            "Epoch 229/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.4028 - acc: 0.8506 - val_loss: 1.2981 - val_acc: 0.6136\n",
            "Epoch 230/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3994 - acc: 0.8506 - val_loss: 1.2999 - val_acc: 0.6136\n",
            "Epoch 231/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3967 - acc: 0.8621 - val_loss: 1.2976 - val_acc: 0.6136\n",
            "Epoch 232/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3946 - acc: 0.8621 - val_loss: 1.3007 - val_acc: 0.6136\n",
            "Epoch 233/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3950 - acc: 0.8391 - val_loss: 1.3078 - val_acc: 0.6136\n",
            "Epoch 234/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3930 - acc: 0.8621 - val_loss: 1.3104 - val_acc: 0.5909\n",
            "Epoch 235/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3903 - acc: 0.8621 - val_loss: 1.3065 - val_acc: 0.6136\n",
            "Epoch 236/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3906 - acc: 0.8506 - val_loss: 1.3152 - val_acc: 0.6136\n",
            "Epoch 237/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.3875 - acc: 0.8506 - val_loss: 1.3209 - val_acc: 0.6136\n",
            "Epoch 238/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3886 - acc: 0.8506 - val_loss: 1.3217 - val_acc: 0.5909\n",
            "Epoch 239/500\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.3850 - acc: 0.8621 - val_loss: 1.3279 - val_acc: 0.5909\n",
            "Epoch 240/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3864 - acc: 0.8621 - val_loss: 1.3310 - val_acc: 0.5909\n",
            "Epoch 241/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3838 - acc: 0.8506 - val_loss: 1.3338 - val_acc: 0.5909\n",
            "Epoch 242/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3826 - acc: 0.8621 - val_loss: 1.3406 - val_acc: 0.5909\n",
            "Epoch 243/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.3791 - acc: 0.8621 - val_loss: 1.3428 - val_acc: 0.5909\n",
            "Epoch 244/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3809 - acc: 0.8621 - val_loss: 1.3421 - val_acc: 0.5909\n",
            "Epoch 245/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3772 - acc: 0.8506 - val_loss: 1.3468 - val_acc: 0.5909\n",
            "Epoch 246/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3768 - acc: 0.8621 - val_loss: 1.3483 - val_acc: 0.5909\n",
            "Epoch 247/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.3744 - acc: 0.8621 - val_loss: 1.3531 - val_acc: 0.5909\n",
            "Epoch 248/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.3721 - acc: 0.8621 - val_loss: 1.3494 - val_acc: 0.5909\n",
            "Epoch 249/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.3681 - acc: 0.8851 - val_loss: 1.3570 - val_acc: 0.5909\n",
            "Epoch 250/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3688 - acc: 0.8621 - val_loss: 1.3601 - val_acc: 0.5909\n",
            "Epoch 251/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3672 - acc: 0.8621 - val_loss: 1.3655 - val_acc: 0.5909\n",
            "Epoch 252/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.3645 - acc: 0.8621 - val_loss: 1.3687 - val_acc: 0.5909\n",
            "Epoch 253/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3645 - acc: 0.8851 - val_loss: 1.3693 - val_acc: 0.5909\n",
            "Epoch 254/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3654 - acc: 0.8621 - val_loss: 1.3670 - val_acc: 0.5909\n",
            "Epoch 255/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.3622 - acc: 0.8736 - val_loss: 1.3763 - val_acc: 0.5909\n",
            "Epoch 256/500\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3624 - acc: 0.8621 - val_loss: 1.3801 - val_acc: 0.5909\n",
            "Epoch 257/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3637 - acc: 0.8621 - val_loss: 1.3756 - val_acc: 0.5909\n",
            "Epoch 258/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3575 - acc: 0.8736 - val_loss: 1.3783 - val_acc: 0.5909\n",
            "Epoch 259/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.3575 - acc: 0.8736 - val_loss: 1.3864 - val_acc: 0.5909\n",
            "Epoch 260/500\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.3533 - acc: 0.8621 - val_loss: 1.3907 - val_acc: 0.5909\n",
            "Epoch 261/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3542 - acc: 0.8736 - val_loss: 1.3905 - val_acc: 0.5909\n",
            "Epoch 262/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3562 - acc: 0.8621 - val_loss: 1.3911 - val_acc: 0.5909\n",
            "Epoch 263/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3546 - acc: 0.8736 - val_loss: 1.3977 - val_acc: 0.5909\n",
            "Epoch 264/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.3527 - acc: 0.8851 - val_loss: 1.3991 - val_acc: 0.5909\n",
            "Epoch 265/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.3513 - acc: 0.8621 - val_loss: 1.4008 - val_acc: 0.5909\n",
            "Epoch 266/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3446 - acc: 0.8621 - val_loss: 1.4045 - val_acc: 0.5909\n",
            "Epoch 267/500\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.3479 - acc: 0.8736 - val_loss: 1.4053 - val_acc: 0.5909\n",
            "Epoch 268/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.3452 - acc: 0.8736 - val_loss: 1.4054 - val_acc: 0.5909\n",
            "Epoch 269/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3455 - acc: 0.8851 - val_loss: 1.4070 - val_acc: 0.5909\n",
            "Epoch 270/500\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.3448 - acc: 0.8736 - val_loss: 1.4160 - val_acc: 0.5909\n",
            "Epoch 271/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3411 - acc: 0.8736 - val_loss: 1.4189 - val_acc: 0.5909\n",
            "Epoch 272/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3395 - acc: 0.8736 - val_loss: 1.4216 - val_acc: 0.5909\n",
            "Epoch 273/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.3413 - acc: 0.8736 - val_loss: 1.4179 - val_acc: 0.5909\n",
            "Epoch 274/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.3387 - acc: 0.8621 - val_loss: 1.4165 - val_acc: 0.5909\n",
            "Epoch 275/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3349 - acc: 0.8736 - val_loss: 1.4269 - val_acc: 0.5909\n",
            "Epoch 276/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.3349 - acc: 0.8621 - val_loss: 1.4261 - val_acc: 0.5909\n",
            "Epoch 277/500\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.3338 - acc: 0.8621 - val_loss: 1.4346 - val_acc: 0.5909\n",
            "Epoch 278/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3328 - acc: 0.8736 - val_loss: 1.4299 - val_acc: 0.5909\n",
            "Epoch 279/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3331 - acc: 0.8736 - val_loss: 1.4286 - val_acc: 0.5909\n",
            "Epoch 280/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3296 - acc: 0.8851 - val_loss: 1.4347 - val_acc: 0.5909\n",
            "Epoch 281/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3306 - acc: 0.8736 - val_loss: 1.4431 - val_acc: 0.5909\n",
            "Epoch 282/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.3289 - acc: 0.8736 - val_loss: 1.4430 - val_acc: 0.5909\n",
            "Epoch 283/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3280 - acc: 0.8851 - val_loss: 1.4466 - val_acc: 0.5909\n",
            "Epoch 284/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3252 - acc: 0.8966 - val_loss: 1.4507 - val_acc: 0.5909\n",
            "Epoch 285/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3228 - acc: 0.8736 - val_loss: 1.4530 - val_acc: 0.5909\n",
            "Epoch 286/500\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.3241 - acc: 0.8851 - val_loss: 1.4537 - val_acc: 0.5909\n",
            "Epoch 287/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3203 - acc: 0.8851 - val_loss: 1.4664 - val_acc: 0.5909\n",
            "Epoch 288/500\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3213 - acc: 0.8851 - val_loss: 1.4634 - val_acc: 0.5909\n",
            "Epoch 289/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3233 - acc: 0.8966 - val_loss: 1.4667 - val_acc: 0.5909\n",
            "Epoch 290/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.3195 - acc: 0.8851 - val_loss: 1.4749 - val_acc: 0.5909\n",
            "Epoch 291/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.3198 - acc: 0.8736 - val_loss: 1.4739 - val_acc: 0.5909\n",
            "Epoch 292/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3156 - acc: 0.8736 - val_loss: 1.4753 - val_acc: 0.5909\n",
            "Epoch 293/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.3165 - acc: 0.8736 - val_loss: 1.4769 - val_acc: 0.5909\n",
            "Epoch 294/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.3143 - acc: 0.8736 - val_loss: 1.4786 - val_acc: 0.5909\n",
            "Epoch 295/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.3150 - acc: 0.8851 - val_loss: 1.4808 - val_acc: 0.5909\n",
            "Epoch 296/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.3121 - acc: 0.8851 - val_loss: 1.4801 - val_acc: 0.5909\n",
            "Epoch 297/500\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.3117 - acc: 0.8851 - val_loss: 1.4826 - val_acc: 0.5909\n",
            "Epoch 298/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3111 - acc: 0.8736 - val_loss: 1.4902 - val_acc: 0.5909\n",
            "Epoch 299/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.3080 - acc: 0.8851 - val_loss: 1.4867 - val_acc: 0.5909\n",
            "Epoch 300/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3084 - acc: 0.8736 - val_loss: 1.5056 - val_acc: 0.5909\n",
            "Epoch 301/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.3077 - acc: 0.8851 - val_loss: 1.5046 - val_acc: 0.5909\n",
            "Epoch 302/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3058 - acc: 0.8966 - val_loss: 1.4976 - val_acc: 0.5909\n",
            "Epoch 303/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.3058 - acc: 0.8851 - val_loss: 1.5074 - val_acc: 0.5909\n",
            "Epoch 304/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3057 - acc: 0.8966 - val_loss: 1.5067 - val_acc: 0.5909\n",
            "Epoch 305/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3014 - acc: 0.8736 - val_loss: 1.5042 - val_acc: 0.6136\n",
            "Epoch 306/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.3043 - acc: 0.8851 - val_loss: 1.5103 - val_acc: 0.6136\n",
            "Epoch 307/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3022 - acc: 0.8966 - val_loss: 1.5149 - val_acc: 0.6136\n",
            "Epoch 308/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.3011 - acc: 0.8851 - val_loss: 1.5135 - val_acc: 0.6136\n",
            "Epoch 309/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.3056 - acc: 0.8966 - val_loss: 1.5241 - val_acc: 0.5909\n",
            "Epoch 310/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2997 - acc: 0.8966 - val_loss: 1.5237 - val_acc: 0.6136\n",
            "Epoch 311/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2978 - acc: 0.8851 - val_loss: 1.5294 - val_acc: 0.6136\n",
            "Epoch 312/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2957 - acc: 0.8851 - val_loss: 1.5368 - val_acc: 0.5909\n",
            "Epoch 313/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2976 - acc: 0.8851 - val_loss: 1.5352 - val_acc: 0.5909\n",
            "Epoch 314/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.2961 - acc: 0.9195 - val_loss: 1.5390 - val_acc: 0.6136\n",
            "Epoch 315/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2946 - acc: 0.9080 - val_loss: 1.5443 - val_acc: 0.6136\n",
            "Epoch 316/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2925 - acc: 0.9080 - val_loss: 1.5422 - val_acc: 0.6136\n",
            "Epoch 317/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2935 - acc: 0.8966 - val_loss: 1.5444 - val_acc: 0.6136\n",
            "Epoch 318/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.2935 - acc: 0.8966 - val_loss: 1.5518 - val_acc: 0.5909\n",
            "Epoch 319/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2906 - acc: 0.8851 - val_loss: 1.5509 - val_acc: 0.5909\n",
            "Epoch 320/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2889 - acc: 0.9080 - val_loss: 1.5496 - val_acc: 0.5909\n",
            "Epoch 321/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2859 - acc: 0.9080 - val_loss: 1.5491 - val_acc: 0.6136\n",
            "Epoch 322/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2851 - acc: 0.8966 - val_loss: 1.5630 - val_acc: 0.6136\n",
            "Epoch 323/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2853 - acc: 0.9080 - val_loss: 1.5647 - val_acc: 0.5909\n",
            "Epoch 324/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2857 - acc: 0.8966 - val_loss: 1.5669 - val_acc: 0.6136\n",
            "Epoch 325/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2854 - acc: 0.9080 - val_loss: 1.5624 - val_acc: 0.6136\n",
            "Epoch 326/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.2870 - acc: 0.9080 - val_loss: 1.5706 - val_acc: 0.6136\n",
            "Epoch 327/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2816 - acc: 0.9080 - val_loss: 1.5821 - val_acc: 0.5682\n",
            "Epoch 328/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.2815 - acc: 0.9080 - val_loss: 1.5863 - val_acc: 0.5909\n",
            "Epoch 329/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2797 - acc: 0.8966 - val_loss: 1.5825 - val_acc: 0.5909\n",
            "Epoch 330/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2773 - acc: 0.8966 - val_loss: 1.5864 - val_acc: 0.6136\n",
            "Epoch 331/500\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.2780 - acc: 0.9080 - val_loss: 1.5903 - val_acc: 0.6136\n",
            "Epoch 332/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2792 - acc: 0.8966 - val_loss: 1.5888 - val_acc: 0.6136\n",
            "Epoch 333/500\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.2751 - acc: 0.9195 - val_loss: 1.5953 - val_acc: 0.5909\n",
            "Epoch 334/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2765 - acc: 0.9195 - val_loss: 1.5916 - val_acc: 0.6136\n",
            "Epoch 335/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2780 - acc: 0.9080 - val_loss: 1.5923 - val_acc: 0.6136\n",
            "Epoch 336/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2746 - acc: 0.9080 - val_loss: 1.5980 - val_acc: 0.6136\n",
            "Epoch 337/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2737 - acc: 0.9080 - val_loss: 1.6047 - val_acc: 0.5909\n",
            "Epoch 338/500\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.2720 - acc: 0.9195 - val_loss: 1.6074 - val_acc: 0.5909\n",
            "Epoch 339/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2689 - acc: 0.9195 - val_loss: 1.6101 - val_acc: 0.5909\n",
            "Epoch 340/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2692 - acc: 0.9195 - val_loss: 1.6147 - val_acc: 0.5909\n",
            "Epoch 341/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2724 - acc: 0.9080 - val_loss: 1.6175 - val_acc: 0.5909\n",
            "Epoch 342/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2680 - acc: 0.9195 - val_loss: 1.6221 - val_acc: 0.5455\n",
            "Epoch 343/500\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.2683 - acc: 0.9195 - val_loss: 1.6202 - val_acc: 0.5909\n",
            "Epoch 344/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2658 - acc: 0.9080 - val_loss: 1.6248 - val_acc: 0.5909\n",
            "Epoch 345/500\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2664 - acc: 0.9080 - val_loss: 1.6300 - val_acc: 0.5909\n",
            "Epoch 346/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2651 - acc: 0.9080 - val_loss: 1.6344 - val_acc: 0.5909\n",
            "Epoch 347/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2640 - acc: 0.9080 - val_loss: 1.6446 - val_acc: 0.5455\n",
            "Epoch 348/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2631 - acc: 0.9195 - val_loss: 1.6496 - val_acc: 0.5682\n",
            "Epoch 349/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.2635 - acc: 0.9080 - val_loss: 1.6533 - val_acc: 0.5455\n",
            "Epoch 350/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2657 - acc: 0.9080 - val_loss: 1.6449 - val_acc: 0.5909\n",
            "Epoch 351/500\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.2629 - acc: 0.9080 - val_loss: 1.6413 - val_acc: 0.5909\n",
            "Epoch 352/500\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.2630 - acc: 0.9310 - val_loss: 1.6445 - val_acc: 0.5909\n",
            "Epoch 353/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2604 - acc: 0.9080 - val_loss: 1.6552 - val_acc: 0.5909\n",
            "Epoch 354/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2595 - acc: 0.9080 - val_loss: 1.6571 - val_acc: 0.5909\n",
            "Epoch 355/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2568 - acc: 0.9080 - val_loss: 1.6624 - val_acc: 0.5682\n",
            "Epoch 356/500\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.2564 - acc: 0.9195 - val_loss: 1.6666 - val_acc: 0.5682\n",
            "Epoch 357/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2571 - acc: 0.9195 - val_loss: 1.6694 - val_acc: 0.5682\n",
            "Epoch 358/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2568 - acc: 0.9195 - val_loss: 1.6727 - val_acc: 0.5682\n",
            "Epoch 359/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.2532 - acc: 0.9080 - val_loss: 1.6751 - val_acc: 0.5682\n",
            "Epoch 360/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2538 - acc: 0.9195 - val_loss: 1.6821 - val_acc: 0.5682\n",
            "Epoch 361/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2531 - acc: 0.9080 - val_loss: 1.6807 - val_acc: 0.5909\n",
            "Epoch 362/500\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2504 - acc: 0.9195 - val_loss: 1.6830 - val_acc: 0.5682\n",
            "Epoch 363/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2537 - acc: 0.9080 - val_loss: 1.6901 - val_acc: 0.5682\n",
            "Epoch 364/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2503 - acc: 0.9425 - val_loss: 1.6994 - val_acc: 0.5682\n",
            "Epoch 365/500\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.2504 - acc: 0.9425 - val_loss: 1.6945 - val_acc: 0.5682\n",
            "Epoch 366/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2480 - acc: 0.9310 - val_loss: 1.7009 - val_acc: 0.5682\n",
            "Epoch 367/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2489 - acc: 0.9080 - val_loss: 1.7077 - val_acc: 0.5682\n",
            "Epoch 368/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2483 - acc: 0.9540 - val_loss: 1.7085 - val_acc: 0.5682\n",
            "Epoch 369/500\n",
            "87/87 [==============================] - 0s 286us/step - loss: 0.2469 - acc: 0.9425 - val_loss: 1.7057 - val_acc: 0.5682\n",
            "Epoch 370/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2480 - acc: 0.9425 - val_loss: 1.7130 - val_acc: 0.5682\n",
            "Epoch 371/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2483 - acc: 0.9195 - val_loss: 1.7183 - val_acc: 0.5682\n",
            "Epoch 372/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2436 - acc: 0.9425 - val_loss: 1.7309 - val_acc: 0.5682\n",
            "Epoch 373/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2429 - acc: 0.9195 - val_loss: 1.7323 - val_acc: 0.5682\n",
            "Epoch 374/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.2417 - acc: 0.9540 - val_loss: 1.7309 - val_acc: 0.5682\n",
            "Epoch 375/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2414 - acc: 0.9310 - val_loss: 1.7281 - val_acc: 0.5682\n",
            "Epoch 376/500\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.2380 - acc: 0.9655 - val_loss: 1.7397 - val_acc: 0.5682\n",
            "Epoch 377/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2420 - acc: 0.9310 - val_loss: 1.7425 - val_acc: 0.5682\n",
            "Epoch 378/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2386 - acc: 0.9425 - val_loss: 1.7460 - val_acc: 0.5682\n",
            "Epoch 379/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.2392 - acc: 0.9540 - val_loss: 1.7436 - val_acc: 0.5682\n",
            "Epoch 380/500\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.2389 - acc: 0.9540 - val_loss: 1.7466 - val_acc: 0.5682\n",
            "Epoch 381/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2371 - acc: 0.9425 - val_loss: 1.7494 - val_acc: 0.5682\n",
            "Epoch 382/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2356 - acc: 0.9540 - val_loss: 1.7611 - val_acc: 0.5682\n",
            "Epoch 383/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2353 - acc: 0.9425 - val_loss: 1.7611 - val_acc: 0.5682\n",
            "Epoch 384/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.2349 - acc: 0.9540 - val_loss: 1.7628 - val_acc: 0.5682\n",
            "Epoch 385/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2350 - acc: 0.9425 - val_loss: 1.7720 - val_acc: 0.5682\n",
            "Epoch 386/500\n",
            "87/87 [==============================] - 0s 308us/step - loss: 0.2320 - acc: 0.9655 - val_loss: 1.7733 - val_acc: 0.5682\n",
            "Epoch 387/500\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2310 - acc: 0.9540 - val_loss: 1.7777 - val_acc: 0.5682\n",
            "Epoch 388/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.2315 - acc: 0.9540 - val_loss: 1.7793 - val_acc: 0.5682\n",
            "Epoch 389/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.2323 - acc: 0.9425 - val_loss: 1.7833 - val_acc: 0.5682\n",
            "Epoch 390/500\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.2286 - acc: 0.9655 - val_loss: 1.7853 - val_acc: 0.5682\n",
            "Epoch 391/500\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.2310 - acc: 0.9425 - val_loss: 1.7852 - val_acc: 0.5682\n",
            "Epoch 392/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2282 - acc: 0.9425 - val_loss: 1.7933 - val_acc: 0.5682\n",
            "Epoch 393/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.2273 - acc: 0.9540 - val_loss: 1.7925 - val_acc: 0.5682\n",
            "Epoch 394/500\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.2268 - acc: 0.9655 - val_loss: 1.7969 - val_acc: 0.5682\n",
            "Epoch 395/500\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.2258 - acc: 0.9540 - val_loss: 1.7930 - val_acc: 0.5682\n",
            "Epoch 396/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2245 - acc: 0.9540 - val_loss: 1.8045 - val_acc: 0.5682\n",
            "Epoch 397/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.2254 - acc: 0.9655 - val_loss: 1.7998 - val_acc: 0.5682\n",
            "Epoch 398/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2249 - acc: 0.9425 - val_loss: 1.8118 - val_acc: 0.5682\n",
            "Epoch 399/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2249 - acc: 0.9540 - val_loss: 1.8149 - val_acc: 0.5682\n",
            "Epoch 400/500\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.2227 - acc: 0.9425 - val_loss: 1.8102 - val_acc: 0.5682\n",
            "Epoch 401/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2236 - acc: 0.9540 - val_loss: 1.8209 - val_acc: 0.5682\n",
            "Epoch 402/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2220 - acc: 0.9655 - val_loss: 1.8255 - val_acc: 0.5682\n",
            "Epoch 403/500\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.2212 - acc: 0.9540 - val_loss: 1.8296 - val_acc: 0.5682\n",
            "Epoch 404/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2233 - acc: 0.9425 - val_loss: 1.8331 - val_acc: 0.5682\n",
            "Epoch 405/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2205 - acc: 0.9540 - val_loss: 1.8293 - val_acc: 0.5682\n",
            "Epoch 406/500\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.2229 - acc: 0.9655 - val_loss: 1.8237 - val_acc: 0.5682\n",
            "Epoch 407/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2200 - acc: 0.9425 - val_loss: 1.8299 - val_acc: 0.5682\n",
            "Epoch 408/500\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.2191 - acc: 0.9540 - val_loss: 1.8351 - val_acc: 0.5682\n",
            "Epoch 409/500\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2179 - acc: 0.9425 - val_loss: 1.8415 - val_acc: 0.5682\n",
            "Epoch 410/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2176 - acc: 0.9655 - val_loss: 1.8440 - val_acc: 0.5682\n",
            "Epoch 411/500\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.2134 - acc: 0.9655 - val_loss: 1.8411 - val_acc: 0.5682\n",
            "Epoch 412/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2177 - acc: 0.9425 - val_loss: 1.8556 - val_acc: 0.5682\n",
            "Epoch 413/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.2170 - acc: 0.9540 - val_loss: 1.8613 - val_acc: 0.5682\n",
            "Epoch 414/500\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.2142 - acc: 0.9655 - val_loss: 1.8581 - val_acc: 0.5682\n",
            "Epoch 415/500\n",
            "87/87 [==============================] - 0s 307us/step - loss: 0.2163 - acc: 0.9655 - val_loss: 1.8619 - val_acc: 0.5682\n",
            "Epoch 416/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2128 - acc: 0.9540 - val_loss: 1.8609 - val_acc: 0.5682\n",
            "Epoch 417/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2119 - acc: 0.9540 - val_loss: 1.8581 - val_acc: 0.5682\n",
            "Epoch 418/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.2140 - acc: 0.9540 - val_loss: 1.8694 - val_acc: 0.5682\n",
            "Epoch 419/500\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.2152 - acc: 0.9425 - val_loss: 1.8706 - val_acc: 0.5682\n",
            "Epoch 420/500\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.2081 - acc: 0.9655 - val_loss: 1.8791 - val_acc: 0.5682\n",
            "Epoch 421/500\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.2086 - acc: 0.9655 - val_loss: 1.8819 - val_acc: 0.5682\n",
            "Epoch 422/500\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2080 - acc: 0.9655 - val_loss: 1.8830 - val_acc: 0.5682\n",
            "Epoch 423/500\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.2090 - acc: 0.9655 - val_loss: 1.8910 - val_acc: 0.5682\n",
            "Epoch 424/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2088 - acc: 0.9540 - val_loss: 1.8857 - val_acc: 0.5682\n",
            "Epoch 425/500\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.2071 - acc: 0.9655 - val_loss: 1.8923 - val_acc: 0.5682\n",
            "Epoch 426/500\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.2072 - acc: 0.9655 - val_loss: 1.8946 - val_acc: 0.5682\n",
            "Epoch 427/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2047 - acc: 0.9655 - val_loss: 1.9015 - val_acc: 0.5682\n",
            "Epoch 428/500\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.2044 - acc: 0.9655 - val_loss: 1.8998 - val_acc: 0.5682\n",
            "Epoch 429/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.2027 - acc: 0.9655 - val_loss: 1.8998 - val_acc: 0.5682\n",
            "Epoch 430/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2029 - acc: 0.9655 - val_loss: 1.8991 - val_acc: 0.5682\n",
            "Epoch 431/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2042 - acc: 0.9655 - val_loss: 1.9031 - val_acc: 0.5682\n",
            "Epoch 432/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.2024 - acc: 0.9655 - val_loss: 1.9085 - val_acc: 0.5682\n",
            "Epoch 433/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2045 - acc: 0.9540 - val_loss: 1.9172 - val_acc: 0.5682\n",
            "Epoch 434/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.2025 - acc: 0.9540 - val_loss: 1.9124 - val_acc: 0.5682\n",
            "Epoch 435/500\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.2025 - acc: 0.9540 - val_loss: 1.9269 - val_acc: 0.5682\n",
            "Epoch 436/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.2012 - acc: 0.9770 - val_loss: 1.9181 - val_acc: 0.5682\n",
            "Epoch 437/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2034 - acc: 0.9770 - val_loss: 1.9143 - val_acc: 0.5682\n",
            "Epoch 438/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.1992 - acc: 0.9425 - val_loss: 1.9262 - val_acc: 0.5682\n",
            "Epoch 439/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1997 - acc: 0.9540 - val_loss: 1.9288 - val_acc: 0.5682\n",
            "Epoch 440/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1991 - acc: 0.9770 - val_loss: 1.9357 - val_acc: 0.5682\n",
            "Epoch 441/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1974 - acc: 0.9655 - val_loss: 1.9359 - val_acc: 0.5682\n",
            "Epoch 442/500\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.1966 - acc: 0.9540 - val_loss: 1.9344 - val_acc: 0.5682\n",
            "Epoch 443/500\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.1978 - acc: 0.9540 - val_loss: 1.9484 - val_acc: 0.5682\n",
            "Epoch 444/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1961 - acc: 0.9655 - val_loss: 1.9395 - val_acc: 0.5682\n",
            "Epoch 445/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1960 - acc: 0.9655 - val_loss: 1.9503 - val_acc: 0.5682\n",
            "Epoch 446/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.1954 - acc: 0.9770 - val_loss: 1.9515 - val_acc: 0.5682\n",
            "Epoch 447/500\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1933 - acc: 0.9655 - val_loss: 1.9531 - val_acc: 0.5682\n",
            "Epoch 448/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1926 - acc: 0.9770 - val_loss: 1.9551 - val_acc: 0.5682\n",
            "Epoch 449/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.1928 - acc: 0.9770 - val_loss: 1.9538 - val_acc: 0.5682\n",
            "Epoch 450/500\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1914 - acc: 0.9655 - val_loss: 1.9631 - val_acc: 0.5682\n",
            "Epoch 451/500\n",
            "87/87 [==============================] - 0s 309us/step - loss: 0.1903 - acc: 0.9770 - val_loss: 1.9633 - val_acc: 0.5682\n",
            "Epoch 452/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1901 - acc: 0.9770 - val_loss: 1.9697 - val_acc: 0.5682\n",
            "Epoch 453/500\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1907 - acc: 0.9770 - val_loss: 1.9759 - val_acc: 0.5682\n",
            "Epoch 454/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.1899 - acc: 0.9770 - val_loss: 1.9711 - val_acc: 0.5682\n",
            "Epoch 455/500\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1912 - acc: 0.9770 - val_loss: 1.9735 - val_acc: 0.5682\n",
            "Epoch 456/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1882 - acc: 0.9770 - val_loss: 1.9783 - val_acc: 0.5682\n",
            "Epoch 457/500\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1887 - acc: 0.9770 - val_loss: 1.9687 - val_acc: 0.5682\n",
            "Epoch 458/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1889 - acc: 0.9655 - val_loss: 1.9816 - val_acc: 0.5682\n",
            "Epoch 459/500\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.1870 - acc: 0.9770 - val_loss: 1.9889 - val_acc: 0.5682\n",
            "Epoch 460/500\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.1874 - acc: 0.9770 - val_loss: 1.9926 - val_acc: 0.5682\n",
            "Epoch 461/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1851 - acc: 0.9770 - val_loss: 1.9976 - val_acc: 0.5682\n",
            "Epoch 462/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1866 - acc: 0.9770 - val_loss: 1.9961 - val_acc: 0.5682\n",
            "Epoch 463/500\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.1877 - acc: 0.9655 - val_loss: 1.9950 - val_acc: 0.5682\n",
            "Epoch 464/500\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.1838 - acc: 0.9770 - val_loss: 2.0033 - val_acc: 0.5682\n",
            "Epoch 465/500\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.1843 - acc: 0.9770 - val_loss: 2.0130 - val_acc: 0.5682\n",
            "Epoch 466/500\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.1848 - acc: 0.9770 - val_loss: 2.0104 - val_acc: 0.5682\n",
            "Epoch 467/500\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.1826 - acc: 0.9770 - val_loss: 2.0046 - val_acc: 0.5682\n",
            "Epoch 468/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1816 - acc: 0.9770 - val_loss: 2.0132 - val_acc: 0.5682\n",
            "Epoch 469/500\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.1833 - acc: 0.9770 - val_loss: 2.0119 - val_acc: 0.5682\n",
            "Epoch 470/500\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1830 - acc: 0.9770 - val_loss: 2.0257 - val_acc: 0.5682\n",
            "Epoch 471/500\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1826 - acc: 0.9770 - val_loss: 2.0241 - val_acc: 0.5682\n",
            "Epoch 472/500\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.1819 - acc: 0.9655 - val_loss: 2.0289 - val_acc: 0.5682\n",
            "Epoch 473/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.1808 - acc: 0.9770 - val_loss: 2.0340 - val_acc: 0.5682\n",
            "Epoch 474/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1792 - acc: 0.9770 - val_loss: 2.0359 - val_acc: 0.5682\n",
            "Epoch 475/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1773 - acc: 0.9770 - val_loss: 2.0340 - val_acc: 0.5682\n",
            "Epoch 476/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1790 - acc: 0.9770 - val_loss: 2.0302 - val_acc: 0.5682\n",
            "Epoch 477/500\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1780 - acc: 0.9770 - val_loss: 2.0458 - val_acc: 0.5682\n",
            "Epoch 478/500\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1771 - acc: 0.9770 - val_loss: 2.0392 - val_acc: 0.5682\n",
            "Epoch 479/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1778 - acc: 0.9770 - val_loss: 2.0438 - val_acc: 0.5682\n",
            "Epoch 480/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.1775 - acc: 0.9770 - val_loss: 2.0471 - val_acc: 0.5682\n",
            "Epoch 481/500\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.1758 - acc: 0.9770 - val_loss: 2.0530 - val_acc: 0.5682\n",
            "Epoch 482/500\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.1753 - acc: 0.9770 - val_loss: 2.0553 - val_acc: 0.5682\n",
            "Epoch 483/500\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.1738 - acc: 0.9770 - val_loss: 2.0583 - val_acc: 0.5682\n",
            "Epoch 484/500\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.1741 - acc: 0.9655 - val_loss: 2.0490 - val_acc: 0.5682\n",
            "Epoch 485/500\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.1738 - acc: 0.9770 - val_loss: 2.0561 - val_acc: 0.5682\n",
            "Epoch 486/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.1733 - acc: 0.9770 - val_loss: 2.0645 - val_acc: 0.5682\n",
            "Epoch 487/500\n",
            "87/87 [==============================] - 0s 191us/step - loss: 0.1723 - acc: 0.9770 - val_loss: 2.0644 - val_acc: 0.5682\n",
            "Epoch 488/500\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.1721 - acc: 0.9770 - val_loss: 2.0631 - val_acc: 0.5682\n",
            "Epoch 489/500\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.1702 - acc: 0.9770 - val_loss: 2.0806 - val_acc: 0.5682\n",
            "Epoch 490/500\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1694 - acc: 0.9770 - val_loss: 2.0720 - val_acc: 0.5682\n",
            "Epoch 491/500\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.1709 - acc: 0.9770 - val_loss: 2.0786 - val_acc: 0.5682\n",
            "Epoch 492/500\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1700 - acc: 0.9770 - val_loss: 2.0677 - val_acc: 0.5682\n",
            "Epoch 493/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.1716 - acc: 0.9770 - val_loss: 2.0837 - val_acc: 0.5682\n",
            "Epoch 494/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.1703 - acc: 0.9770 - val_loss: 2.0954 - val_acc: 0.5682\n",
            "Epoch 495/500\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1686 - acc: 0.9770 - val_loss: 2.0939 - val_acc: 0.5682\n",
            "Epoch 496/500\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.1691 - acc: 0.9770 - val_loss: 2.0948 - val_acc: 0.5682\n",
            "Epoch 497/500\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.1709 - acc: 0.9770 - val_loss: 2.1048 - val_acc: 0.5682\n",
            "Epoch 498/500\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1660 - acc: 0.9770 - val_loss: 2.1155 - val_acc: 0.5682\n",
            "Epoch 499/500\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1665 - acc: 0.9770 - val_loss: 2.1053 - val_acc: 0.5682\n",
            "Epoch 500/500\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.1672 - acc: 0.9770 - val_loss: 2.1179 - val_acc: 0.5682\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/500\n",
            "88/88 [==============================] - 0s 4ms/step - loss: 1.1675 - acc: 0.3977 - val_loss: 1.0801 - val_acc: 0.4419\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.1124 - acc: 0.4545 - val_loss: 1.0548 - val_acc: 0.4419\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0672 - acc: 0.4545 - val_loss: 1.0380 - val_acc: 0.4419\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0329 - acc: 0.4886 - val_loss: 1.0273 - val_acc: 0.3953\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0056 - acc: 0.5341 - val_loss: 1.0178 - val_acc: 0.4186\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.9804 - acc: 0.5455 - val_loss: 1.0108 - val_acc: 0.3953\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9594 - acc: 0.5568 - val_loss: 1.0078 - val_acc: 0.3953\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9417 - acc: 0.5682 - val_loss: 1.0071 - val_acc: 0.3953\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.9267 - acc: 0.5682 - val_loss: 1.0071 - val_acc: 0.4186\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9107 - acc: 0.5682 - val_loss: 1.0081 - val_acc: 0.4419\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8991 - acc: 0.6023 - val_loss: 1.0076 - val_acc: 0.4419\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8846 - acc: 0.5909 - val_loss: 1.0105 - val_acc: 0.4419\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8768 - acc: 0.6250 - val_loss: 1.0112 - val_acc: 0.4419\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8636 - acc: 0.6364 - val_loss: 1.0142 - val_acc: 0.4651\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8532 - acc: 0.6364 - val_loss: 1.0171 - val_acc: 0.4651\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8446 - acc: 0.6250 - val_loss: 1.0194 - val_acc: 0.4651\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8349 - acc: 0.6477 - val_loss: 1.0257 - val_acc: 0.4651\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8254 - acc: 0.6250 - val_loss: 1.0276 - val_acc: 0.4651\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8159 - acc: 0.6364 - val_loss: 1.0339 - val_acc: 0.4651\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8123 - acc: 0.6477 - val_loss: 1.0386 - val_acc: 0.4651\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8007 - acc: 0.6477 - val_loss: 1.0410 - val_acc: 0.4651\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7922 - acc: 0.6591 - val_loss: 1.0474 - val_acc: 0.4651\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7863 - acc: 0.6364 - val_loss: 1.0525 - val_acc: 0.4651\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.7822 - acc: 0.6591 - val_loss: 1.0566 - val_acc: 0.4651\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7746 - acc: 0.6591 - val_loss: 1.0630 - val_acc: 0.4884\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.7664 - acc: 0.6477 - val_loss: 1.0678 - val_acc: 0.5116\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7596 - acc: 0.6591 - val_loss: 1.0741 - val_acc: 0.5116\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.7558 - acc: 0.6591 - val_loss: 1.0792 - val_acc: 0.5116\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.7499 - acc: 0.6705 - val_loss: 1.0833 - val_acc: 0.5116\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7449 - acc: 0.6477 - val_loss: 1.0875 - val_acc: 0.5116\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7417 - acc: 0.6818 - val_loss: 1.0947 - val_acc: 0.5116\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7373 - acc: 0.6705 - val_loss: 1.1016 - val_acc: 0.5116\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.7331 - acc: 0.6477 - val_loss: 1.1093 - val_acc: 0.5116\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7283 - acc: 0.6705 - val_loss: 1.1155 - val_acc: 0.5116\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7220 - acc: 0.6818 - val_loss: 1.1214 - val_acc: 0.5116\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7180 - acc: 0.6818 - val_loss: 1.1263 - val_acc: 0.5116\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7103 - acc: 0.6818 - val_loss: 1.1308 - val_acc: 0.5116\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.7087 - acc: 0.6705 - val_loss: 1.1375 - val_acc: 0.5116\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.7027 - acc: 0.6818 - val_loss: 1.1437 - val_acc: 0.5116\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7005 - acc: 0.6705 - val_loss: 1.1499 - val_acc: 0.5116\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6979 - acc: 0.6932 - val_loss: 1.1586 - val_acc: 0.5116\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.6953 - acc: 0.6818 - val_loss: 1.1634 - val_acc: 0.5116\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.6894 - acc: 0.6705 - val_loss: 1.1693 - val_acc: 0.5116\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.6856 - acc: 0.6705 - val_loss: 1.1764 - val_acc: 0.5116\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.6817 - acc: 0.6818 - val_loss: 1.1799 - val_acc: 0.5349\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.6763 - acc: 0.6818 - val_loss: 1.1878 - val_acc: 0.5349\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.6729 - acc: 0.6932 - val_loss: 1.1948 - val_acc: 0.5349\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.6702 - acc: 0.6932 - val_loss: 1.2011 - val_acc: 0.5349\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.6692 - acc: 0.6705 - val_loss: 1.2073 - val_acc: 0.5349\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.6665 - acc: 0.6932 - val_loss: 1.2134 - val_acc: 0.5349\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.6614 - acc: 0.6932 - val_loss: 1.2197 - val_acc: 0.5349\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.6597 - acc: 0.6818 - val_loss: 1.2266 - val_acc: 0.5349\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.6537 - acc: 0.6932 - val_loss: 1.2336 - val_acc: 0.5349\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.6520 - acc: 0.6932 - val_loss: 1.2383 - val_acc: 0.5349\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.6503 - acc: 0.6932 - val_loss: 1.2460 - val_acc: 0.5116\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.6461 - acc: 0.6932 - val_loss: 1.2515 - val_acc: 0.5116\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.6418 - acc: 0.6932 - val_loss: 1.2566 - val_acc: 0.5116\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.6405 - acc: 0.6932 - val_loss: 1.2637 - val_acc: 0.5116\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.6389 - acc: 0.7045 - val_loss: 1.2711 - val_acc: 0.5116\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.6357 - acc: 0.7159 - val_loss: 1.2741 - val_acc: 0.4884\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.6328 - acc: 0.7045 - val_loss: 1.2820 - val_acc: 0.4884\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.6314 - acc: 0.6932 - val_loss: 1.2861 - val_acc: 0.5116\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.6251 - acc: 0.7159 - val_loss: 1.2931 - val_acc: 0.5116\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.6241 - acc: 0.7159 - val_loss: 1.2989 - val_acc: 0.4884\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.6198 - acc: 0.7045 - val_loss: 1.3049 - val_acc: 0.5116\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.6195 - acc: 0.7045 - val_loss: 1.3129 - val_acc: 0.4884\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6148 - acc: 0.7159 - val_loss: 1.3159 - val_acc: 0.5116\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.6132 - acc: 0.7159 - val_loss: 1.3199 - val_acc: 0.5116\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.6080 - acc: 0.7159 - val_loss: 1.3297 - val_acc: 0.5116\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.6072 - acc: 0.7273 - val_loss: 1.3356 - val_acc: 0.5116\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.6051 - acc: 0.7273 - val_loss: 1.3423 - val_acc: 0.5116\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.5993 - acc: 0.7273 - val_loss: 1.3485 - val_acc: 0.5116\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.5987 - acc: 0.7386 - val_loss: 1.3536 - val_acc: 0.5116\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.5962 - acc: 0.7273 - val_loss: 1.3584 - val_acc: 0.5116\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.5931 - acc: 0.7159 - val_loss: 1.3659 - val_acc: 0.5116\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5897 - acc: 0.7500 - val_loss: 1.3699 - val_acc: 0.5116\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.5877 - acc: 0.7386 - val_loss: 1.3756 - val_acc: 0.5116\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.5874 - acc: 0.7500 - val_loss: 1.3839 - val_acc: 0.5116\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.5831 - acc: 0.7386 - val_loss: 1.3867 - val_acc: 0.5116\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.5811 - acc: 0.7386 - val_loss: 1.3938 - val_acc: 0.5116\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.5795 - acc: 0.7500 - val_loss: 1.3996 - val_acc: 0.5116\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.5763 - acc: 0.7273 - val_loss: 1.4037 - val_acc: 0.5116\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5742 - acc: 0.7386 - val_loss: 1.4116 - val_acc: 0.4884\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.5697 - acc: 0.7386 - val_loss: 1.4151 - val_acc: 0.4884\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.5678 - acc: 0.7500 - val_loss: 1.4236 - val_acc: 0.4884\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.5651 - acc: 0.7386 - val_loss: 1.4276 - val_acc: 0.4884\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.5628 - acc: 0.7386 - val_loss: 1.4317 - val_acc: 0.4884\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5624 - acc: 0.7386 - val_loss: 1.4370 - val_acc: 0.4884\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.5592 - acc: 0.7500 - val_loss: 1.4459 - val_acc: 0.4884\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.5558 - acc: 0.7500 - val_loss: 1.4511 - val_acc: 0.4884\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.5538 - acc: 0.7614 - val_loss: 1.4540 - val_acc: 0.4884\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.5492 - acc: 0.7614 - val_loss: 1.4630 - val_acc: 0.4884\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.5476 - acc: 0.7727 - val_loss: 1.4654 - val_acc: 0.4884\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5459 - acc: 0.7727 - val_loss: 1.4714 - val_acc: 0.4651\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.5431 - acc: 0.7614 - val_loss: 1.4785 - val_acc: 0.4884\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.5405 - acc: 0.7727 - val_loss: 1.4819 - val_acc: 0.4651\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.5409 - acc: 0.7841 - val_loss: 1.4885 - val_acc: 0.4651\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.5373 - acc: 0.7841 - val_loss: 1.4935 - val_acc: 0.4651\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.5345 - acc: 0.7841 - val_loss: 1.4984 - val_acc: 0.4651\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5333 - acc: 0.7841 - val_loss: 1.5040 - val_acc: 0.4651\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.5291 - acc: 0.7955 - val_loss: 1.5105 - val_acc: 0.4651\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.5269 - acc: 0.7841 - val_loss: 1.5145 - val_acc: 0.4651\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.5235 - acc: 0.7955 - val_loss: 1.5192 - val_acc: 0.4651\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5215 - acc: 0.8182 - val_loss: 1.5239 - val_acc: 0.4651\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.5202 - acc: 0.8182 - val_loss: 1.5282 - val_acc: 0.4651\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5163 - acc: 0.8182 - val_loss: 1.5333 - val_acc: 0.4651\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.5143 - acc: 0.8182 - val_loss: 1.5377 - val_acc: 0.4651\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5122 - acc: 0.7955 - val_loss: 1.5423 - val_acc: 0.4651\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.5086 - acc: 0.8182 - val_loss: 1.5485 - val_acc: 0.4651\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5076 - acc: 0.8182 - val_loss: 1.5531 - val_acc: 0.4651\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5083 - acc: 0.8182 - val_loss: 1.5551 - val_acc: 0.4651\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.5053 - acc: 0.8182 - val_loss: 1.5623 - val_acc: 0.4651\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.4998 - acc: 0.8182 - val_loss: 1.5693 - val_acc: 0.4651\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.4972 - acc: 0.8182 - val_loss: 1.5729 - val_acc: 0.4651\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.4949 - acc: 0.8182 - val_loss: 1.5806 - val_acc: 0.4651\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.4951 - acc: 0.8068 - val_loss: 1.5865 - val_acc: 0.4651\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.4896 - acc: 0.8182 - val_loss: 1.5908 - val_acc: 0.4651\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.4892 - acc: 0.8182 - val_loss: 1.5978 - val_acc: 0.4651\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.4855 - acc: 0.8182 - val_loss: 1.6024 - val_acc: 0.4651\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.4852 - acc: 0.8182 - val_loss: 1.6091 - val_acc: 0.4651\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.4825 - acc: 0.8182 - val_loss: 1.6138 - val_acc: 0.4651\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.4797 - acc: 0.8182 - val_loss: 1.6181 - val_acc: 0.4651\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.4784 - acc: 0.8182 - val_loss: 1.6229 - val_acc: 0.4651\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4754 - acc: 0.8182 - val_loss: 1.6306 - val_acc: 0.4651\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4743 - acc: 0.8182 - val_loss: 1.6347 - val_acc: 0.4651\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4731 - acc: 0.8182 - val_loss: 1.6366 - val_acc: 0.4651\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 0s 283us/step - loss: 0.4703 - acc: 0.8182 - val_loss: 1.6473 - val_acc: 0.4884\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.4673 - acc: 0.8295 - val_loss: 1.6518 - val_acc: 0.4884\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.4660 - acc: 0.8182 - val_loss: 1.6609 - val_acc: 0.4884\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4651 - acc: 0.8182 - val_loss: 1.6651 - val_acc: 0.4884\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4625 - acc: 0.8295 - val_loss: 1.6698 - val_acc: 0.5116\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.4603 - acc: 0.8295 - val_loss: 1.6734 - val_acc: 0.5116\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.4572 - acc: 0.8182 - val_loss: 1.6806 - val_acc: 0.5116\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4557 - acc: 0.8182 - val_loss: 1.6806 - val_acc: 0.5116\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.4532 - acc: 0.8295 - val_loss: 1.6875 - val_acc: 0.5116\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.4525 - acc: 0.8295 - val_loss: 1.6964 - val_acc: 0.5116\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4492 - acc: 0.8295 - val_loss: 1.7015 - val_acc: 0.5116\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.4457 - acc: 0.8295 - val_loss: 1.7073 - val_acc: 0.5116\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.4446 - acc: 0.8295 - val_loss: 1.7081 - val_acc: 0.5116\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.4411 - acc: 0.8295 - val_loss: 1.7178 - val_acc: 0.5116\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.4396 - acc: 0.8295 - val_loss: 1.7208 - val_acc: 0.5116\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.4400 - acc: 0.8295 - val_loss: 1.7266 - val_acc: 0.5116\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.4357 - acc: 0.8409 - val_loss: 1.7361 - val_acc: 0.5116\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4357 - acc: 0.8295 - val_loss: 1.7369 - val_acc: 0.5116\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4321 - acc: 0.8295 - val_loss: 1.7466 - val_acc: 0.5116\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.4299 - acc: 0.8409 - val_loss: 1.7514 - val_acc: 0.5116\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.4281 - acc: 0.8409 - val_loss: 1.7562 - val_acc: 0.5116\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.4275 - acc: 0.8409 - val_loss: 1.7610 - val_acc: 0.5116\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.4257 - acc: 0.8523 - val_loss: 1.7674 - val_acc: 0.5116\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4218 - acc: 0.8409 - val_loss: 1.7761 - val_acc: 0.5116\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.4209 - acc: 0.8409 - val_loss: 1.7810 - val_acc: 0.5116\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4186 - acc: 0.8409 - val_loss: 1.7814 - val_acc: 0.5116\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.4174 - acc: 0.8409 - val_loss: 1.7894 - val_acc: 0.4884\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.4147 - acc: 0.8409 - val_loss: 1.7989 - val_acc: 0.4884\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.4117 - acc: 0.8409 - val_loss: 1.8039 - val_acc: 0.4884\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.4133 - acc: 0.8523 - val_loss: 1.8052 - val_acc: 0.4884\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.4124 - acc: 0.8409 - val_loss: 1.8128 - val_acc: 0.4884\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.4093 - acc: 0.8523 - val_loss: 1.8206 - val_acc: 0.4884\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.4056 - acc: 0.8523 - val_loss: 1.8211 - val_acc: 0.4884\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.4039 - acc: 0.8409 - val_loss: 1.8296 - val_acc: 0.4884\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.4019 - acc: 0.8409 - val_loss: 1.8323 - val_acc: 0.4884\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.4015 - acc: 0.8409 - val_loss: 1.8369 - val_acc: 0.4884\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.3998 - acc: 0.8636 - val_loss: 1.8436 - val_acc: 0.4884\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.3966 - acc: 0.8409 - val_loss: 1.8491 - val_acc: 0.5116\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3964 - acc: 0.8523 - val_loss: 1.8460 - val_acc: 0.4884\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.3933 - acc: 0.8636 - val_loss: 1.8537 - val_acc: 0.4651\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3924 - acc: 0.8636 - val_loss: 1.8600 - val_acc: 0.4651\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.3899 - acc: 0.8523 - val_loss: 1.8660 - val_acc: 0.4651\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.3879 - acc: 0.8636 - val_loss: 1.8653 - val_acc: 0.4651\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.3857 - acc: 0.8636 - val_loss: 1.8714 - val_acc: 0.4651\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.3853 - acc: 0.8864 - val_loss: 1.8781 - val_acc: 0.4651\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.3849 - acc: 0.8636 - val_loss: 1.8791 - val_acc: 0.4651\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.3807 - acc: 0.8750 - val_loss: 1.8822 - val_acc: 0.4651\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.3793 - acc: 0.8750 - val_loss: 1.8863 - val_acc: 0.4651\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.3780 - acc: 0.8750 - val_loss: 1.8926 - val_acc: 0.4651\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.3755 - acc: 0.8750 - val_loss: 1.8940 - val_acc: 0.4651\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.3745 - acc: 0.8636 - val_loss: 1.9002 - val_acc: 0.4651\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3736 - acc: 0.8864 - val_loss: 1.9039 - val_acc: 0.4651\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3713 - acc: 0.8864 - val_loss: 1.9080 - val_acc: 0.4651\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.3703 - acc: 0.8864 - val_loss: 1.9102 - val_acc: 0.4651\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3696 - acc: 0.8977 - val_loss: 1.9168 - val_acc: 0.4651\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3662 - acc: 0.9091 - val_loss: 1.9226 - val_acc: 0.4651\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.3636 - acc: 0.9091 - val_loss: 1.9203 - val_acc: 0.4651\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.3636 - acc: 0.9091 - val_loss: 1.9292 - val_acc: 0.4651\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3619 - acc: 0.9091 - val_loss: 1.9311 - val_acc: 0.4651\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.3600 - acc: 0.9091 - val_loss: 1.9360 - val_acc: 0.4651\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.3586 - acc: 0.9091 - val_loss: 1.9429 - val_acc: 0.4651\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.3575 - acc: 0.9091 - val_loss: 1.9429 - val_acc: 0.4651\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3536 - acc: 0.9091 - val_loss: 1.9498 - val_acc: 0.4651\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.3524 - acc: 0.9091 - val_loss: 1.9554 - val_acc: 0.4651\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.3515 - acc: 0.9091 - val_loss: 1.9548 - val_acc: 0.4651\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.3514 - acc: 0.9091 - val_loss: 1.9563 - val_acc: 0.4651\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.3481 - acc: 0.9091 - val_loss: 1.9668 - val_acc: 0.4651\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3483 - acc: 0.9091 - val_loss: 1.9727 - val_acc: 0.4651\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.3459 - acc: 0.9091 - val_loss: 1.9713 - val_acc: 0.4651\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.3431 - acc: 0.9205 - val_loss: 1.9804 - val_acc: 0.4651\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.3420 - acc: 0.9091 - val_loss: 1.9787 - val_acc: 0.4651\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.3401 - acc: 0.9205 - val_loss: 1.9853 - val_acc: 0.4651\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.3396 - acc: 0.9091 - val_loss: 1.9876 - val_acc: 0.4651\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3373 - acc: 0.9205 - val_loss: 1.9837 - val_acc: 0.4651\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.3361 - acc: 0.9091 - val_loss: 1.9898 - val_acc: 0.4651\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.3350 - acc: 0.9205 - val_loss: 1.9954 - val_acc: 0.4651\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.3323 - acc: 0.9205 - val_loss: 2.0017 - val_acc: 0.4651\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.3314 - acc: 0.9205 - val_loss: 2.0104 - val_acc: 0.4651\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.3314 - acc: 0.9205 - val_loss: 2.0106 - val_acc: 0.4651\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.3280 - acc: 0.9205 - val_loss: 2.0108 - val_acc: 0.4651\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.3276 - acc: 0.9205 - val_loss: 2.0155 - val_acc: 0.4651\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.3259 - acc: 0.9205 - val_loss: 2.0193 - val_acc: 0.4884\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.3229 - acc: 0.9091 - val_loss: 2.0252 - val_acc: 0.4884\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.3230 - acc: 0.9205 - val_loss: 2.0340 - val_acc: 0.4884\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.3216 - acc: 0.9091 - val_loss: 2.0301 - val_acc: 0.4884\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3208 - acc: 0.9205 - val_loss: 2.0327 - val_acc: 0.4651\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.3182 - acc: 0.9205 - val_loss: 2.0372 - val_acc: 0.4884\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.3172 - acc: 0.9205 - val_loss: 2.0393 - val_acc: 0.4884\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.3157 - acc: 0.9205 - val_loss: 2.0464 - val_acc: 0.4884\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.3144 - acc: 0.9091 - val_loss: 2.0497 - val_acc: 0.5116\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.3122 - acc: 0.9205 - val_loss: 2.0537 - val_acc: 0.4884\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.3115 - acc: 0.9205 - val_loss: 2.0607 - val_acc: 0.5116\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.3097 - acc: 0.9205 - val_loss: 2.0573 - val_acc: 0.4884\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.3080 - acc: 0.9205 - val_loss: 2.0634 - val_acc: 0.5116\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.3070 - acc: 0.9205 - val_loss: 2.0648 - val_acc: 0.5116\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.3045 - acc: 0.9205 - val_loss: 2.0753 - val_acc: 0.5116\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.3033 - acc: 0.9205 - val_loss: 2.0757 - val_acc: 0.5116\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.3013 - acc: 0.9205 - val_loss: 2.0769 - val_acc: 0.5116\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.3009 - acc: 0.9205 - val_loss: 2.0822 - val_acc: 0.5116\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.2989 - acc: 0.9205 - val_loss: 2.0859 - val_acc: 0.5116\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2978 - acc: 0.9205 - val_loss: 2.0874 - val_acc: 0.4884\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.2968 - acc: 0.9318 - val_loss: 2.0962 - val_acc: 0.5116\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2950 - acc: 0.9318 - val_loss: 2.0995 - val_acc: 0.5116\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.2932 - acc: 0.9205 - val_loss: 2.0997 - val_acc: 0.5116\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2918 - acc: 0.9205 - val_loss: 2.0979 - val_acc: 0.5116\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.2915 - acc: 0.9205 - val_loss: 2.1048 - val_acc: 0.5116\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2901 - acc: 0.9205 - val_loss: 2.1133 - val_acc: 0.5116\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.2889 - acc: 0.9318 - val_loss: 2.1133 - val_acc: 0.5116\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.2860 - acc: 0.9318 - val_loss: 2.1161 - val_acc: 0.5116\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.2863 - acc: 0.9318 - val_loss: 2.1220 - val_acc: 0.5116\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2837 - acc: 0.9318 - val_loss: 2.1235 - val_acc: 0.5116\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.2828 - acc: 0.9318 - val_loss: 2.1251 - val_acc: 0.5116\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.2827 - acc: 0.9318 - val_loss: 2.1273 - val_acc: 0.5116\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2808 - acc: 0.9318 - val_loss: 2.1307 - val_acc: 0.5116\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.2801 - acc: 0.9318 - val_loss: 2.1361 - val_acc: 0.5116\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2779 - acc: 0.9318 - val_loss: 2.1387 - val_acc: 0.5116\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 0.2777 - acc: 0.9318 - val_loss: 2.1397 - val_acc: 0.5116\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.2764 - acc: 0.9318 - val_loss: 2.1468 - val_acc: 0.5116\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.2741 - acc: 0.9318 - val_loss: 2.1480 - val_acc: 0.5116\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2747 - acc: 0.9318 - val_loss: 2.1509 - val_acc: 0.5116\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.2720 - acc: 0.9318 - val_loss: 2.1570 - val_acc: 0.5116\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.2708 - acc: 0.9318 - val_loss: 2.1578 - val_acc: 0.5116\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.2690 - acc: 0.9318 - val_loss: 2.1639 - val_acc: 0.5116\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.2678 - acc: 0.9318 - val_loss: 2.1617 - val_acc: 0.5116\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.2669 - acc: 0.9318 - val_loss: 2.1663 - val_acc: 0.5116\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.2664 - acc: 0.9318 - val_loss: 2.1686 - val_acc: 0.5116\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.2644 - acc: 0.9318 - val_loss: 2.1757 - val_acc: 0.5116\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.2645 - acc: 0.9432 - val_loss: 2.1787 - val_acc: 0.5116\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2622 - acc: 0.9318 - val_loss: 2.1796 - val_acc: 0.5116\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.2612 - acc: 0.9318 - val_loss: 2.1813 - val_acc: 0.5116\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.2602 - acc: 0.9318 - val_loss: 2.1829 - val_acc: 0.4884\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.2582 - acc: 0.9318 - val_loss: 2.1877 - val_acc: 0.4884\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.2584 - acc: 0.9318 - val_loss: 2.1934 - val_acc: 0.4884\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.2571 - acc: 0.9318 - val_loss: 2.1977 - val_acc: 0.5116\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.2566 - acc: 0.9318 - val_loss: 2.1958 - val_acc: 0.4884\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 0s 286us/step - loss: 0.2546 - acc: 0.9318 - val_loss: 2.2037 - val_acc: 0.4884\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.2525 - acc: 0.9318 - val_loss: 2.2052 - val_acc: 0.4884\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2517 - acc: 0.9318 - val_loss: 2.2087 - val_acc: 0.4884\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.2498 - acc: 0.9318 - val_loss: 2.2110 - val_acc: 0.4884\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.2508 - acc: 0.9318 - val_loss: 2.2127 - val_acc: 0.4884\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.2479 - acc: 0.9318 - val_loss: 2.2173 - val_acc: 0.4884\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.2477 - acc: 0.9318 - val_loss: 2.2188 - val_acc: 0.4884\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.2473 - acc: 0.9432 - val_loss: 2.2215 - val_acc: 0.4884\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.2452 - acc: 0.9318 - val_loss: 2.2266 - val_acc: 0.4884\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.2447 - acc: 0.9318 - val_loss: 2.2305 - val_acc: 0.4884\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.2438 - acc: 0.9432 - val_loss: 2.2309 - val_acc: 0.4884\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.2426 - acc: 0.9432 - val_loss: 2.2366 - val_acc: 0.4884\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.2406 - acc: 0.9318 - val_loss: 2.2371 - val_acc: 0.4884\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.2401 - acc: 0.9318 - val_loss: 2.2387 - val_acc: 0.4884\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2385 - acc: 0.9318 - val_loss: 2.2407 - val_acc: 0.4884\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.2380 - acc: 0.9318 - val_loss: 2.2484 - val_acc: 0.4884\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2378 - acc: 0.9318 - val_loss: 2.2481 - val_acc: 0.4884\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.2356 - acc: 0.9432 - val_loss: 2.2511 - val_acc: 0.4884\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.2352 - acc: 0.9432 - val_loss: 2.2551 - val_acc: 0.4884\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.2344 - acc: 0.9318 - val_loss: 2.2606 - val_acc: 0.4884\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.2330 - acc: 0.9318 - val_loss: 2.2598 - val_acc: 0.4884\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.2321 - acc: 0.9318 - val_loss: 2.2638 - val_acc: 0.4884\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2317 - acc: 0.9318 - val_loss: 2.2646 - val_acc: 0.4884\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.2303 - acc: 0.9432 - val_loss: 2.2709 - val_acc: 0.4884\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.2290 - acc: 0.9318 - val_loss: 2.2710 - val_acc: 0.4884\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.2280 - acc: 0.9545 - val_loss: 2.2786 - val_acc: 0.4884\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.2271 - acc: 0.9432 - val_loss: 2.2780 - val_acc: 0.4884\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.2261 - acc: 0.9432 - val_loss: 2.2847 - val_acc: 0.4884\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2248 - acc: 0.9432 - val_loss: 2.2869 - val_acc: 0.4884\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.2244 - acc: 0.9545 - val_loss: 2.2891 - val_acc: 0.4884\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.2230 - acc: 0.9432 - val_loss: 2.2925 - val_acc: 0.4884\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.2215 - acc: 0.9659 - val_loss: 2.2950 - val_acc: 0.4884\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.2218 - acc: 0.9318 - val_loss: 2.3002 - val_acc: 0.4884\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.2218 - acc: 0.9545 - val_loss: 2.3038 - val_acc: 0.4884\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.2192 - acc: 0.9545 - val_loss: 2.3053 - val_acc: 0.4884\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2184 - acc: 0.9432 - val_loss: 2.3083 - val_acc: 0.4884\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.2174 - acc: 0.9545 - val_loss: 2.3086 - val_acc: 0.4884\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.2165 - acc: 0.9545 - val_loss: 2.3125 - val_acc: 0.4884\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.2157 - acc: 0.9545 - val_loss: 2.3169 - val_acc: 0.4884\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.2162 - acc: 0.9659 - val_loss: 2.3168 - val_acc: 0.4884\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.2142 - acc: 0.9659 - val_loss: 2.3193 - val_acc: 0.4884\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.2131 - acc: 0.9545 - val_loss: 2.3259 - val_acc: 0.4884\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.2117 - acc: 0.9545 - val_loss: 2.3279 - val_acc: 0.4884\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.2111 - acc: 0.9659 - val_loss: 2.3306 - val_acc: 0.4884\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.2109 - acc: 0.9545 - val_loss: 2.3379 - val_acc: 0.4884\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 0s 293us/step - loss: 0.2102 - acc: 0.9659 - val_loss: 2.3365 - val_acc: 0.4884\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.2085 - acc: 0.9659 - val_loss: 2.3411 - val_acc: 0.4884\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.2084 - acc: 0.9659 - val_loss: 2.3406 - val_acc: 0.4884\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.2059 - acc: 0.9659 - val_loss: 2.3453 - val_acc: 0.4884\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.2058 - acc: 0.9659 - val_loss: 2.3500 - val_acc: 0.4884\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.2049 - acc: 0.9659 - val_loss: 2.3531 - val_acc: 0.4884\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.2041 - acc: 0.9659 - val_loss: 2.3583 - val_acc: 0.4884\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.2035 - acc: 0.9659 - val_loss: 2.3593 - val_acc: 0.4884\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.2023 - acc: 0.9545 - val_loss: 2.3583 - val_acc: 0.4884\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.2023 - acc: 0.9659 - val_loss: 2.3626 - val_acc: 0.4884\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.2013 - acc: 0.9659 - val_loss: 2.3670 - val_acc: 0.4884\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.2003 - acc: 0.9659 - val_loss: 2.3708 - val_acc: 0.4884\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1989 - acc: 0.9659 - val_loss: 2.3761 - val_acc: 0.4884\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.1979 - acc: 0.9659 - val_loss: 2.3797 - val_acc: 0.4884\n",
            "Epoch 321/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.1975 - acc: 0.9659 - val_loss: 2.3826 - val_acc: 0.4884\n",
            "Epoch 322/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1973 - acc: 0.9659 - val_loss: 2.3840 - val_acc: 0.4884\n",
            "Epoch 323/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.1966 - acc: 0.9659 - val_loss: 2.3866 - val_acc: 0.4884\n",
            "Epoch 324/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1950 - acc: 0.9659 - val_loss: 2.3889 - val_acc: 0.4884\n",
            "Epoch 325/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1939 - acc: 0.9659 - val_loss: 2.3937 - val_acc: 0.4884\n",
            "Epoch 326/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.1946 - acc: 0.9659 - val_loss: 2.3959 - val_acc: 0.4884\n",
            "Epoch 327/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1932 - acc: 0.9659 - val_loss: 2.4017 - val_acc: 0.4884\n",
            "Epoch 328/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1933 - acc: 0.9659 - val_loss: 2.4003 - val_acc: 0.4884\n",
            "Epoch 329/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.1909 - acc: 0.9659 - val_loss: 2.4024 - val_acc: 0.4884\n",
            "Epoch 330/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.1891 - acc: 0.9659 - val_loss: 2.4076 - val_acc: 0.4884\n",
            "Epoch 331/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.1893 - acc: 0.9659 - val_loss: 2.4130 - val_acc: 0.4884\n",
            "Epoch 332/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.1891 - acc: 0.9659 - val_loss: 2.4105 - val_acc: 0.4884\n",
            "Epoch 333/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1881 - acc: 0.9659 - val_loss: 2.4184 - val_acc: 0.4884\n",
            "Epoch 334/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1864 - acc: 0.9659 - val_loss: 2.4211 - val_acc: 0.4884\n",
            "Epoch 335/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.1869 - acc: 0.9659 - val_loss: 2.4211 - val_acc: 0.4884\n",
            "Epoch 336/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1858 - acc: 0.9659 - val_loss: 2.4248 - val_acc: 0.4884\n",
            "Epoch 337/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.1842 - acc: 0.9659 - val_loss: 2.4281 - val_acc: 0.4884\n",
            "Epoch 338/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.1844 - acc: 0.9659 - val_loss: 2.4305 - val_acc: 0.4884\n",
            "Epoch 339/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.1833 - acc: 0.9659 - val_loss: 2.4312 - val_acc: 0.4884\n",
            "Epoch 340/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1822 - acc: 0.9659 - val_loss: 2.4353 - val_acc: 0.4884\n",
            "Epoch 341/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.1819 - acc: 0.9659 - val_loss: 2.4360 - val_acc: 0.4884\n",
            "Epoch 342/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.1811 - acc: 0.9659 - val_loss: 2.4426 - val_acc: 0.4884\n",
            "Epoch 343/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.1798 - acc: 0.9659 - val_loss: 2.4462 - val_acc: 0.4884\n",
            "Epoch 344/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.1799 - acc: 0.9659 - val_loss: 2.4465 - val_acc: 0.4884\n",
            "Epoch 345/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.1796 - acc: 0.9659 - val_loss: 2.4530 - val_acc: 0.4884\n",
            "Epoch 346/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1784 - acc: 0.9659 - val_loss: 2.4563 - val_acc: 0.4884\n",
            "Epoch 347/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1778 - acc: 0.9659 - val_loss: 2.4603 - val_acc: 0.4884\n",
            "Epoch 348/500\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.1765 - acc: 0.9659 - val_loss: 2.4640 - val_acc: 0.4884\n",
            "Epoch 349/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1753 - acc: 0.9659 - val_loss: 2.4702 - val_acc: 0.4884\n",
            "Epoch 350/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1760 - acc: 0.9659 - val_loss: 2.4671 - val_acc: 0.4884\n",
            "Epoch 351/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.1743 - acc: 0.9659 - val_loss: 2.4759 - val_acc: 0.5116\n",
            "Epoch 352/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1745 - acc: 0.9659 - val_loss: 2.4803 - val_acc: 0.5116\n",
            "Epoch 353/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1736 - acc: 0.9659 - val_loss: 2.4795 - val_acc: 0.4884\n",
            "Epoch 354/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1730 - acc: 0.9659 - val_loss: 2.4813 - val_acc: 0.4884\n",
            "Epoch 355/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1725 - acc: 0.9659 - val_loss: 2.4850 - val_acc: 0.4884\n",
            "Epoch 356/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.1710 - acc: 0.9659 - val_loss: 2.4887 - val_acc: 0.4884\n",
            "Epoch 357/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1700 - acc: 0.9659 - val_loss: 2.4923 - val_acc: 0.4884\n",
            "Epoch 358/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1695 - acc: 0.9659 - val_loss: 2.4963 - val_acc: 0.5116\n",
            "Epoch 359/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.1691 - acc: 0.9659 - val_loss: 2.4970 - val_acc: 0.4884\n",
            "Epoch 360/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.1684 - acc: 0.9659 - val_loss: 2.5030 - val_acc: 0.5116\n",
            "Epoch 361/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1682 - acc: 0.9659 - val_loss: 2.5006 - val_acc: 0.4884\n",
            "Epoch 362/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1669 - acc: 0.9659 - val_loss: 2.5073 - val_acc: 0.5116\n",
            "Epoch 363/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1658 - acc: 0.9659 - val_loss: 2.5088 - val_acc: 0.5116\n",
            "Epoch 364/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.1657 - acc: 0.9659 - val_loss: 2.5164 - val_acc: 0.5116\n",
            "Epoch 365/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.1645 - acc: 0.9659 - val_loss: 2.5171 - val_acc: 0.5116\n",
            "Epoch 366/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1639 - acc: 0.9659 - val_loss: 2.5200 - val_acc: 0.5116\n",
            "Epoch 367/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1634 - acc: 0.9659 - val_loss: 2.5230 - val_acc: 0.5116\n",
            "Epoch 368/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.1627 - acc: 0.9659 - val_loss: 2.5291 - val_acc: 0.5116\n",
            "Epoch 369/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1628 - acc: 0.9659 - val_loss: 2.5311 - val_acc: 0.5116\n",
            "Epoch 370/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.1610 - acc: 0.9659 - val_loss: 2.5340 - val_acc: 0.5116\n",
            "Epoch 371/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1601 - acc: 0.9659 - val_loss: 2.5368 - val_acc: 0.5116\n",
            "Epoch 372/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1606 - acc: 0.9659 - val_loss: 2.5362 - val_acc: 0.5116\n",
            "Epoch 373/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.1592 - acc: 0.9659 - val_loss: 2.5442 - val_acc: 0.5116\n",
            "Epoch 374/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1591 - acc: 0.9773 - val_loss: 2.5444 - val_acc: 0.5116\n",
            "Epoch 375/500\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.1583 - acc: 0.9773 - val_loss: 2.5495 - val_acc: 0.5116\n",
            "Epoch 376/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.1576 - acc: 0.9659 - val_loss: 2.5490 - val_acc: 0.5116\n",
            "Epoch 377/500\n",
            "88/88 [==============================] - 0s 381us/step - loss: 0.1568 - acc: 0.9773 - val_loss: 2.5540 - val_acc: 0.5116\n",
            "Epoch 378/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.1570 - acc: 0.9659 - val_loss: 2.5560 - val_acc: 0.5116\n",
            "Epoch 379/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1562 - acc: 0.9659 - val_loss: 2.5631 - val_acc: 0.5116\n",
            "Epoch 380/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.1556 - acc: 0.9659 - val_loss: 2.5630 - val_acc: 0.5116\n",
            "Epoch 381/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.1548 - acc: 0.9659 - val_loss: 2.5702 - val_acc: 0.5116\n",
            "Epoch 382/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.1533 - acc: 0.9773 - val_loss: 2.5721 - val_acc: 0.5116\n",
            "Epoch 383/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1535 - acc: 0.9773 - val_loss: 2.5731 - val_acc: 0.5116\n",
            "Epoch 384/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1526 - acc: 0.9773 - val_loss: 2.5821 - val_acc: 0.5116\n",
            "Epoch 385/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.1528 - acc: 0.9659 - val_loss: 2.5774 - val_acc: 0.5116\n",
            "Epoch 386/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.1515 - acc: 0.9773 - val_loss: 2.5800 - val_acc: 0.5116\n",
            "Epoch 387/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1506 - acc: 0.9773 - val_loss: 2.5822 - val_acc: 0.5116\n",
            "Epoch 388/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.1502 - acc: 0.9773 - val_loss: 2.5896 - val_acc: 0.5116\n",
            "Epoch 389/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1495 - acc: 0.9773 - val_loss: 2.5937 - val_acc: 0.5116\n",
            "Epoch 390/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1496 - acc: 0.9773 - val_loss: 2.5918 - val_acc: 0.5116\n",
            "Epoch 391/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1486 - acc: 0.9773 - val_loss: 2.5950 - val_acc: 0.5116\n",
            "Epoch 392/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.1480 - acc: 0.9773 - val_loss: 2.6041 - val_acc: 0.5116\n",
            "Epoch 393/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1475 - acc: 0.9773 - val_loss: 2.6003 - val_acc: 0.5116\n",
            "Epoch 394/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1470 - acc: 0.9773 - val_loss: 2.6055 - val_acc: 0.5116\n",
            "Epoch 395/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.1462 - acc: 0.9773 - val_loss: 2.6103 - val_acc: 0.5116\n",
            "Epoch 396/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.1459 - acc: 0.9773 - val_loss: 2.6135 - val_acc: 0.5116\n",
            "Epoch 397/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.1449 - acc: 0.9773 - val_loss: 2.6129 - val_acc: 0.5116\n",
            "Epoch 398/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.1455 - acc: 0.9773 - val_loss: 2.6166 - val_acc: 0.5116\n",
            "Epoch 399/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.1450 - acc: 0.9773 - val_loss: 2.6204 - val_acc: 0.5116\n",
            "Epoch 400/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.1439 - acc: 0.9773 - val_loss: 2.6230 - val_acc: 0.5116\n",
            "Epoch 401/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.1431 - acc: 0.9773 - val_loss: 2.6282 - val_acc: 0.5116\n",
            "Epoch 402/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.1431 - acc: 0.9773 - val_loss: 2.6288 - val_acc: 0.5116\n",
            "Epoch 403/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1425 - acc: 0.9773 - val_loss: 2.6332 - val_acc: 0.5116\n",
            "Epoch 404/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.1410 - acc: 0.9773 - val_loss: 2.6358 - val_acc: 0.5116\n",
            "Epoch 405/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.1412 - acc: 0.9773 - val_loss: 2.6398 - val_acc: 0.5116\n",
            "Epoch 406/500\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.1407 - acc: 0.9773 - val_loss: 2.6437 - val_acc: 0.5116\n",
            "Epoch 407/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.1402 - acc: 0.9773 - val_loss: 2.6428 - val_acc: 0.5116\n",
            "Epoch 408/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.1393 - acc: 0.9773 - val_loss: 2.6491 - val_acc: 0.5116\n",
            "Epoch 409/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.1391 - acc: 0.9773 - val_loss: 2.6501 - val_acc: 0.5116\n",
            "Epoch 410/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1392 - acc: 0.9773 - val_loss: 2.6529 - val_acc: 0.5116\n",
            "Epoch 411/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1380 - acc: 0.9773 - val_loss: 2.6537 - val_acc: 0.5116\n",
            "Epoch 412/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1371 - acc: 0.9773 - val_loss: 2.6595 - val_acc: 0.5116\n",
            "Epoch 413/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.1373 - acc: 0.9773 - val_loss: 2.6599 - val_acc: 0.5116\n",
            "Epoch 414/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.1369 - acc: 0.9773 - val_loss: 2.6608 - val_acc: 0.5116\n",
            "Epoch 415/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1360 - acc: 0.9773 - val_loss: 2.6670 - val_acc: 0.5116\n",
            "Epoch 416/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.1355 - acc: 0.9773 - val_loss: 2.6684 - val_acc: 0.5116\n",
            "Epoch 417/500\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.1353 - acc: 0.9773 - val_loss: 2.6707 - val_acc: 0.5116\n",
            "Epoch 418/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1347 - acc: 0.9773 - val_loss: 2.6762 - val_acc: 0.5116\n",
            "Epoch 419/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.1342 - acc: 0.9773 - val_loss: 2.6739 - val_acc: 0.5116\n",
            "Epoch 420/500\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.1340 - acc: 0.9773 - val_loss: 2.6790 - val_acc: 0.5116\n",
            "Epoch 421/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.1328 - acc: 0.9773 - val_loss: 2.6867 - val_acc: 0.5116\n",
            "Epoch 422/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.1323 - acc: 0.9773 - val_loss: 2.6896 - val_acc: 0.5349\n",
            "Epoch 423/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.1318 - acc: 0.9773 - val_loss: 2.6897 - val_acc: 0.5116\n",
            "Epoch 424/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.1308 - acc: 0.9886 - val_loss: 2.6913 - val_acc: 0.5116\n",
            "Epoch 425/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.1322 - acc: 0.9773 - val_loss: 2.6919 - val_acc: 0.5116\n",
            "Epoch 426/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.1303 - acc: 0.9886 - val_loss: 2.6938 - val_acc: 0.5116\n",
            "Epoch 427/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1303 - acc: 0.9886 - val_loss: 2.6983 - val_acc: 0.5116\n",
            "Epoch 428/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1295 - acc: 0.9773 - val_loss: 2.6995 - val_acc: 0.5116\n",
            "Epoch 429/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1287 - acc: 0.9773 - val_loss: 2.7034 - val_acc: 0.5116\n",
            "Epoch 430/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.1292 - acc: 0.9886 - val_loss: 2.7068 - val_acc: 0.5349\n",
            "Epoch 431/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1281 - acc: 0.9886 - val_loss: 2.7078 - val_acc: 0.5116\n",
            "Epoch 432/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.1272 - acc: 0.9886 - val_loss: 2.7098 - val_acc: 0.5116\n",
            "Epoch 433/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1266 - acc: 0.9886 - val_loss: 2.7117 - val_acc: 0.5116\n",
            "Epoch 434/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1272 - acc: 0.9886 - val_loss: 2.7155 - val_acc: 0.5349\n",
            "Epoch 435/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.1259 - acc: 0.9886 - val_loss: 2.7196 - val_acc: 0.5116\n",
            "Epoch 436/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1259 - acc: 0.9886 - val_loss: 2.7219 - val_acc: 0.5116\n",
            "Epoch 437/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1253 - acc: 0.9886 - val_loss: 2.7264 - val_acc: 0.5116\n",
            "Epoch 438/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1249 - acc: 0.9886 - val_loss: 2.7283 - val_acc: 0.5116\n",
            "Epoch 439/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.1245 - acc: 0.9886 - val_loss: 2.7301 - val_acc: 0.5116\n",
            "Epoch 440/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1241 - acc: 0.9886 - val_loss: 2.7341 - val_acc: 0.5116\n",
            "Epoch 441/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.1238 - acc: 0.9886 - val_loss: 2.7379 - val_acc: 0.5116\n",
            "Epoch 442/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1227 - acc: 0.9886 - val_loss: 2.7368 - val_acc: 0.5116\n",
            "Epoch 443/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.1233 - acc: 0.9886 - val_loss: 2.7422 - val_acc: 0.5116\n",
            "Epoch 444/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1224 - acc: 0.9886 - val_loss: 2.7467 - val_acc: 0.5116\n",
            "Epoch 445/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.1220 - acc: 0.9886 - val_loss: 2.7465 - val_acc: 0.5116\n",
            "Epoch 446/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.1216 - acc: 0.9886 - val_loss: 2.7516 - val_acc: 0.5116\n",
            "Epoch 447/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.1209 - acc: 0.9886 - val_loss: 2.7552 - val_acc: 0.5116\n",
            "Epoch 448/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.1209 - acc: 0.9886 - val_loss: 2.7554 - val_acc: 0.5116\n",
            "Epoch 449/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.1206 - acc: 0.9886 - val_loss: 2.7616 - val_acc: 0.5349\n",
            "Epoch 450/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.1200 - acc: 0.9886 - val_loss: 2.7600 - val_acc: 0.5349\n",
            "Epoch 451/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1199 - acc: 0.9886 - val_loss: 2.7651 - val_acc: 0.5116\n",
            "Epoch 452/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1191 - acc: 0.9886 - val_loss: 2.7658 - val_acc: 0.5349\n",
            "Epoch 453/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.1188 - acc: 0.9886 - val_loss: 2.7695 - val_acc: 0.5116\n",
            "Epoch 454/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1185 - acc: 0.9886 - val_loss: 2.7742 - val_acc: 0.5349\n",
            "Epoch 455/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.1182 - acc: 0.9886 - val_loss: 2.7739 - val_acc: 0.5116\n",
            "Epoch 456/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1176 - acc: 0.9886 - val_loss: 2.7761 - val_acc: 0.5116\n",
            "Epoch 457/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1168 - acc: 0.9886 - val_loss: 2.7825 - val_acc: 0.5116\n",
            "Epoch 458/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1169 - acc: 0.9886 - val_loss: 2.7844 - val_acc: 0.5116\n",
            "Epoch 459/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1163 - acc: 0.9886 - val_loss: 2.7858 - val_acc: 0.5349\n",
            "Epoch 460/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.1154 - acc: 0.9886 - val_loss: 2.7868 - val_acc: 0.5349\n",
            "Epoch 461/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1153 - acc: 0.9886 - val_loss: 2.7891 - val_acc: 0.5116\n",
            "Epoch 462/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.1153 - acc: 0.9886 - val_loss: 2.7977 - val_acc: 0.5116\n",
            "Epoch 463/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1143 - acc: 0.9886 - val_loss: 2.7970 - val_acc: 0.5349\n",
            "Epoch 464/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.1141 - acc: 0.9886 - val_loss: 2.7989 - val_acc: 0.5349\n",
            "Epoch 465/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.1138 - acc: 0.9886 - val_loss: 2.7989 - val_acc: 0.5349\n",
            "Epoch 466/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.1136 - acc: 0.9886 - val_loss: 2.8039 - val_acc: 0.5349\n",
            "Epoch 467/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.1129 - acc: 0.9886 - val_loss: 2.8069 - val_acc: 0.5349\n",
            "Epoch 468/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1129 - acc: 0.9886 - val_loss: 2.8167 - val_acc: 0.5349\n",
            "Epoch 469/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1122 - acc: 0.9886 - val_loss: 2.8155 - val_acc: 0.5349\n",
            "Epoch 470/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.1119 - acc: 0.9886 - val_loss: 2.8173 - val_acc: 0.5349\n",
            "Epoch 471/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1117 - acc: 0.9886 - val_loss: 2.8186 - val_acc: 0.5349\n",
            "Epoch 472/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1116 - acc: 0.9886 - val_loss: 2.8216 - val_acc: 0.5581\n",
            "Epoch 473/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1111 - acc: 0.9886 - val_loss: 2.8255 - val_acc: 0.5349\n",
            "Epoch 474/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1106 - acc: 0.9886 - val_loss: 2.8274 - val_acc: 0.5349\n",
            "Epoch 475/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1107 - acc: 0.9886 - val_loss: 2.8286 - val_acc: 0.5349\n",
            "Epoch 476/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.1097 - acc: 0.9886 - val_loss: 2.8326 - val_acc: 0.5581\n",
            "Epoch 477/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.1093 - acc: 0.9886 - val_loss: 2.8340 - val_acc: 0.5581\n",
            "Epoch 478/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.1088 - acc: 0.9886 - val_loss: 2.8398 - val_acc: 0.5581\n",
            "Epoch 479/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.1092 - acc: 0.9886 - val_loss: 2.8388 - val_acc: 0.5349\n",
            "Epoch 480/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1081 - acc: 0.9886 - val_loss: 2.8445 - val_acc: 0.5349\n",
            "Epoch 481/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1079 - acc: 0.9886 - val_loss: 2.8489 - val_acc: 0.5581\n",
            "Epoch 482/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1074 - acc: 0.9886 - val_loss: 2.8479 - val_acc: 0.5581\n",
            "Epoch 483/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.1071 - acc: 0.9886 - val_loss: 2.8501 - val_acc: 0.5581\n",
            "Epoch 484/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.1070 - acc: 0.9886 - val_loss: 2.8569 - val_acc: 0.5581\n",
            "Epoch 485/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.1067 - acc: 0.9886 - val_loss: 2.8580 - val_acc: 0.5581\n",
            "Epoch 486/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.1057 - acc: 0.9886 - val_loss: 2.8612 - val_acc: 0.5581\n",
            "Epoch 487/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.1057 - acc: 0.9886 - val_loss: 2.8614 - val_acc: 0.5581\n",
            "Epoch 488/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.1053 - acc: 0.9886 - val_loss: 2.8649 - val_acc: 0.5349\n",
            "Epoch 489/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.1052 - acc: 0.9886 - val_loss: 2.8655 - val_acc: 0.5581\n",
            "Epoch 490/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.1044 - acc: 0.9886 - val_loss: 2.8692 - val_acc: 0.5581\n",
            "Epoch 491/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.1045 - acc: 0.9886 - val_loss: 2.8714 - val_acc: 0.5581\n",
            "Epoch 492/500\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.1036 - acc: 0.9886 - val_loss: 2.8725 - val_acc: 0.5581\n",
            "Epoch 493/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.1035 - acc: 0.9886 - val_loss: 2.8765 - val_acc: 0.5581\n",
            "Epoch 494/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.1029 - acc: 0.9886 - val_loss: 2.8774 - val_acc: 0.5581\n",
            "Epoch 495/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1027 - acc: 0.9886 - val_loss: 2.8796 - val_acc: 0.5581\n",
            "Epoch 496/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1028 - acc: 0.9886 - val_loss: 2.8835 - val_acc: 0.5581\n",
            "Epoch 497/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.1019 - acc: 0.9886 - val_loss: 2.8876 - val_acc: 0.5581\n",
            "Epoch 498/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1016 - acc: 0.9886 - val_loss: 2.8907 - val_acc: 0.5581\n",
            "Epoch 499/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.1019 - acc: 0.9886 - val_loss: 2.8932 - val_acc: 0.5581\n",
            "Epoch 500/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.1012 - acc: 0.9886 - val_loss: 2.8952 - val_acc: 0.5581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ba613427-2e71-4cb6-e0e8-be8df10c52fc",
        "id": "6OH7qa6a14-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.09052215e-01, -1.30570140e+00, -1.50987053e+00,\n",
              "         4.78413407e-01, -5.48688420e-01, -2.19260239e-01,\n",
              "        -3.87514118e-01, -2.35668681e-01, -3.27761894e-01,\n",
              "        -3.30961271e-01, -8.29963414e-01,  1.09070485e+00,\n",
              "        -4.68298254e-01, -2.54403631e-01,  2.05362316e-01,\n",
              "        -1.58492380e-01, -8.76044573e-01],\n",
              "       [ 1.93706034e+00,  2.47792697e-01, -6.14324644e-01,\n",
              "        -1.43358516e+00,  1.74280631e-01, -1.01385115e+00,\n",
              "        -5.98180597e-01,  7.79397065e-02, -3.34625115e-01,\n",
              "        -3.30973016e-01, -1.31203202e+00,  4.46338623e-02,\n",
              "        -1.30913346e+00, -5.65265481e-01,  2.19195593e+00,\n",
              "         9.70100653e-01, -1.05149009e+00],\n",
              "       [ 7.34257050e-01,  9.68988268e-02,  3.85769839e-01,\n",
              "         1.61200050e-01, -5.40961150e-01, -3.83658358e-01,\n",
              "        -1.13750969e-01, -2.28591476e-01, -1.15055286e-01,\n",
              "        -3.30299955e-01,  1.35982249e-01,  3.71533408e-01,\n",
              "        -1.21254356e-01, -4.84378805e-01, -5.30458542e-01,\n",
              "        -4.87994766e-01,  1.88481002e-01],\n",
              "       [ 7.68510240e-01, -1.82042583e-01, -3.33326380e-01,\n",
              "         6.50849331e-02, -5.40195473e-01, -8.49453029e-01,\n",
              "         1.35406834e-01, -8.66383484e-01, -3.31690446e-01,\n",
              "        -3.30963175e-01, -7.37179436e-01,  4.04397633e-01,\n",
              "         3.35949635e-01,  3.71018266e-01, -4.24856494e-01,\n",
              "         1.10650982e+00, -5.76281334e-01],\n",
              "       [-2.67444175e-01,  2.78483283e-01,  5.18248153e-01,\n",
              "         1.05778670e+00, -5.44927981e-01, -2.74059612e-01,\n",
              "        -3.14211181e-01,  1.39098834e+00,  3.52354987e-01,\n",
              "         1.74307426e+00,  1.16373157e+00, -1.00481406e+00,\n",
              "        -2.31777499e-01, -7.50286880e-01, -4.86354308e-01,\n",
              "        -7.66223485e-01,  4.69101139e-01],\n",
              "       [ 1.54957687e+00,  1.90539890e+00, -2.22052320e-01,\n",
              "         3.87307146e-01, -5.45340485e-01, -1.01385115e+00,\n",
              "        -2.73626628e-01, -6.67618363e-01, -3.33713506e-01,\n",
              "        -3.30970545e-01, -1.05552128e+00, -1.16554466e+00,\n",
              "         7.92773556e-01, -6.04756035e-03, -1.36103435e-01,\n",
              "         2.84618093e-01, -7.68000271e-01],\n",
              "       [ 1.87054098e+00,  1.34674226e+00, -1.02135859e+00,\n",
              "        -6.88962901e-01,  1.41898628e+00, -7.67253969e-01,\n",
              "        -2.64847015e-01, -5.26713015e-01, -3.29415595e-01,\n",
              "        -3.30965430e-01, -1.76967570e+00,  1.59330175e+00,\n",
              "        -1.24929440e+00, -4.09476984e-01,  2.95159541e-01,\n",
              "         2.24992687e+00, -7.63153541e-01],\n",
              "       [ 1.40230428e+00,  4.89008981e-01, -1.59088213e+00,\n",
              "        -7.77936324e-01,  6.37226628e-01, -8.76852715e-01,\n",
              "        -2.27818116e-01, -3.15371130e-01, -3.33772401e-01,\n",
              "        -3.30969539e-01, -9.21826930e-01,  2.84393304e+00,\n",
              "        -1.70848419e+00, -1.46089724e-01,  1.24885248e+00,\n",
              "         1.45114145e+00, -9.68926187e-01],\n",
              "       [ 1.49637582e+00,  1.54770319e+00, -6.67501968e-01,\n",
              "        -1.28154597e+00, -5.45678923e-01, -1.06865052e+00,\n",
              "         5.59983970e-01, -8.66709024e-01, -3.31610893e-01,\n",
              "        -3.30972296e-01, -1.64687168e+00, -5.07742521e-01,\n",
              "         1.05796673e-01,  6.81620219e-01,  1.91473680e-01,\n",
              "         5.11821617e+00, -6.81890737e-01],\n",
              "       [ 1.00150735e+00, -8.58201787e-01,  3.11646964e-01,\n",
              "        -1.43442573e+00, -5.49929348e-01, -9.04252402e-01,\n",
              "        -5.28211743e-01, -1.20002808e-01, -3.35769175e-01,\n",
              "        -3.30973760e-01, -4.42456637e-01,  2.35041057e+00,\n",
              "        -1.82929991e+00,  2.68994980e-01,  2.63286133e+00,\n",
              "         1.29973450e+00, -1.07303619e+00],\n",
              "       [-6.27982153e-01,  1.82083041e+00,  6.89179021e-01,\n",
              "         1.26719711e+00, -5.43758936e-01, -1.64460866e-01,\n",
              "        -6.68394416e-02,  9.78698647e-01,  6.80756634e-01,\n",
              "         2.73073776e+00,  1.92168550e+00, -1.03524748e+00,\n",
              "         5.49325368e-01,  4.74349821e-01, -4.82848243e-01,\n",
              "        -8.08082795e-01,  9.01514579e-01],\n",
              "       [ 5.22267532e-01, -1.06875098e-01, -6.47157410e-01,\n",
              "        -2.81002626e-02, -5.47216346e-01,  5.47366256e-02,\n",
              "        -9.22810177e-01,  2.97018333e-01, -1.85535435e-01,\n",
              "        -3.30751288e-01, -1.01051841e+00,  5.33572761e-01,\n",
              "        -8.41591880e-01, -1.03863818e+00, -5.91370564e-01,\n",
              "        -4.78529164e-01,  7.65523623e-02],\n",
              "       [ 2.31051081e-01, -3.20246036e-01,  3.33606515e-01,\n",
              "        -1.92448965e+00, -5.47077623e-01, -1.39470679e+00,\n",
              "         1.71658188e-01, -8.49813810e-01, -3.35016786e-01,\n",
              "        -3.30973339e-01, -1.13057908e+00, -3.96647241e-02,\n",
              "        -7.76402925e-02, -1.46431122e-01,  1.57365298e+00,\n",
              "         2.06583169e+00, -9.92270817e-01],\n",
              "       [-6.44453936e-01, -8.78723589e-03, -1.04418142e-01,\n",
              "        -5.22952367e-01,  9.09313099e-01,  1.69871781e+00,\n",
              "        -1.21474566e+00,  1.75610763e+00, -9.44599440e-02,\n",
              "        -3.30471019e-01,  8.05783791e-01, -1.11896137e+00,\n",
              "        -5.71419421e-01, -1.13904175e+00,  2.36554102e-01,\n",
              "        -7.98738149e-01, -5.48945706e-01],\n",
              "       [-3.48340628e-01, -2.04547197e+00, -1.03467134e+00,\n",
              "        -1.55390505e-02, -5.46627799e-01, -1.15084958e+00,\n",
              "        -6.69297508e-02, -7.37879216e-01, -3.34679763e-01,\n",
              "        -3.30971699e-01, -1.03184069e+00,  1.50423112e+00,\n",
              "        -6.09364543e-01,  1.28156402e-01,  5.77786575e-02,\n",
              "         1.87632640e+00, -7.99203206e-01],\n",
              "       [ 1.31428729e+00,  8.33731470e-01, -2.06133494e+00,\n",
              "        -4.94935111e-01, -1.48787803e-01, -4.93257104e-01,\n",
              "        -1.11052146e+00,  2.96631799e-01, -3.34734762e-01,\n",
              "        -3.30973644e-01, -1.88126819e+00,  8.14095169e-01,\n",
              "        -1.86868564e+00, -9.97315097e-01,  1.51500876e+00,\n",
              "         6.33359739e-01, -1.04880671e+00],\n",
              "       [-7.78919476e-02, -8.97044120e-03,  6.51523027e-01,\n",
              "         7.49297997e-01, -5.27596185e-01, -5.20656791e-01,\n",
              "        -4.28916598e-02,  4.08624768e-01,  1.37001203e+00,\n",
              "         4.65521584e+00,  7.28554740e-01, -6.10275581e-01,\n",
              "         4.10570346e-01,  3.16088294e-01, -7.78599488e-01,\n",
              "        -7.78277706e-01,  2.46846217e+00],\n",
              "       [-1.07862168e+00, -9.07261519e-01, -1.76684217e+00,\n",
              "         4.54004496e-01, -5.45792945e-01, -8.22053342e-01,\n",
              "        -1.15501373e-01, -6.66255332e-01, -3.34572715e-01,\n",
              "        -3.30971879e-01, -1.27247672e+00,  1.63479878e+00,\n",
              "        -7.17735476e-01,  3.80034857e-01, -3.52185277e-01,\n",
              "         2.09857952e+00, -5.85517130e-01],\n",
              "       [-4.10778722e-01, -1.47731624e+00,  2.03066111e-02,\n",
              "         5.14359698e-01, -5.42388460e-01,  9.04126907e-01,\n",
              "         1.61247582e-01, -6.87811940e-01, -3.23281141e-01,\n",
              "        -3.30938728e-01, -8.53130201e-03, -2.23899349e-02,\n",
              "         4.65265450e-01, -1.25718098e-01, -5.02362406e-01,\n",
              "         3.93224819e-01, -3.54460457e-01],\n",
              "       [-2.67691522e+00, -7.83949702e-01, -4.35930764e-01,\n",
              "         7.82858235e-01, -5.43538425e-01, -3.56258672e-01,\n",
              "        -2.29288143e-01, -4.04234755e-01, -3.23171479e-01,\n",
              "        -3.30952033e-01, -6.84554256e-01,  1.72157557e-02,\n",
              "         1.25295199e-01, -5.44093976e-01, -6.52698092e-01,\n",
              "         2.90087082e-01, -1.25754879e-01],\n",
              "       [ 8.66821394e-01, -3.81661127e-01, -9.31529767e-01,\n",
              "        -1.08735653e-01,  2.68419262e+00, -8.22053342e-01,\n",
              "        -1.16075046e+00,  4.28848179e-01, -2.98252121e-01,\n",
              "        -3.30895891e-01, -5.72195095e-01,  2.93311022e-01,\n",
              "        -9.78331074e-01, -7.76096481e-01, -3.14273460e-01,\n",
              "        -3.22049443e-01, -5.37955363e-01],\n",
              "       [ 7.23052291e-02,  1.04327058e+00, -6.85581854e-01,\n",
              "         3.47382948e-01,  2.82555274e+00, -8.22053342e-01,\n",
              "        -8.49521291e-01, -1.35127259e-01, -3.32460424e-01,\n",
              "        -3.30967487e-01, -9.50662606e-01, -1.34840990e-01,\n",
              "        -4.28277955e-01, -5.22481418e-01, -6.28998767e-02,\n",
              "         3.20908629e-01, -7.80744615e-01],\n",
              "       [ 1.20793562e+00, -1.03223723e+00, -1.62396721e+00,\n",
              "         3.11100130e-01,  1.49808593e+00, -9.86451461e-01,\n",
              "        -4.48263664e-01, -2.91367343e-01, -3.31854994e-01,\n",
              "        -3.30968638e-01, -1.24663188e+00,  2.52799032e-01,\n",
              "        -6.54119703e-01, -4.47802506e-01,  1.69383467e-01,\n",
              "         1.04489784e+00, -8.04946099e-01],\n",
              "       [-6.87842207e-01,  1.72331016e+00,  9.61258968e-01,\n",
              "         5.87565237e-01, -5.46222606e-01, -1.64460866e-01,\n",
              "         7.23101620e-01,  7.19138408e-01, -1.35740067e-01,\n",
              "        -3.30621865e-01,  9.72960680e-01,  5.02376472e-01,\n",
              "        -3.50762360e-01, -6.61316119e-01,  2.15088506e-01,\n",
              "        -5.02291020e-01, -5.16875211e-01],\n",
              "       [-7.06431308e-01, -9.89748699e-03,  1.32801751e+00,\n",
              "        -1.37618450e+00, -5.39625628e-01,  6.60326170e+00,\n",
              "         3.83648904e+00, -1.69002919e+00, -2.97148140e-01,\n",
              "        -3.30858604e-01,  2.98396859e+00, -8.38653288e-01,\n",
              "         2.67323855e+00,  1.97331940e+00,  4.59022522e-01,\n",
              "        -5.16599725e-01, -7.19356677e-01],\n",
              "       [ 2.06934501e+00,  1.21207305e+00, -8.02529761e-01,\n",
              "        -3.23665850e+00,  3.52435672e-01, -1.01385115e+00,\n",
              "         5.82901521e-02, -7.18648977e-01, -3.35432838e-01,\n",
              "        -3.30974284e-01, -1.66414662e+00,  2.66798240e+00,\n",
              "        -1.79401751e+00,  3.67356718e-01,  2.40487403e+00,\n",
              "         4.02913212e+00, -1.04661333e+00],\n",
              "       [-4.81158272e-02, -2.32919315e-01,  1.22501708e+00,\n",
              "        -8.12476213e-01, -4.97813037e-01,  1.01372565e+00,\n",
              "         1.33700890e+00, -1.28646438e+00, -2.95623368e-01,\n",
              "        -3.30844390e-01, -3.13005653e-01, -3.33040465e-01,\n",
              "         1.52391557e+00,  1.11048723e+00, -7.64915655e-01,\n",
              "         4.28233562e-01,  4.28892001e-01],\n",
              "       [-9.82392012e-01, -1.50981574e-01,  4.83117627e-01,\n",
              "         2.22916324e-01, -5.37588700e-01, -5.48621204e-02,\n",
              "        -3.23635973e-01,  2.80315082e-01,  1.13406449e-01,\n",
              "        -3.29875677e-01,  8.18070701e-01, -1.43941597e+00,\n",
              "         1.04414111e+00,  1.63780813e-01, -7.28423285e-01,\n",
              "        -7.65482987e-01,  8.05402450e-01],\n",
              "       [ 6.97724794e-01,  1.63023754e-01, -1.75187371e+00,\n",
              "         1.14426911e+00, -5.48805576e-01,  8.21363121e-02,\n",
              "        -1.28708397e+00,  8.40567159e-01, -2.89064255e-01,\n",
              "        -3.30913130e-01, -1.78303534e-01, -1.94079236e+00,\n",
              "         1.47162088e-01, -7.67067466e-01, -4.77411270e-01,\n",
              "        -6.59369742e-01, -4.29973239e-01],\n",
              "       [ 1.47188959e+00,  5.62595026e-01, -1.49271155e+00,\n",
              "         1.02886770e+00,  6.05581723e-01, -3.56258672e-01,\n",
              "        -9.01440816e-01,  4.88482086e-01, -3.20638346e-01,\n",
              "        -3.30944186e-01, -3.05015407e-01,  4.24598068e-01,\n",
              "        -1.03575902e+00, -6.13459514e-01,  7.86519903e-01,\n",
              "        -3.65165869e-01, -8.72051850e-01],\n",
              "       [ 1.78305420e+00,  1.37711411e+00,  1.49916132e-01,\n",
              "        -5.15432158e-01, -5.46664013e-01, -1.17824927e+00,\n",
              "        -7.14148531e-01, -3.26597763e-01, -3.31802117e-01,\n",
              "        -3.30964935e-01, -1.10153207e+00, -3.25076660e-01,\n",
              "        -2.76899548e-01, -4.81278689e-01,  1.63951096e-01,\n",
              "         7.64810335e-01, -8.13461099e-01],\n",
              "       [ 1.33916856e+00,  1.74729366e+00,  9.32576683e-01,\n",
              "        -3.92248540e-01, -5.42436939e-01, -5.48056477e-01,\n",
              "         5.42853861e-01, -3.37319011e-01,  1.32217300e-01,\n",
              "        -3.30650739e-01,  9.93278185e-02,  2.18568209e+00,\n",
              "        -7.52734919e-01, -5.58159325e-01, -2.94368551e-01,\n",
              "         3.00978986e-04, -1.51329794e-01],\n",
              "       [-4.34428134e-01,  3.02004017e-01,  1.01503799e-01,\n",
              "        -7.25101197e-01, -5.28519067e-01,  8.21927847e-01,\n",
              "         1.45567081e+00, -7.68295455e-01,  4.14797781e-01,\n",
              "         2.25528874e+00,  5.75813151e-01,  6.77986198e-01,\n",
              "         7.49259558e-01, -2.17678153e-01, -7.81576726e-01,\n",
              "        -6.63814520e-01,  1.22155484e+00],\n",
              "       [-1.31924424e+00, -2.13873130e+00, -1.64219371e+00,\n",
              "         5.92161706e-01,  3.05030254e-01, -1.09661493e-01,\n",
              "        -8.13304283e-01,  4.59418668e-01, -2.47396709e-01,\n",
              "        -3.30904876e-01, -6.48364725e-01,  2.66347433e-01,\n",
              "        -5.20524304e-01, -8.21052616e-01,  4.23155858e-01,\n",
              "        -7.44445162e-01, -7.34242330e-01],\n",
              "       [ 4.98694935e-01,  7.35612793e-01,  6.35586964e-01,\n",
              "         6.54250049e-01,  3.16485533e-02, -1.64460866e-01,\n",
              "        -3.77661612e-01,  2.09671116e+00, -2.80318175e-01,\n",
              "        -3.30830305e-01,  6.43998628e-01,  5.19144703e-01,\n",
              "        -1.77373780e+00, -1.05512656e+00,  3.58224772e+00,\n",
              "        -7.01535374e-01, -9.40195936e-01],\n",
              "       [-5.53790101e-01,  1.03819108e+00,  1.37825865e+00,\n",
              "        -5.27827103e-01, -4.81328114e-01,  9.58926280e-01,\n",
              "         1.12437799e+00, -1.09948055e+00, -1.42972054e-01,\n",
              "        -3.30380716e-01,  1.64834270e-01, -1.08663847e+00,\n",
              "         1.87716536e+00,  3.95541488e+00, -6.54081800e-01,\n",
              "        -5.41593407e-01,  9.48322491e-01],\n",
              "       [-1.26926479e+00, -6.36680117e-01,  1.01970124e+00,\n",
              "        -5.33978697e-01, -5.33738171e-01,  9.86325966e-01,\n",
              "         1.25522324e+00, -6.93947595e-01, -1.94759811e-01,\n",
              "        -3.30667183e-01,  1.30284625e+00, -2.03162186e+00,\n",
              "         2.27840513e+00,  4.24096956e+00,  6.55272926e-01,\n",
              "        -7.58507969e-01, -2.33948455e-01],\n",
              "       [-3.20926151e-02, -6.03988430e-01, -7.78588951e-01,\n",
              "         6.46497882e-01,  1.92052260e+00,  5.75330669e-01,\n",
              "         1.94163568e-01,  1.55176070e-01, -2.84214659e-01,\n",
              "        -3.30810446e-01,  1.22361023e+00, -4.23498024e-02,\n",
              "         1.00554220e-01, -3.74807906e-01,  4.69542622e-01,\n",
              "        -6.39069582e-01, -7.50674863e-01],\n",
              "       [-1.04351660e+00, -1.81410010e+00,  3.47721555e-01,\n",
              "        -1.29945878e-02, -5.39025604e-01, -4.11058045e-01,\n",
              "        -3.10854978e-01, -4.68380791e-01, -2.99799844e-01,\n",
              "        -3.30884312e-01, -6.04430429e-01,  3.95954001e-01,\n",
              "        -2.75961036e-02, -3.55011848e-01, -6.40261061e-01,\n",
              "         1.25434692e-01,  1.69221243e-03],\n",
              "       [-1.70779933e+00, -1.82177506e+00,  6.30653686e-01,\n",
              "         7.72815870e-02, -5.41667522e-01, -7.94653656e-01,\n",
              "        -2.36091860e-01, -5.49058455e-01, -3.32324336e-01,\n",
              "        -3.30964704e-01, -2.78998649e-01,  7.31967045e-01,\n",
              "        -5.20673201e-02,  1.47627164e+00, -2.60634812e-01,\n",
              "         1.91408748e-01, -6.63483974e-01],\n",
              "       [-2.61293931e-01, -8.16561598e-01,  1.04338706e+00,\n",
              "        -1.15650705e+00, -5.41614180e-01,  2.60290747e+00,\n",
              "         1.30285401e+00, -7.54923071e-01, -2.82469107e-01,\n",
              "        -3.30793928e-01,  5.28076978e-01, -1.69820568e-01,\n",
              "         8.39421745e-01, -4.61928261e-01, -2.21762703e-01,\n",
              "        -2.37855717e-01, -5.14552169e-01],\n",
              "       [ 8.62558431e-01,  1.62239571e+00,  3.56990345e-01,\n",
              "        -2.80327055e-01, -5.41526545e-01,  9.04126907e-01,\n",
              "         7.81323904e-01, -7.68937274e-01, -1.92343026e-01,\n",
              "        -3.30479367e-01,  3.40655060e-01,  9.32793053e-01,\n",
              "         3.24131351e-01, -4.79406591e-01, -4.16664194e-01,\n",
              "        -3.06035404e-01, -1.41367216e-01],\n",
              "       [ 1.09128720e-01, -1.44962712e+00, -1.13746835e+00,\n",
              "        -1.59265282e-02,  2.15276655e+00, -2.19260239e-01,\n",
              "        -2.57819423e-01, -6.92733551e-01, -3.35814248e-01,\n",
              "        -3.30974439e-01, -1.20398783e+00,  9.16355465e-01,\n",
              "        -4.47333051e-01,  1.32357085e-01,  2.86341634e-01,\n",
              "         1.47032779e+00, -8.80778234e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WoqA4rT14-Q",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2b45577f-66b1-4a2a-e218-749851736276",
        "id": "YEyjhsg414-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9506f676-6bb8-4d0c-c679-77b019836d3a",
        "id": "9WylvzXG14-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-fac1faddc5b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_acc_histories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'all_acc_histories' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw9GTwCC14-f",
        "colab": {}
      },
      "source": [
        "average_acc_history_reduced = [np.mean([x[i] for x in all_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_reduced = [np.mean([x[i] for x in all_loss_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_acc_history_reduced = [np.mean([x[i] for x in all_val_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_loss_history_reduced = [np.mean([x[i] for x in all_val_loss_histories_reduced]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b028cf0b-2fc4-4822-c5ff-65dbbacd5b9d",
        "id": "MdMC3doS14-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "len(average_val_acc_history_reduced)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-b9e814837513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_val_acc_history_reduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'average_val_acc_history_reduced' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "outputId": "2748b766-151d-469e-9c4d-5006c93e3329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 10)                180       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrcg4Bx625fC"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9_gKwYk25fK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCcaAgTi25fZ",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "62713074-5a8d-4653-9a58-ad6b37bcaf78",
        "id": "6H8nOl_X25fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_reduced, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_reduced, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f69388296d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3dd3gU5fbA8e9JCCJFulKlKCq9BUSR\njtJBqjQRLAgWLIiCPwVFvdeCXi4qKEpRVBS5iCAKSgcFqdJ7k9BFCL2f3x/vJISYBmQzSfZ8nmcf\ndmdnZ8/Ohjn7dlFVjDHGBK8QvwMwxhjjL0sExhgT5CwRGGNMkLNEYIwxQc4SgTHGBDlLBMYYE+Qs\nEZhEichPIvJAcu/rJxHZLiL1A3BcFZGbvfsficjLSdn3Ct6nk4j8fKVxJnDc2iISkdzHTeD94j0H\nItJVROanVCzBLIPfAZjAEJFjMR5mBk4D573Hj6rql0k9lqo2CsS+6Z2q9kiO44hIUWAbEKaq57xj\nfwkk+Ts0JiGWCNIpVc0adV9EtgMPq+r02PuJSIaoi4sxJjhZ1VCQiSr6i8gLIrIXGCUiOUXkBxE5\nICKHvPuFYrxmtog87N3vKiLzRWSQt+82EWl0hfsWE5G5InJURKaLyIci8kU8cSclxtdE5FfveD+L\nSJ4Yz98vIjtE5KCI/F8C5+d2EdkrIqExtrUUkZXe/aoiskBEDovIHhH5QEQyxnOs0SLyeozHfbzX\n7BaRB2Pt20RElovIERHZKSKvxHh6rvfvYRE5JiJ3xK42EZE7RWSxiER6/96Z1HOTEBEp6b3+sIis\nEZHmMZ5rLCJrvWPuEpHnvO15vO/nsIj8LSLzRCTRa42I5BaRSd45WATcFOv5/3rn5oiILBWRGkn5\nDCZxlgiCUz4gF1AE6I77OxjlPb4ROAl8kMDrbwc2AHmAt4ERIiJXsO9XwCIgN/AKcH8C75mUGDsC\n3YDrgYxA1IWpFDDMO34B7/0KEQdV/R04DtSNddyvvPvngWe8z3MHUA94LIG48WJo6MVzN1ACiN0+\ncRzoAuQAmgA9ReRe77ma3r85VDWrqi6IdexcwBRgiPfZ3gOmiEjuWJ/hH+cmkZjDgMnAz97rngS+\nFJFbvV1G4KoZswFlgJne9t5ABJAXuAF4EUjKXDYfAqeA/MCD3i2mxUAF3N/uV8C3IpIpCcc1ibBE\nEJwuAANU9bSqnlTVg6r6P1U9oapHgTeAWgm8foeqfqKq54HPcP9xb7icfUXkRqAK0F9Vz6jqfGBS\nfG+YxBhHqepGVT0JjMNdNADaAD+o6lxVPQ287J2D+IwFOgCISDagsbcNVV2qqgtV9Zyqbgc+jiOO\nuLTz4lutqsdxiS/m55utqqtU9YKqrvTeLynHBZc4NqnqGC+uscB6oFmMfeI7NwmpBmQF3vS+o5nA\nD3jnBjgLlBKR61T1kKoui7E9P1BEVc+q6jxNZFIzrwTWGvf3cFxVV+P+XqKp6hfe38E5VX0XuAa4\nNY7DmctkiSA4HVDVU1EPRCSziHzsVZ0cwVVF5IhZPRLL3qg7qnrCu5v1MvctAPwdYxvAzvgCTmKM\ne2PcPxEjpgIxj+1diA/G9164X5utROQaoBWwTFV3eHHc4lV77PXi+BeudJCYS2IAdsT6fLeLyCyv\n6isS6JHE40Yde0esbTuAgjEex3duEo1ZVWMmzZjHbY1LkjtEZI6I3OFtfwfYDPwsIltFpG8S3isv\nrs0yoXP0nIis86q/DgPZSfo5MgmwRBCcYv866437ZXW7ql7HxaqI+Kp7ksMeIJeIZI6xrXAC+19N\njHtiHtt7z9zx7ayqa3EXoUZcWi0EroppPVDCi+PFK4kBV70V01e4ElFhVc0OfBTjuIlVq+zGVZnF\ndCOwKwlxJXbcwrHq96OPq6qLVbUFrtpoIq6kgaoeVdXeqlocaA48KyL1EnmvA8A54jlHXnvA87iS\nVU5VzQFEEti/0aBhicAAZMPVuR/26psHBPoNvV/YS4BXRCSj92uyWQIvuZoYxwNNReQur2F3IIn/\n7X8FPIVLON/GiuMIcExEbgN6JjGGcUBXESnlJaLY8WfDlZBOiUhVXAKKcgBXlVU8nmP/CNwiIh1F\nJIOI3AeUwlXjXI3fcaWH50UkTERq476jr73vrJOIZFfVs7hzcgFARJqKyM1eW1Akrl0loao4vKrD\nCbi/h8xeu07M8SjZcIniAJBBRPoD113l5zMeSwQGYDBwLfAXsBCYmkLv2wnX4HoQeB34BjfeIS5X\nHKOqrgEex13c9wCHcI2ZCYmqo5+pqn/F2P4c7iJ9FPjEizkpMfzkfYaZuGqTmbF2eQwYKCJHgf54\nv669157AtYn86vXEqRbr2AeBprhS00HcL+emseK+bKp6Bnfhb4Q770OBLqq63tvlfmC7V0XWA/d9\ngmsMnw4cAxYAQ1V1VhLe8glcldVeYDSuc0CUabjvfCOutHaKBKoSzeURW5jGpBYi8g2wXlUDXiIx\nxlxkJQLjGxGpIiI3iUiI172yBa6u2RiTgmxksfFTPly9cG5cVU1PVV3ub0jGBB+rGjLGmCBnVUPG\nGBPk0lzVUJ48ebRo0aJ+h2GMMWnK0qVL/1LVvHE9l+YSQdGiRVmyZInfYRhjTJoiIrFHn0ezqiFj\njAlylgiMMSbIWSIwxpggl+baCIwxKe/s2bNERERw6tSpxHc2vsqUKROFChUiLCwsya+xRGCMSVRE\nRATZsmWjaNGixL8GkfGbqnLw4EEiIiIoVqxYkl9nVUPGmESdOnWK3LlzWxJI5USE3LlzX3bJzRKB\nMSZJLAmkDVfyPQVPIti4EZ5+Gs6e9TsSY4xJVYInEWzaBP/9L4wbl/i+xphU5fDhwwwdOvSKXtu4\ncWMOHz6c4D79+/dn+vTpV3T82IoWLcpff13VUhApLngSQaNGULIkDBoENtGeMWlKQong3LlzCb72\nxx9/JEeOHAnuM3DgQOrXr3/F8aV1wZMIQkKgd2/44w+YlZTFkowxqUXfvn3ZsmULFSpUoE+fPsye\nPZsaNWrQvHlzSpUqBcC9995L5cqVKV26NMOHD49+bdQv9O3bt1OyZEkeeeQRSpcuzT333MPJkycB\n6Nq1K+PHj4/ef8CAAVSqVImyZcuyfr1bkO3AgQPcfffdlC5dmocffpgiRYok+sv/vffeo0yZMpQp\nU4bBgwcDcPz4cZo0aUL58uUpU6YM33zzTfRnLFWqFOXKleO5555L3hOYiODqPtqpE7z4oisV1K3r\ndzTGpElPP+1+TyWnChXAu07G6c0332T16tX84b3x7NmzWbZsGatXr47uJjly5Ehy5crFyZMnqVKl\nCq1btyZ37tyXHGfTpk2MHTuWTz75hHbt2vG///2Pzp07/+P98uTJw7Jlyxg6dCiDBg3i008/5dVX\nX6Vu3br069ePqVOnMmLEiAQ/09KlSxk1ahS///47qsrtt99OrVq12Lp1KwUKFGDKlCkAREZGcvDg\nQb777jvWr1+PiCRalZXcgqdEAJApEzz5JPz0E6xZ43c0xpirULVq1Uv6yg8ZMoTy5ctTrVo1du7c\nyaZNm/7xmmLFilGhQgUAKleuzPbt2+M8dqtWrf6xz/z582nfvj0ADRs2JGfOnAnGN3/+fFq2bEmW\nLFnImjUrrVq1Yt68eZQtW5ZffvmFF154gXnz5pE9e3ayZ89OpkyZeOihh5gwYQKZM2e+3NNxVYKr\nRADQsyf861/w3nuQSEY3xvxTQr/cU1KWLFmi78+ePZvp06ezYMECMmfOTO3atePsS3/NNddE3w8N\nDY2uGopvv9DQ0ETbIC7XLbfcwrJly/jxxx956aWXqFevHv3792fRokXMmDGD8ePH88EHHzBz5sxk\nfd+EBFeJACB3bujWDb74Avbu9TsaY0wSZMuWjaNHj8b7fGRkJDlz5iRz5sysX7+ehQsXJnsM1atX\nZ5zX6/Dnn3/m0KFDCe5fo0YNJk6cyIkTJzh+/DjfffcdNWrUYPfu3WTOnJnOnTvTp08fli1bxrFj\nx4iMjKRx48b85z//YcWKFckef0KCr0QA8MwzMGwYfPABvP6639EYYxKRO3duqlevTpkyZWjUqBFN\nmjS55PmGDRvy0UcfUbJkSW699VaqVauW7DEMGDCADh06MGbMGO644w7y5ctHtmzZ4t2/UqVKdO3a\nlapVqwLw8MMPU7FiRaZNm0afPn0ICQkhLCyMYcOGcfToUVq0aMGpU6dQVd57771kjz8haW7N4vDw\ncE2WhWlatYLZs2HnTohRxDTG/NO6desoWbKk32H46vTp04SGhpIhQwYWLFhAz549oxuvU5u4vi8R\nWaqq4XHtH5wlAoDnnoPvvoPRo+Hxx/2OxhiTyv3555+0a9eOCxcukDFjRj755BO/Q0o2wZsI7rwT\n7rjDdSXt3h0uY8pWY0zwKVGiBMuXL/c7jIAImsbiffvgzTfhwoUYG196CbZvhzFj/ArLGGN8FzSJ\nYNYs6NcPvv02xsZGjaByZXjjDUjmLmLGGJNWBCwRiMhIEdkvIqvjeb6TiKwUkVUi8puIlA9ULABt\n20KpUvDqq3D+fHQQ0L8/bN0KX30VyLc3xphUK5AlgtFAwwSe3wbUUtWywGvA8AT2vWqhoTBgAKxb\nF2sC0mbN3Pj211+3UoExJigFLBGo6lzg7wSe/01Vo0ZkLAQKBSqWKG3aQJkyMHBgHKWCTZvAm/zJ\nGJP2Zc2aFYDdu3fTpk2bOPepXbs2iXVHHzx4MCdOnIh+nJRprZPilVdeYdCgQVd9nOSQWtoIHgJ+\nCvSbhIS4UsH69fD11zGeaNECypZ1GcJKBcakKwUKFIieWfRKxE4ESZnWOq3xPRGISB1cInghgX26\ni8gSEVly4MCBq3q/Vq2gXLlY1/yQENd4sHGj9SAyJhXq27cvH374YfTjqF/Tx44do169etFTRn//\n/ff/eO327dspU6YMACdPnqR9+/aULFmSli1bXjLXUM+ePQkPD6d06dIMGDAAcBPZ7d69mzp16lCn\nTh3g0oVn4ppmOqHpruPzxx9/UK1aNcqVK0fLli2jp68YMmRI9NTUURPezZkzhwoVKlChQgUqVqyY\n4NQbSaaqAbsBRYHVCTxfDtgC3JLUY1auXFmv1oQJqqA6cmSMjRcuqIaHqxYponrq1FW/hzHpydq1\nay8+eOop1Vq1kvf21FMJvv+yZcu0Zs2a0Y9Lliypf/75p549e1YjIyNVVfXAgQN600036YULF1RV\nNUuWLKqqum3bNi1durSqqr777rvarVs3VVVdsWKFhoaG6uLFi1VV9eDBg6qqeu7cOa1Vq5auWLFC\nVVWLFCmiBw4ciH7vqMdLlizRMmXK6LFjx/To0aNaqlQpXbZsmW7btk1DQ0N1+fLlqqratm1bHTNm\nzD8+04ABA/Sdd95RVdWyZcvq7NmzVVX15Zdf1qe885E/f3495V2PDh06pKqqTZs21fnz56uq6tGj\nR/Xs2bP/OPYl35cHWKLxXFd9KxGIyI3ABOB+Vd2Yku99771QpYqrJoqeoFDEzUq6YwekoxGDxqQH\nFStWZP/+/ezevZsVK1aQM2dOChcujKry4osvUq5cOerXr8+uXbvYt29fvMeZO3du9PoD5cqVo1y5\nctHPjRs3jkqVKlGxYkXWrFnD2rVrE4wpvmmmIenTXYObMO/w4cPUqlULgAceeIC5c+dGx9ipUye+\n+OILMmRw43+rV6/Os88+y5AhQzh8+HD09qsRsJHFIjIWqA3kEZEIYAAQBqCqHwH9gdzAUBEBOKfx\nzIOR/LG5wWX16sHQofDss94T9etDrVquB1G3bjYHkTFx8Wke6rZt2zJ+/Hj27t3LfffdB8CXX37J\ngQMHWLp0KWFhYRQtWjTO6acTs23bNgYNGsTixYvJmTMnXbt2vaLjREnqdNeJmTJlCnPnzmXy5Mm8\n8cYbrFq1ir59+9KkSRN+/PFHqlevzrRp07jtttuuOFYIbK+hDqqaX1XDVLWQqo5Q1Y+8JICqPqyq\nOVW1gndLkSQQpW5duOceVwiIjPQ2irjBZfv2uZlJjTGpxn333cfXX3/N+PHjadu2LeB+TV9//fWE\nhYUxa9YsduzYkeAxatasyVfemKHVq1ezcuVKAI4cOUKWLFnInj07+/bt46efLvZdiW8K7Pimmb5c\n2bNnJ2fOnNGliTFjxlCrVi0uXLjAzp07qVOnDm+99RaRkZEcO3aMLVu2ULZsWV544QWqVKkSvZTm\n1QjeuYaAf//bDSweNAhee83bWL06NGniigyPPAK5cvkaozHGKV26NEePHqVgwYLkz58fgE6dOtGs\nWTPKli1LeHh4or+Me/bsSbdu3ShZsiQlS5akcuXKAJQvX56KFSty2223UbhwYapXrx79mu7du9Ow\nYUMKFCjArBjrncc3zXRC1UDx+eyzz+jRowcnTpygePHijBo1ivPnz9O5c2ciIyNRVXr16kWOHDl4\n+eWXmTVrFiEhIZQuXZpGjRpd9vvFFrzTUHvat4fJk2HLFsiXz9u4apUbZPb00/Duu8n2XsakVTYN\nddpyudNQ+9591G+vvw5nz8LLL8fYWLYsdO3qqoe2bfMrNGOMSRFBnwhuvtmtZz9iBFyyxsTAgW5e\nihdf9C02Y4xJCUGfCMCVBnLlcjVB0TVlBQtC795uCPKiRb7GZ0xqkNaqkYPVlXxPlgiAHDlcY/Gc\nOTBxYownnn8err/eJQT7T2CCWKZMmTh48KAlg1ROVTl48CCZMmW6rNcFfWNxlHPnXPvwyZOwdi1E\ndwMePhwefRTGjnUty8YEobNnzxIREXFVfetNysiUKROFChUiLNaqiwk1FlsiiOGXX9zYgjfeiNE0\ncP48VK3qxhasXw/ejIbGGJOWWK+hJLr7bjcp3WuvxegsFBoK778Pu3a50WfGGJPOWCKIZfBgd+1/\n8skYzQJ33gn33+/GFGze7Gt8xhiT3CwRxFK4sJuResqUWA3Hb73lGg4u6VpkjDFpnyWCOPTq5dYs\n6NULjh3zNubP76YrnTIFJkzwNT5jjElOlgjiEBYGw4ZBRAS88kqMJ556ynUtevJJSIal6owxJjWw\nRBCPO++Ehx92bQbeBIWQIQN8+qnrQdSvn6/xGWNMcrFEkIA334ScOaFHjxiL3Veu7NoJPvoI5s/3\nNT5jjEkOlggSkDu36yi0YAEMGRLjiVdfhSJFoHt3OH3at/iMMSY5WCJIxP33u+UJXnzRrW0PuEFl\nw4bBunWu2GCMMWmYJYJEiLhZJjJlcqtXRlcRNWoEHTq4QWbr1vkaozHGXA1LBElQoICrGvrtt1jL\ntf7nP25d4wcfdJMVGWNMGmSJIIk6d4ZmzeCll2DDBm/jDTe4xWsWLoR33vE1PmOMuVKWCJJIBD7+\nGK69NlYVUYcO0KaNG2y2YoWvMRpjzJWwRHAZ8ud3888tWOAWvAdchhg2zK1s06ULnDnja4zGGHO5\nLBFcpo4dXQHgpZfg99+9jXnywCefuJFnlwxFNsaY1M8SwWWK6kVUsKBbpyYy0nuiWTPXaPzmmzBv\nnq8xGmPM5bBEcAVy5nQLlu3c6caURU9GOngwFCvmBh9EZwhjjEndLBFcoTvucAvYjBsHI0Z4G7Nl\ngy++cBmiVy9f4zPGmKSyRHAVXngB6td31/y1a72Nd9zhGhA+/9xlCWOMSeUsEVyFkBAYM8YVBNq3\nhxMnvCdefhluvx0eeQS2bPE1RmOMSYwlgquUL5/78b96NfTs6bUXZMgAX3/t1rxs2xZOnfI7TGOM\niZclgmTQoIHrNfr55252agCKFnUbli+HZ57xMTpjjEmYJYJk8tJL0LixW8Rs4UJvY9Om8PzzLjt8\n9ZWv8RljTHwsESSTkBDXYahwYTfgbO9e74k33oAaNVw/0/XrfY3RGGPiYokgGeXM6da1P3QIWrb0\nmgYyZHCDDjJndhni+HG/wzTGmEtYIkhm5cu7poGFC2MMNitYEL780q1b0LUrXLjgd5jGGBPNEkEA\ntG4NAwe6rqVvv+1tvPtu92D8ePekMcakEhn8DiC9euklN8isXz8oWRKaNweefRbWrHFrHpcqBe3a\n+R2mMcZYiSBQRGDkSKhc2c1YunIlF6esrl7dVREtXep3mMYYE7hEICIjRWS/iKyO53kRkSEisllE\nVopIpUDF4pdrr4Xvv4fs2V1P0ogI4JprXIty3rzQogXs2eN3mMaYIBfIEsFooGECzzcCSni37sCw\nAMbimwIF4Icf4PBht9794cPA9dfDpEnuwb33wsmTfodpjAliAUsEqjoX+DuBXVoAn6uzEMghIvkD\nFY+fKlZ0hYANG9x1/9QpXPeiL76ARYvcnETRc1kbY0zK8rONoCCwM8bjCG/bP4hIdxFZIiJLDhw4\nkCLBJbf69WH0aJgzx61oeeECLiu88YbrWvraa36HaIwJUmmi15CqDgeGA4SHh6fZn84dO8Lu3dCn\nj1v/ePBgkH79YONGGDAAChVyq5wZY0wK8jMR7AIKx3hcyNuWrvXuDbt2uSSQJw+8/LK49Y737HEj\n0PLnd40JxhiTQvysGpoEdPF6D1UDIlU13XehEYF333XVQ/37w1tvAWFhbqBZuXJu2uolS/wO0xgT\nRAJWIhCRsUBtII+IRAADgDAAVf0I+BFoDGwGTgDdAhVLahMS4sYYnD0LfftCxozwzDPZ4Mcf3Qpn\nTZrAggVQvLjfoRpjgkDAEoGqdkjkeQUeD9T7p3ahoW5OojNn3IDjjBnh8cfzwdSpcOedbpGDefPc\nyjfGGBNANrLYR1ETkzZvDk884ZoKuPVWmDLFtRncfTccPOh3mMaYdM4Sgc/Cwtwa940awaOPwmef\nAdWquQFnmza5kkFkpN9hGmPSMUsEqUDUrBP16kG3bm6cGXXruo0rV7qlz44d8ztMY0w6ZYkglciU\nyc1LVKeO61E0ahQuAXz1lVvcoEULm4rCGBMQlghSkcyZYfJk1zTw4IPw8ce4Vc1Gj4ZZs9z9M2f8\nDtMYk86kiZHFwSRzZlcyaNMGevRwhYCnn74fTpxwGzp2hK+/di3NxhiTDOxqkgplyuSaBzp2hGee\ncR2HBg58FDl+3A1Nbt/eVRllzOh3qMaYdMASQSqVMSN8840rBLz+uksG77//LKEibuBBmzbw7beu\npdkYY66CJYJULDQUhg+H3LndVBR//w2ff/4MGa+5Bh5/3M1eOmGCWwHHGGOukCWCVE4E3nzTJYPn\nn3dr2Ywf/xhZM2Z0k9Q1a+YaFbJk8TtUY0waZb2G0og+fWDECJg+HWrVgr1NH77Ym6hBA2/pM2OM\nuXyWCNKQBx90A47Xr3eDj9dV6eJ6EC1aBDVr2vrHxpgrYokgjWnc2K1yduoUVK8Oc29o62Yt3brV\nbdi82e8QjTFpjCWCNCg83M1SfcMNbvDZ2AP1XRXRkSMuGSxf7neIxpg0xBJBGlWsGPz6q6si6tgR\nXvq+ChfmznfdSWvXhtmz/Q7RGJNGWCJIw3Llgl9+gYcegjfegFYv3saxn3+DggWhYUP47ju/QzTG\npAGWCNK4jBndOgZDhsAPP0C1NoXYPmYeVKwIrVu7J4wxJgGWCNIBEXjySbe42e7dUPme3MzpP8PN\nWPrUU26eivPn/Q7TGJNKWSJIR+rXdz1Jb7gB6jXLzLB6410iGDwY2rZ1E9cZY0wslgjSmZtvdssX\nNGwIjz0ZSo9Tgzk3aDBMnOgWO9i3z+8QjTGpjCWCdOi669ysE337ujUNav7vKf765DtYtQqqVIFl\ny/wO0RiTilgiSKdCQ+Hf/3brIa9aBaX6tWDRe/Pdk9Wrw9ix/gZojEk1LBGkc23bwuLFkDcv3PF4\nJQZ3XoJWqeIGH/Tta43IxpikJQIRySIiId79W0SkuYiEBTY0k1xuuw1+/x3uuw+e+ff1tMw6nZNd\ne7i5rZs1swnrjAlySS0RzAUyiUhB4GfgfmB0oIIyyS9rVvjyS3j/ffhpRkZu/nkYG54e5kak3X67\nm8nOGBOUkpoIRFVPAK2AoaraFigduLBMIIjAE0+4XkVZs0LJ//bgkw4z0UOHXDKYMsXvEI0xPkhy\nIhCRO4BOQNTVIjQwIZlAq1gRli6Fbt2g+5gatCq8hNOFb3LVRAMGWLuBMUEmqYngaaAf8J2qrhGR\n4sCswIVlAi1rVrfQzVdfwczNN1J053y217gfBg50c13/9ZffIRpjUkiSEoGqzlHV5qr6ltdo/Jeq\n9gpwbCYFdOjgZq0uUjIzxeaO5vO7hqNz5rhiw8KFfodnjEkBSe019JWIXCciWYDVwFoR6RPY0ExK\nKV4c5s2Dvn2Frr8+QusbfuXU+Qxu1bP33wdVv0M0xgRQUquGSqnqEeBe4CegGK7nkEknwsLcALTp\n02GJVqbQvmVsKNoAevVyYw6OHPE7RGNMgCQ1EYR54wbuBSap6lnAfiamQ3XrwsqV0KRzTkpu+p4P\nCryBjhsHlSq5kWnGmHQnqYngY2A7kAWYKyJFAPuJmE7lyAGffQbfjg/hldMvUi90DkcOnkHvvBPe\neQcuXPA7RGNMMkpqY/EQVS2oqo3V2QHUCXBsxmetW8Pq1ZClwV0UObyCuTlawPPPu6lN9+71Ozxj\nTDJJamNxdhF5T0SWeLd3caUDk87lyweTJsGgT3PS/PS3PBH2MWdnz0fLlYOffvI7PGNMMkhq1dBI\n4CjQzrsdAUYFKiiTuoi4dZHXrhP+bNid8meXsPlYPjfeoHdvOH3a7xCNMVchqYngJlUdoKpbvdur\nQPFABmZSn4IF3ToH/ceWom7m3xka8ji89x4Xbq8Ga9b4HZ4x5golNRGcFJG7oh6ISHXgZGIvEpGG\nIrJBRDaLSN84nr9RRGaJyHIRWSkijZMeuvGDCLRvD8vWXcuv7T+gOd9zaPUuLlSqDO+9Zw3JxqRB\nSU0EPYAPRWS7iGwHPgAeTegFIhIKfAg0AkoBHUSkVKzdXgLGqWpFoD0w9DJiNz7Km9fNZvrIpObU\nzbuaH840gN69OVerLuzY4Xd4xpjLkNReQytUtTxQDijnXbjrJvKyqsBmryrpDPA10CL2oYHrvPvZ\ngd1JjtykCs2awdz11zO1x0QeZCQnf13G2ZJl0VGjbUSyMWnEZa1QpqpHvBHGAM8msntBYGeMxxHe\ntpheATqLSATwI/BkXAcSkTvAqGsAABpSSURBVO5RPZYOHDhwOSGbFJA9OwwdJnRf0I12t67kt5MV\nkQe7caJRa7Dvy5hU72qWqpRkeP8OwGhVLQQ0BsZErYQWk6oOV9VwVQ3PmzdvMrytCYRq1WDyqqIs\nfXsmL4a9Q+i0KRwvXoZzEyb5HZoxJgFXkwgSK/fvAgrHeFzI2xbTQ8A4AFVdAGQC8lxFTMZnGTLA\ns31C6bH5OZ6rvZRNx/KToXUL9jV7CCIj/Q7PGBOHBBOBiBwVkSNx3I4CBRI59mKghIgUE5GMuMbg\n2D8N/wTqee9VEpcIrC4hHbjxRnh/Vhn+/HYRH2TrR54fRnMofyn+/myy36EZY2JJMBGoajZVvS6O\nWzZVzZDIa88BTwDTgHW43kFrRGSgiDT3dusNPCIiK4CxQFdVa2FMT5q3yUjX3f9i2AO/E3EqN7m6\nNmdtxY6c2WX53pjUQtLadTc8PFyXLFnidxjmCmxac4ZFrd6k7cbXORaanT97D6HCm+3d4ARjTECJ\nyFJVDY/ruatpIzDmspQonZFOG/qz8MNl7MxQnApvd+T3/C3YNj9205ExJiVZIjApruZjZSh56Ddm\nNRlE2X3TyVmjNBMaDefYERuVbIwfLBEYX2S8NpQ6P/Tm2K8r2ZOvIq2mPsr6PHfxw79X2Tg0Y1KY\nJQLjq+vvvJmSu2ey+aXR3KSbaPhiRb4s+DwLZxz3OzRjgoYlAuM/EW5+7QGy717Plhpd6bznHfLX\nL8Wb1SezZYvfwRmT/lkiMKlGSN7c3Dr3U05Mncu1ebPS97fmrCrRilcf3snff/sdnTHplyUCk+pk\nblCD6yOWc6Tfv2kUMpVnR5RiUMH/8J93ztkaOMYEgCUCkzplzMh1/+rLNZvWwF01+NepZ6n9fBXu\nK76IceNsYlNjkpMlApO6FStGtrlT4NtvKZl7PxN2V+PQfY/SoPJfzJzpd3DGpA+WCEzqJwJt2pBp\n6zro9RSPhIxg3B+38L96H9Kg3jkWL/Y7QGPSNksEJu247jpC/vsfQlauIFutinzIE7w7pzK9q86l\ndWtYt87vAI1JmywRmLSndGlCZ06Hb7+lVP5DzKUW7Sd15J7Su+jWDbZv9ztAY9IWSwQmbfKqi0I2\nrIeXX6ZN6AQ2Z7iVgmPepGyJU/TqBfv2+R2kMWmDJQKTtmXODAMHImvXck3j+rx+vh/bMt3GwQ/G\nUryY8vzzsH+/30Eak7pZIjDpQ/HiMHEiTJ9Onptz8qV2ZEXmaiwY9CvFisFzz1kJwZj4WCIw6Uu9\nerBkCYwaxc3XRDBP72JWnjZ8/94WihWD3r1h716/gzQmdbFEYNKf0FDo2hU2boRXX6XqwalszFCS\nCUWfZfR/DlGsGDzzDOzZ43egxqQOlghM+pUlC/TvD5s2IV260HD9YPZfdxMjyg7m4yGnKV4cnn7a\nEoIxlghM+pc/P3z6KfzxB6FVw+m4+BkiC9zGkCpjGPr+eYoVgyeegG3b/A7UGH9YIjDBo1w5mDYN\npk0j7PpcPDKvC8dursA7NScz/GOlRAno2BGWL/c7UGNSliUCE1xE4J57YPFi+OYbMuppnvylOUcr\n3MWQtvP44QeoVAkaNIAZM2xyOxMcLBGY4BQSAu3awZo1MHw41+zezmNf1+RgtSaM7PUHK1dC/fpQ\npQqMGwfnzvkdsDGBY4nABLewMHjkEdi8Gd5+m7AlC+g2pCIR1e9j3MD1HD0K990Ht94KQ4fCyZN+\nB2xM8rNEYAzAtddCnz6wdSu89BKh036k7SulWXf7A0wdupW8eeHxx6FIEXjtNWzFNJOuWCIwJqYc\nOdyVfutWeOYZQr4dR4Net7KgfA8WfBtB1aquR+qNN7qupzt2+B2wMVfPEoExccmbFwYNgi1boHt3\nZNRIqnW+mR9KPMPaWfto3Ro+/BBuugnuvx9WrvQ7YGOunCUCYxJSoIC74m/cCJ06wZAhlGxUlM9y\nPMX2X3fRqxd89x2ULw+NG8Ps2dbTyKQ9lgiMSYqiRWHECFi/Hjp0gKFDKVijOO+d7Mmu33bw+utu\niqM6deD22+F//4Pz5/0O2piksURgzOUoUQJGjoRNm6BbNxgxguyVb+b/tj7EnzM3M2yYa0hu0wZu\nuw0+/hhOnfI7aGMSZonAmCtRtCh89JFrVO7ZE776ikzlb6XHr/ezYeI6xo2D7NmhRw+367/+BQcP\n+h20MXGzRGDM1ShUCIYMcRMVPfMMTJhAaLnStP22HYtHrGTmTKhQAf7v/9yujzxiDcsm9bFEYExy\nyJfP9TLavh369oWpU5EK5akzpCVT31jKqlWud9GXX7qG5Tp1XCOztSOY1MASgTHJKW9eVw+0fTsM\nGOC6EYWHU+b5xgzvtoCdO+Gtt1yNUqtWrvvpoEFw6JDfgZtgZonAmEDIlQteecUlhDfegEWL4M47\nyX1ffZ6/fQ5btrieRUWLugHNhQq59oS1a32O2wQlSwTGBFL27PDiiy4hvPMOrF4NtWuToW5NWmX9\nmdmzlD/+gPbtYfRoKF0a7r4bJk+2aiOTciwRGJMSsmaF555zjcr//a+rG2rQACpWpPyqLxjx0Vki\nIlyt0rp10Ly5qzZ6803Yv9/v4E16Z4nAmJR07bXQq5ebumLkSDh71rUiFy9Ons/epd/jR9i2zU19\nXbw49Ovnqo06dYL5823UsgmMgCYCEWkoIhtEZLOI9I1nn3YislZE1ojIV4GMx5hU45pr3IC0Vavg\nhx/g5ptdiaFwYcL+73na3rmLmTNdm0HPnjBlCtSo4RZZGzYMjh71+wOY9CRgiUBEQoEPgUZAKaCD\niJSKtU8JoB9QXVVLA08HKh5jUqWQEGjSBGbNcqumNWoE777rWpEfeICS51bx3//Crl3wySdu+YTH\nHnNTIPXsaWMSTPIIZImgKrBZVbeq6hnga6BFrH0eAT5U1UMAqmq1oSZ4hYfD11+7RXIeewzGj3dF\ngEaNyLJwBg8/pCxdCgsXQuvWMGqUG5Nw113w2Wdw/LjfH8CkVYFMBAWBnTEeR3jbYroFuEVEfhWR\nhSLSMK4DiUh3EVkiIksOHDgQoHCNSSWKFXMNyjt3uq6ny5e7dTMrVUJGj+L28qcYPdqVEgYNco3J\nXbteLCUsWWJtCeby+N1YnAEoAdQGOgCfiEiO2Dup6nBVDVfV8Lx586ZwiMb4JFeui11PP/3UNSw/\n+KBrPe7Xj9zH/6R3b9iwAebMgRYtXBfUKlWgYkX44AMbqGaSJpCJYBdQOMbjQt62mCKASap6VlW3\nARtxicEYEyVTJnjoIdewPHMm1KwJb7/tSg6tWyNzZlOzhvL557Bnj1tbOTQUnnwS8ud3PY5mzYIL\nF/z+ICa1CmQiWAyUEJFiIpIRaA9MirXPRFxpABHJg6sq2hrAmIxJu0TcJEUTJrhxCH36uCks6tRx\njQXDh5Mj7Dg9e8LSpbBsGTz8sOtxVLcu3HIL/PvfsHu33x/EpDYBSwSqeg54ApgGrAPGqeoaERko\nIs293aYBB0VkLTAL6KOqNlmvMYkpUsSNNouIcAvmhIbCo4+6aqPnnoOtW6Orh/bsgTFjoHBhV9N0\n441uwNr338OZM35/EJMaiKaxVqXw8HBdsmSJ32EYk7qowq+/wvvvu0mMLlxw3VIfewzuucclCtx6\nOiNHuraEvXshTx43vUWXLq7Tkoi/H8MEjogsVdXwOJ+zRGBMOrNrl1s0Z/hw16WoSBG3EMKDD7pG\nA+DcOfj5Z/j8c5g4EU6fdiuqdekCnTu70oNJXywRGBOMzpxxV/nhw2HGDMiQwdUJPfqo644a4mqG\nDx+Gb791SWH+/ItNEV26uKmys2Xz+XOYZGGJwJhgt2mTG5o8ahT89ZebyOiRR9w0FzfcEL3b1q3w\nxRcuKWzZApkzu2TQpYtrcPZqmEwaZInAGOOcPu2WRvv4Y9fjKCwM7r0Xund3V3qvlKAKCxa4hPDN\nN67UUKCAqzbq0sVNl23SFksExph/2rDBVRuNHg1//+3aErp1c8OUixSJ3u3UKTcv3uefw08/ufaF\nSpVcQmjf/pIChUnFLBEYY+J36pRrSxg5EqZPd9vq1XNJoWVLN3W2Z/9+Nx3S55+7sQohIW7XDh3c\nrjn+MS+ASS0sERhjkmbHDjeD3ahRbmqL7NmhY0fX46hy5Uv6l65ZA2PHutvWrZAxo5s8tUMHaNbM\ntS+Y1MMSgTHm8ly44NoQRo504xJOnYKyZV1C6NQJYsz5pepm0B471rUn7NkDWbK4DkodOriF2DJm\n9O+jGMcSgTHmyh0+7OqDRo50V/ywMPeT/8EH3VU+Q4boXc+fh3nzXFIYP941PeTI4abN7tABate2\nnkd+sURgjEkeq1e7aqPPP3fdUPPnd63GXbpAqUvWneLMGdfkMHasa4I4dgzy5YN27Vwjc7VqNpI5\nJVkiMMYkrzNn3Gx2I0fCjz+6qqSKFV3/0g4dokcwRzlxwu02dqx72enTbhG2Nm3crWpVSwqBZonA\nGBM4e/e6qqMvv3Sr4kR1Jerc2XUlijU0OTLSTXj39deuxHD2rJvSonVrlxTuuCN6OINJRpYIjDEp\nY/16lxC++ML1Orr2WrdiTufObvK7sLBLdj98GCZPdu0J06a5kkL+/BeTwl13WZtCcrFEYIxJWarw\n228uIYwb51qNo6Y67dw5zrqgI0dctdH48a4a6dQpuP56N8VFmzZQq9Yl7dLmMlkiMMb458wZmDrV\nJYVJk9zP/ptvdt1Q77sPSpb8x0uOHXOjmMePd6OaT5yA3Lldl9QWLeDuu22cwuWyRGCMSR0iI90K\na1984dbPVHXjE9q1c7dbbvnHS06ccNVG48e7EkNkpKtxatDATZPUtKlLEiZhlgiMManP7t1usNq4\ncW7+a4AKFS4mhZtu+sdLzp6FOXNcd9SJE93SCyEhbhnne+91pYWiRVP2Y6QVlgiMMalbRIT7yT9u\nnJv2FNyUFlFJIY6ru6pblzkqKaxe7baXL++Swr33uvvWLdWxRGCMSTt27LiYFBYtctuqVnUJoW1b\nt+hyHLZscd1SJ050BQxV1y21SRN3q1s3uNsVLBEYY9Kmbdvc8mnjxrnpTsENNGjXzvUxjWdNzf37\nXSPzDz/AL7+4xudMmdzKa02busQQY6btoGCJwBiT9m3e7JLCN9/AihVuW3i4G7TWqpVbdDkOp0+7\n+Y9++ME1Nm/e7LaXLn0xKdxxR/rvmmqJwBiTvmzc6FZa++47+P13t+2221xCaNnyH1Nmx35pVFKY\nO9cttJMzJzRs6JJCw4bpsxeSJQJjTPq1a5drGPjuOzd19vnzrsqoZUt3u+uueH/uR0a6qqMpU9wg\ntv37XS+katVcUmjQwE2hlB6mvLBEYIwJDgcPup/7EybAzz+74cm5c7t+pS1auDmQsmSJ86UXLrip\nkqZMcbeoJom8ed3sGA0auH/T6tKclgiMMcHn2DE3Em3CBJccjhxxLcZ1615sHIinBxLAvn2utDB1\nqsspBw647RUruuqjBg1c20JaWXTHEoExJridOeNajCdPdretW9328uVdUmja1HVRjacO6MIFWL7c\n5ZVp09w0SufOQdasrpDRoIFLDsWKpeBnukyWCIwxJooqbNjgSgmTJ8Ovv7p2hbx5XSmhaVM3mdF1\n18V7iCNHYOZMlxSmTnUTrQKUKOFeWr++W40tZ84U+URJYonAGGPi8/ff7oo+ebKb6e7wYTdddu3a\nF0sLxYvH+3JV2LTpYlKYMweOH3eFi0qVXImhXj3XZn3ttSn3sWKzRGCMMUlx7pyr95k82ZUY1q93\n22+7DRo1cvU/NWu6toZ4nDnjBkTPmOFuCxa4w15zDdx558XEEB6esmMXLBEYY8yV2LzZJYSffnI/\n9U+fdj/ra9d2SaFhQ1cflMCERseOueaJqMTwxx9u+3XXucNEJYZSpQI7L5IlAmOMuVonTrhkMHWq\nu23c6LYXK3YxKdSt61qQE3DggJuBOyoxbNnitufL515er577N7lnUbVEYIwxyW3r1osNAzNmuIaB\nsDDXGBCVGMqWTfRn/vbtF5PCjBluUBu4uZBq1754u9rEYInAGGMC6fRp17YQVVpYudJtz5fPdSGq\nX9/91C9UKMHDqLrptOfMcYOkZ892Y+TAJYY+feDxx68sREsExhiTknbtcqWF6dPdLWo02m23XUwM\ntWtD9uwJHubCBVi79mJSaNoUuna9spAsERhjjF8uXHA/86OSwpw5rr0hNBSqVHFJ4e673QRHARym\nbInAGGNSizNnYOHCi4lh0SI3oC1zZtc1tU4dd6tUySWLZGKJwBhjUqvISFdK+OUXN1x57Vq3/brr\noFati4mhXLmrmgY1oUQQ0MlVRaShiGwQkc0i0jeB/VqLiIpInEEaY0y6lT07NG8O778Pa9bAnj0w\ndiy0b+8GtD37rJvpLm9eePfdgIQQsHFtIhIKfAjcDUQAi0VkkqqujbVfNuAp4PdAxWKMMWlGvnwu\nCbRv7x5HRLiBB7NmQcGCAXnLQA5wrgpsVtWtACLyNdACWBtrv9eAt4A+AYzFGGPSpkKF4P773S1A\nAlk1VBDYGeNxhLctmohUAgqr6pSEDiQi3UVkiYgsORDVDcsYY0yy8G0BNhEJAd4Deie2r6oOV9Vw\nVQ3Pmzdv4IMzxpggEshEsAsoHONxIW9blGxAGWC2iGwHqgGTrMHYGGNSViATwWKghIgUE5GMQHtg\nUtSTqhqpqnlUtaiqFgUWAs1V1fqGGmNMCgpYIlDVc8ATwDRgHTBOVdeIyEARaR6o9zXGGHN5Aros\ngqr+CPwYa1v/ePatHchYjDHGxM23xmJjjDGpgyUCY4wJcmluriEROQDsuIKX5gH+SuZwkoPFdflS\na2wW1+VJrXFB6o3tauIqoqpx9r9Pc4ngSonIkvgmXPKTxXX5UmtsFtflSa1xQeqNLVBxWdWQMcYE\nOUsExhgT5IIpEQz3O4B4WFyXL7XGZnFdntQaF6Te2AISV9C0ERhjjIlbMJUIjDHGxMESgTHGBLl0\nnwiSulxmCsVSWERmichaEVkjIk95218RkV0i8od3a+xDbNtFZJX3/ku8bblE5BcR2eT9mzOFY7o1\nxjn5Q0SOiMjTfp0vERkpIvtFZHWMbXGeI3GGeH93K721N1IyrndEZL333t+JSA5ve1ERORnj3H2U\nwnHF+92JSD/vfG0QkQYpHNc3MWLaLiJ/eNtT8nzFd30I/N+YqqbbGxAKbAGKAxmBFUApH+PJD1Ty\n7mcDNgKlgFeA53w+V9uBPLG2vQ309e73Bd7y+bvcCxTx63wBNYFKwOrEzhHQGPgJENwU67+ncFz3\nABm8+2/FiKtozP18OF9xfnfe/4MVwDVAMe//bWhKxRXr+XeB/j6cr/iuDwH/G0vvJYLo5TJV9QwQ\ntVymL1R1j6ou8+4fxc3KGphFSJNHC+Az7/5nwL0+xlIP2KKqVzKqPFmo6lzg71ib4ztHLYDP1VkI\n5BCR/CkVl6r+rG4GYHBTvBcKxHtfblwJaAF8raqnVXUbsBn3/zdF4xIRAdoBYwPx3glJ4PoQ8L+x\n9J4IEl0u0y8iUhSoCPzubXrCK96NTOkqGI8CP4vIUhHp7m27QVX3ePf3Ajf4EFeU9lz6n9Pv8xUl\nvnOUmv72HsT9coxSTESWi8gcEanhQzxxfXep5XzVAPap6qYY21L8fMW6PgT8byy9J4JUSUSyAv8D\nnlbVI8Aw4CagArAHVzRNaXepaiWgEfC4iNSM+aS6sqgvfY3FLWzUHPjW25Qaztc/+HmO4iMi/wec\nA770Nu0BblTVisCzwFcicl0KhpQqv7sYOnDpD44UP19xXB+iBepvLL0ngsSWy0xxIhKG+5K/VNUJ\nAKq6T1XPq+oF4BMCVCROiKru8v7dD3znxbAvqqjp/bs/pePyNAKWqeo+L0bfz1cM8Z0j3//2RKQr\n0BTo5F1A8KpeDnr3l+Lq4m9JqZgS+O5Sw/nKALQCvonaltLnK67rAynwN5beE0GCy2WmNK/+cQSw\nTlXfi7E9Zr1eS2B17NcGOK4sIpIt6j6uoXE17lw94O32APB9SsYVwyW/0vw+X7HEd44mAV28nh3V\ngMgYxfuAE5GGwPO45V9PxNieV0RCvfvFgRLA1hSMK77vbhLQXkSuEZFiXlyLUiouT31gvapGRG1I\nyfMV3/WBlPgbS4nWcD9vuJb1jbhM/n8+x3IXrli3EvjDuzUGxgCrvO2TgPwpHFdxXI+NFcCaqPME\n5AZmAJuA6UAuH85ZFuAgkD3GNl/OFy4Z7QHO4upjH4rvHOF6cnzo/d2tAsJTOK7NuPrjqL+zj7x9\nW3vf8R/AMqBZCscV73cH/J93vjYAjVIyLm/7aKBHrH1T8nzFd30I+N+YTTFhjDFBLr1XDRljjEmE\nJQJjjAlylgiMMSbIWSIwxpggZ4nAGGOCnCUCYzwicl4une002War9Wax9HO8gzHxyuB3AMakIidV\ntYLfQRiT0qxEYEwivPnp3xa3XsMiEbnZ215URGZ6E6jNEJEbve03iFsDYIV3u9M7VKiIfOLNNf+z\niFzr7d/Lm4N+pYh87dPHNEHMEoExF10bq2rovhjPRapqWeADYLC37X3gM1Uth5vUbYi3fQgwR1XL\n4+a9X+NtLwF8qKqlgcO4Uavg5piv6B2nR6A+nDHxsZHFxnhE5JiqZo1j+3agrqpu9SYF26uquUXk\nL9wUCWe97XtUNY+IHAAKqerpGMcoCvyiqiW8xy8AYar6uohMBY4BE4GJqnoswB/VmEtYicCYpNF4\n7l+O0zHun+diG10T3JwxlYDF3iyYxqQYSwTGJM19Mf5d4N3/DTejLUAnYJ53fwbQE0BEQkUke3wH\nFZEQoLCqzgJeALID/yiVGBNI9svDmIuuFW/Rcs9UVY3qQppTRFbiftV38LY9CYwSkT7AAaCbt/0p\nYLiIPIT75d8TN9tlXEKBL7xkIcAQVT2cbJ/ImCSwNgJjEuG1EYSr6l9+x2JMIFjVkDHGBDkrERhj\nTJCzEoExxgQ5SwTGGBPkLBEYY0yQs0RgjDFBzhKBMcYEuf8HERfcPUMenjEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMh2BG152967"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "019d92c7-1ad4-43bd-f2f6-9b97a0014517",
        "id": "WNEq_n08297J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_reduced, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_reduced, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f69387c8d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5BQnjMi93Dgn"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c98b7599-8470-4e7f-8c8b-0ed5ad365c31",
        "id": "yANP0XPF3Dg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_reduced, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_reduced, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 1.2255 - acc: 0.3130\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.1124 - acc: 0.4122\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0401 - acc: 0.4885\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9896 - acc: 0.5496\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9585 - acc: 0.5802\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9361 - acc: 0.6031\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9200 - acc: 0.6260\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8996 - acc: 0.6183\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8865 - acc: 0.6260\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8760 - acc: 0.6260\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8648 - acc: 0.6336\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8563 - acc: 0.6565\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8488 - acc: 0.6565\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.8386 - acc: 0.6565\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8345 - acc: 0.6565\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8299 - acc: 0.6794\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8245 - acc: 0.6794\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8177 - acc: 0.6794\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8139 - acc: 0.6870\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8103 - acc: 0.6718\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 0s 264us/step - loss: 0.8041 - acc: 0.6718\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.7974 - acc: 0.6794\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.7946 - acc: 0.6718\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.7916 - acc: 0.6794\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.7870 - acc: 0.6641\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.7822 - acc: 0.6947\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.7811 - acc: 0.6641\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.7750 - acc: 0.7023\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.7717 - acc: 0.6870\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.7671 - acc: 0.6870\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.7666 - acc: 0.6947\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7610 - acc: 0.6870\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7597 - acc: 0.7099\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7538 - acc: 0.7023\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.7499 - acc: 0.7099\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7472 - acc: 0.6947\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7445 - acc: 0.7023\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.7427 - acc: 0.6947\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7378 - acc: 0.7023\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.7361 - acc: 0.7023\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.7320 - acc: 0.6794\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.7321 - acc: 0.7099\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.7274 - acc: 0.7176\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.7249 - acc: 0.6870\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7202 - acc: 0.7099\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.7165 - acc: 0.6947\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.7168 - acc: 0.7176\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7140 - acc: 0.7099\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7093 - acc: 0.7099\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7063 - acc: 0.7099\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7037 - acc: 0.7099\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.7022 - acc: 0.7099\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6985 - acc: 0.7099\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.6939 - acc: 0.7328\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6912 - acc: 0.7252\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6871 - acc: 0.7252\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6858 - acc: 0.6947\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.6828 - acc: 0.7328\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 0s 224us/step - loss: 0.6771 - acc: 0.7176\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.6745 - acc: 0.7252\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6709 - acc: 0.7328\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.6680 - acc: 0.7099\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6643 - acc: 0.7099\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6617 - acc: 0.7252\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.6582 - acc: 0.7252\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.6545 - acc: 0.7252\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.6523 - acc: 0.7252\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6498 - acc: 0.7481\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.6457 - acc: 0.7405\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6455 - acc: 0.7557\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.6399 - acc: 0.7481\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.6355 - acc: 0.7405\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6332 - acc: 0.7405\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.6328 - acc: 0.7557\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6268 - acc: 0.7634\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6275 - acc: 0.7481\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6213 - acc: 0.7557\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6197 - acc: 0.7481\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.6150 - acc: 0.7481\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.6146 - acc: 0.7328\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.6099 - acc: 0.7405\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6074 - acc: 0.7557\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.6054 - acc: 0.7634\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6030 - acc: 0.7481\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.6015 - acc: 0.7634\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5978 - acc: 0.7634\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5959 - acc: 0.7710\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5908 - acc: 0.7557\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5888 - acc: 0.7786\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5887 - acc: 0.7710\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.5848 - acc: 0.7634\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.5814 - acc: 0.7710\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.5792 - acc: 0.7863\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5781 - acc: 0.7710\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5750 - acc: 0.7939\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5727 - acc: 0.7939\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5713 - acc: 0.7786\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.5707 - acc: 0.7939\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5663 - acc: 0.7786\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5657 - acc: 0.7710\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.5618 - acc: 0.8015\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.5589 - acc: 0.7939\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.5560 - acc: 0.7939\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5554 - acc: 0.7939\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.5516 - acc: 0.8015\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5502 - acc: 0.7939\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5473 - acc: 0.7863\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.5450 - acc: 0.7939\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.5426 - acc: 0.8015\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.5417 - acc: 0.8015\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.5370 - acc: 0.8015\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.5391 - acc: 0.8092\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5320 - acc: 0.8092\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5328 - acc: 0.8015\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.5290 - acc: 0.7939\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5270 - acc: 0.8015\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5240 - acc: 0.8015\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5227 - acc: 0.8168\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.5196 - acc: 0.8092\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5182 - acc: 0.7939\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5140 - acc: 0.8015\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5143 - acc: 0.8092\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5101 - acc: 0.8092\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.5094 - acc: 0.7939\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.5061 - acc: 0.8092\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5035 - acc: 0.8244\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4995 - acc: 0.8244\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5017 - acc: 0.8244\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4974 - acc: 0.8168\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4959 - acc: 0.8244\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.4941 - acc: 0.8168\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.4916 - acc: 0.8092\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4889 - acc: 0.8244\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4881 - acc: 0.8092\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4848 - acc: 0.8015\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.4847 - acc: 0.8168\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4811 - acc: 0.8168\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.4782 - acc: 0.8092\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4781 - acc: 0.8244\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.4728 - acc: 0.8092\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.4748 - acc: 0.8092\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4724 - acc: 0.8092\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4681 - acc: 0.8168\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4672 - acc: 0.7939\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.4646 - acc: 0.8168\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4643 - acc: 0.8168\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4629 - acc: 0.8168\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.4610 - acc: 0.8244\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4577 - acc: 0.8244\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.4558 - acc: 0.8092\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 0s 277us/step - loss: 0.4552 - acc: 0.8244\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4539 - acc: 0.8244\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4503 - acc: 0.8168\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.4489 - acc: 0.8168\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4451 - acc: 0.8321\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4495 - acc: 0.8397\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4421 - acc: 0.8321\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4428 - acc: 0.8168\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4394 - acc: 0.8321\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4371 - acc: 0.8321\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4357 - acc: 0.8168\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4338 - acc: 0.8244\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4320 - acc: 0.8321\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.4322 - acc: 0.8244\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4297 - acc: 0.8321\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.4268 - acc: 0.8321\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4260 - acc: 0.8397\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4249 - acc: 0.8321\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4233 - acc: 0.8244\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4229 - acc: 0.8321\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4202 - acc: 0.8321\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.4180 - acc: 0.8168\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4152 - acc: 0.8244\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4151 - acc: 0.8321\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4135 - acc: 0.8321\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4113 - acc: 0.8244\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4107 - acc: 0.8321\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4084 - acc: 0.8473\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4061 - acc: 0.8397\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.4055 - acc: 0.8321\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.4020 - acc: 0.8244\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.4014 - acc: 0.8397\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3996 - acc: 0.8321\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3966 - acc: 0.8626\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.3967 - acc: 0.8550\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.3949 - acc: 0.8550\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3905 - acc: 0.8702\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3906 - acc: 0.8550\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.3912 - acc: 0.8702\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3880 - acc: 0.8550\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 0s 257us/step - loss: 0.3862 - acc: 0.8779\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 0s 236us/step - loss: 0.3857 - acc: 0.8473\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3827 - acc: 0.8702\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3875 - acc: 0.8626\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.3826 - acc: 0.8550\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3808 - acc: 0.8779\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.3798 - acc: 0.8702\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.3782 - acc: 0.8626\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3762 - acc: 0.8702\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3753 - acc: 0.8702\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5e05c29-c9fa-4dac-fd13-ba29c13c7d1b",
        "id": "kQLgWQtS3DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "30cfd131-b234-466d-b914-b989b79198c6",
        "id": "_c5TGT4H3DhR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2647058823529412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMbKL5Yw3DhZ"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viwAabDN6yW9",
        "colab_type": "text"
      },
      "source": [
        "#Using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nicqs7CG65iH",
        "colab_type": "code",
        "outputId": "2f0f18ce-0b5f-4588-9153-c9069ec7f14a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Collecting scipy==1.4.1; python_version >= \"3\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (45.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: -ensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: -ensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, scipy, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: scipy 1.2.0\n",
            "    Uninstalling scipy-1.2.0:\n",
            "      Successfully uninstalled scipy-1.2.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "Successfully installed scipy-1.4.1 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tPhecjM6xuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.wrappers.scikit_learn import KerasRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gihe2no0B6po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s2z0QPE8e5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhmQysgZ7R5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu'))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.005, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aevQ3mxf63mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# wrap the model using the function you created\n",
        "clf = KerasRegressor(build_fn=build_model, epochs=1000, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKhggrQs7lx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps = [('scaler', StandardScaler()), ('red_dim', PCA()), ('clf', clf)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwxH7hpP73EG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fhCwXF0-juS",
        "colab_type": "code",
        "outputId": "df3f4b6f-4e62-4599-bcf8-824dd25bf3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "scores = cross_val_score(pipeline, train_data, one_hot_train_labels, cv=3, scoring='accuracy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "87/87 [==============================] - 0s 532us/step - loss: 1.6561 - accuracy: 0.3333\n",
            "Epoch 2/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 1.4802 - accuracy: 0.3678\n",
            "Epoch 3/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.3504 - accuracy: 0.3908\n",
            "Epoch 4/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 1.2682 - accuracy: 0.4023\n",
            "Epoch 5/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 1.2160 - accuracy: 0.4253\n",
            "Epoch 6/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 1.1765 - accuracy: 0.4943\n",
            "Epoch 7/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.1436 - accuracy: 0.4943\n",
            "Epoch 8/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 1.1167 - accuracy: 0.5057\n",
            "Epoch 9/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 1.0902 - accuracy: 0.5172\n",
            "Epoch 10/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 1.0699 - accuracy: 0.5402\n",
            "Epoch 11/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 1.0477 - accuracy: 0.5402\n",
            "Epoch 12/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.0322 - accuracy: 0.5517\n",
            "Epoch 13/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.0157 - accuracy: 0.5402\n",
            "Epoch 14/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.9988 - accuracy: 0.5517\n",
            "Epoch 15/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.9861 - accuracy: 0.5862\n",
            "Epoch 16/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9729 - accuracy: 0.6092\n",
            "Epoch 17/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.9612 - accuracy: 0.6667\n",
            "Epoch 18/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9522 - accuracy: 0.6552\n",
            "Epoch 19/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.9407 - accuracy: 0.6552\n",
            "Epoch 20/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9316 - accuracy: 0.6552\n",
            "Epoch 21/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.9227 - accuracy: 0.6552\n",
            "Epoch 22/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.9148 - accuracy: 0.6552\n",
            "Epoch 23/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.9066 - accuracy: 0.6552\n",
            "Epoch 24/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9011 - accuracy: 0.6667\n",
            "Epoch 25/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.8911 - accuracy: 0.6782\n",
            "Epoch 26/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.8861 - accuracy: 0.6897\n",
            "Epoch 27/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.8789 - accuracy: 0.6782\n",
            "Epoch 28/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.8714 - accuracy: 0.6667\n",
            "Epoch 29/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.8661 - accuracy: 0.6782\n",
            "Epoch 30/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.8593 - accuracy: 0.6897\n",
            "Epoch 31/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.8531 - accuracy: 0.6897\n",
            "Epoch 32/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.8477 - accuracy: 0.6897\n",
            "Epoch 33/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.8425 - accuracy: 0.6782\n",
            "Epoch 34/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.8376 - accuracy: 0.6897\n",
            "Epoch 35/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.8312 - accuracy: 0.6782\n",
            "Epoch 36/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.8270 - accuracy: 0.6897\n",
            "Epoch 37/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.8214 - accuracy: 0.6897\n",
            "Epoch 38/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.8159 - accuracy: 0.6897\n",
            "Epoch 39/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.8111 - accuracy: 0.7126\n",
            "Epoch 40/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.8064 - accuracy: 0.7126\n",
            "Epoch 41/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.8022 - accuracy: 0.7126\n",
            "Epoch 42/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.7976 - accuracy: 0.7011\n",
            "Epoch 43/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.7952 - accuracy: 0.7241\n",
            "Epoch 44/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.7892 - accuracy: 0.7241\n",
            "Epoch 45/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.7854 - accuracy: 0.7241\n",
            "Epoch 46/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.7814 - accuracy: 0.7241\n",
            "Epoch 47/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.7781 - accuracy: 0.7241\n",
            "Epoch 48/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.7744 - accuracy: 0.7241\n",
            "Epoch 49/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.7713 - accuracy: 0.7241\n",
            "Epoch 50/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.7696 - accuracy: 0.7126\n",
            "Epoch 51/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7646 - accuracy: 0.7241\n",
            "Epoch 52/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7620 - accuracy: 0.7241\n",
            "Epoch 53/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.7577 - accuracy: 0.7241\n",
            "Epoch 54/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.7547 - accuracy: 0.7241\n",
            "Epoch 55/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7513 - accuracy: 0.7241\n",
            "Epoch 56/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.7492 - accuracy: 0.7241\n",
            "Epoch 57/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.7459 - accuracy: 0.7241\n",
            "Epoch 58/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.7438 - accuracy: 0.7241\n",
            "Epoch 59/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.7406 - accuracy: 0.7241\n",
            "Epoch 60/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.7374 - accuracy: 0.7241\n",
            "Epoch 61/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.7357 - accuracy: 0.7241\n",
            "Epoch 62/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7340 - accuracy: 0.7241\n",
            "Epoch 63/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.7305 - accuracy: 0.7241\n",
            "Epoch 64/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.7282 - accuracy: 0.7241\n",
            "Epoch 65/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.7266 - accuracy: 0.7241\n",
            "Epoch 66/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.7233 - accuracy: 0.7241\n",
            "Epoch 67/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7208 - accuracy: 0.7241\n",
            "Epoch 68/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.7186 - accuracy: 0.7241\n",
            "Epoch 69/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7159 - accuracy: 0.7241\n",
            "Epoch 70/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7137 - accuracy: 0.7241\n",
            "Epoch 71/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7120 - accuracy: 0.7241\n",
            "Epoch 72/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.7101 - accuracy: 0.7241\n",
            "Epoch 73/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.7085 - accuracy: 0.7241\n",
            "Epoch 74/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.7060 - accuracy: 0.7241\n",
            "Epoch 75/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.7039 - accuracy: 0.7241\n",
            "Epoch 76/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.7028 - accuracy: 0.7241\n",
            "Epoch 77/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.7007 - accuracy: 0.7126\n",
            "Epoch 78/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.6977 - accuracy: 0.7126\n",
            "Epoch 79/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6961 - accuracy: 0.7126\n",
            "Epoch 80/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6948 - accuracy: 0.7126\n",
            "Epoch 81/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6933 - accuracy: 0.7126\n",
            "Epoch 82/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6920 - accuracy: 0.7126\n",
            "Epoch 83/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6911 - accuracy: 0.7126\n",
            "Epoch 84/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6883 - accuracy: 0.7126\n",
            "Epoch 85/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.6875 - accuracy: 0.7126\n",
            "Epoch 86/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6852 - accuracy: 0.7126\n",
            "Epoch 87/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.6841 - accuracy: 0.7126\n",
            "Epoch 88/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.6828 - accuracy: 0.7126\n",
            "Epoch 89/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.6795 - accuracy: 0.7126\n",
            "Epoch 90/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.6791 - accuracy: 0.7126\n",
            "Epoch 91/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.6767 - accuracy: 0.7126\n",
            "Epoch 92/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6753 - accuracy: 0.7126\n",
            "Epoch 93/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6743 - accuracy: 0.7126\n",
            "Epoch 94/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6726 - accuracy: 0.7126\n",
            "Epoch 95/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6710 - accuracy: 0.7126\n",
            "Epoch 96/1000\n",
            "87/87 [==============================] - 0s 109us/step - loss: 0.6699 - accuracy: 0.7241\n",
            "Epoch 97/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6684 - accuracy: 0.7241\n",
            "Epoch 98/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6668 - accuracy: 0.7241\n",
            "Epoch 99/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6661 - accuracy: 0.7241\n",
            "Epoch 100/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6647 - accuracy: 0.7241\n",
            "Epoch 101/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6628 - accuracy: 0.7241\n",
            "Epoch 102/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6620 - accuracy: 0.7241\n",
            "Epoch 103/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6609 - accuracy: 0.7241\n",
            "Epoch 104/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.6592 - accuracy: 0.7241\n",
            "Epoch 105/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6590 - accuracy: 0.7241\n",
            "Epoch 106/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.6564 - accuracy: 0.7241\n",
            "Epoch 107/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.6547 - accuracy: 0.7356\n",
            "Epoch 108/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.6531 - accuracy: 0.7356\n",
            "Epoch 109/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6521 - accuracy: 0.7356\n",
            "Epoch 110/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6507 - accuracy: 0.7356\n",
            "Epoch 111/1000\n",
            "87/87 [==============================] - 0s 62us/step - loss: 0.6505 - accuracy: 0.7356\n",
            "Epoch 112/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.6481 - accuracy: 0.7356\n",
            "Epoch 113/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6479 - accuracy: 0.7241\n",
            "Epoch 114/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.6461 - accuracy: 0.7356\n",
            "Epoch 115/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6444 - accuracy: 0.7356\n",
            "Epoch 116/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.6444 - accuracy: 0.7241\n",
            "Epoch 117/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.6432 - accuracy: 0.7241\n",
            "Epoch 118/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6409 - accuracy: 0.7356\n",
            "Epoch 119/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.6400 - accuracy: 0.7356\n",
            "Epoch 120/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.6402 - accuracy: 0.7356\n",
            "Epoch 121/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.6375 - accuracy: 0.7356\n",
            "Epoch 122/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6361 - accuracy: 0.7356\n",
            "Epoch 123/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.6359 - accuracy: 0.7471\n",
            "Epoch 124/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.6343 - accuracy: 0.7471\n",
            "Epoch 125/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6342 - accuracy: 0.7356\n",
            "Epoch 126/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6318 - accuracy: 0.7356\n",
            "Epoch 127/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6318 - accuracy: 0.7356\n",
            "Epoch 128/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6298 - accuracy: 0.7356\n",
            "Epoch 129/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6288 - accuracy: 0.7356\n",
            "Epoch 130/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6282 - accuracy: 0.7356\n",
            "Epoch 131/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6269 - accuracy: 0.7241\n",
            "Epoch 132/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.6256 - accuracy: 0.7356\n",
            "Epoch 133/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6254 - accuracy: 0.7356\n",
            "Epoch 134/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.6232 - accuracy: 0.7356\n",
            "Epoch 135/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.6241 - accuracy: 0.7471\n",
            "Epoch 136/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.6217 - accuracy: 0.7356\n",
            "Epoch 137/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.6214 - accuracy: 0.7241\n",
            "Epoch 138/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.6205 - accuracy: 0.7241\n",
            "Epoch 139/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6189 - accuracy: 0.7241\n",
            "Epoch 140/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6180 - accuracy: 0.7356\n",
            "Epoch 141/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.6171 - accuracy: 0.7356\n",
            "Epoch 142/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.6159 - accuracy: 0.7356\n",
            "Epoch 143/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.6160 - accuracy: 0.7241\n",
            "Epoch 144/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6143 - accuracy: 0.7241\n",
            "Epoch 145/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6142 - accuracy: 0.7241\n",
            "Epoch 146/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.6122 - accuracy: 0.7241\n",
            "Epoch 147/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.6117 - accuracy: 0.7241\n",
            "Epoch 148/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.6107 - accuracy: 0.7241\n",
            "Epoch 149/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6096 - accuracy: 0.7241\n",
            "Epoch 150/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.6094 - accuracy: 0.7356\n",
            "Epoch 151/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.6087 - accuracy: 0.7356\n",
            "Epoch 152/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.6073 - accuracy: 0.7241\n",
            "Epoch 153/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6062 - accuracy: 0.7241\n",
            "Epoch 154/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6056 - accuracy: 0.7356\n",
            "Epoch 155/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6045 - accuracy: 0.7356\n",
            "Epoch 156/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.6036 - accuracy: 0.7471\n",
            "Epoch 157/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6048 - accuracy: 0.7241\n",
            "Epoch 158/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6018 - accuracy: 0.7241\n",
            "Epoch 159/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6011 - accuracy: 0.7356\n",
            "Epoch 160/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5999 - accuracy: 0.7241\n",
            "Epoch 161/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6003 - accuracy: 0.7241\n",
            "Epoch 162/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.5987 - accuracy: 0.7241\n",
            "Epoch 163/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5985 - accuracy: 0.7241\n",
            "Epoch 164/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5975 - accuracy: 0.7356\n",
            "Epoch 165/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.5972 - accuracy: 0.7356\n",
            "Epoch 166/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5954 - accuracy: 0.7241\n",
            "Epoch 167/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5947 - accuracy: 0.7356\n",
            "Epoch 168/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5943 - accuracy: 0.7356\n",
            "Epoch 169/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5931 - accuracy: 0.7356\n",
            "Epoch 170/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5922 - accuracy: 0.7356\n",
            "Epoch 171/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5909 - accuracy: 0.7356\n",
            "Epoch 172/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5906 - accuracy: 0.7241\n",
            "Epoch 173/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5896 - accuracy: 0.7241\n",
            "Epoch 174/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5886 - accuracy: 0.7356\n",
            "Epoch 175/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5884 - accuracy: 0.7241\n",
            "Epoch 176/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5865 - accuracy: 0.7471\n",
            "Epoch 177/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5874 - accuracy: 0.7471\n",
            "Epoch 178/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5847 - accuracy: 0.7356\n",
            "Epoch 179/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.5846 - accuracy: 0.7241\n",
            "Epoch 180/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5854 - accuracy: 0.7356\n",
            "Epoch 181/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5829 - accuracy: 0.7241\n",
            "Epoch 182/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5838 - accuracy: 0.7241\n",
            "Epoch 183/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.5810 - accuracy: 0.7241\n",
            "Epoch 184/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.5810 - accuracy: 0.7241\n",
            "Epoch 185/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5791 - accuracy: 0.7241\n",
            "Epoch 186/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5784 - accuracy: 0.7356\n",
            "Epoch 187/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5782 - accuracy: 0.7356\n",
            "Epoch 188/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.5770 - accuracy: 0.7356\n",
            "Epoch 189/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5767 - accuracy: 0.7241\n",
            "Epoch 190/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5772 - accuracy: 0.7241\n",
            "Epoch 191/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5750 - accuracy: 0.7471\n",
            "Epoch 192/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5748 - accuracy: 0.7586\n",
            "Epoch 193/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5752 - accuracy: 0.7241\n",
            "Epoch 194/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5723 - accuracy: 0.7356\n",
            "Epoch 195/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5724 - accuracy: 0.7586\n",
            "Epoch 196/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5707 - accuracy: 0.7586\n",
            "Epoch 197/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5706 - accuracy: 0.7471\n",
            "Epoch 198/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5697 - accuracy: 0.7586\n",
            "Epoch 199/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5691 - accuracy: 0.7471\n",
            "Epoch 200/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.5701 - accuracy: 0.7356\n",
            "Epoch 201/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5686 - accuracy: 0.7356\n",
            "Epoch 202/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.5662 - accuracy: 0.7701\n",
            "Epoch 203/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5670 - accuracy: 0.7471\n",
            "Epoch 204/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.5665 - accuracy: 0.7701\n",
            "Epoch 205/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.5647 - accuracy: 0.7701\n",
            "Epoch 206/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5636 - accuracy: 0.7701\n",
            "Epoch 207/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5633 - accuracy: 0.7701\n",
            "Epoch 208/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5623 - accuracy: 0.7701\n",
            "Epoch 209/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5617 - accuracy: 0.7701\n",
            "Epoch 210/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5611 - accuracy: 0.7701\n",
            "Epoch 211/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5613 - accuracy: 0.7701\n",
            "Epoch 212/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5603 - accuracy: 0.7701\n",
            "Epoch 213/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.5594 - accuracy: 0.7471\n",
            "Epoch 214/1000\n",
            "87/87 [==============================] - 0s 61us/step - loss: 0.5583 - accuracy: 0.7586\n",
            "Epoch 215/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5583 - accuracy: 0.7586\n",
            "Epoch 216/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5578 - accuracy: 0.7701\n",
            "Epoch 217/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5590 - accuracy: 0.7471\n",
            "Epoch 218/1000\n",
            "87/87 [==============================] - 0s 104us/step - loss: 0.5554 - accuracy: 0.7701\n",
            "Epoch 219/1000\n",
            "87/87 [==============================] - 0s 129us/step - loss: 0.5546 - accuracy: 0.7701\n",
            "Epoch 220/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.5547 - accuracy: 0.7586\n",
            "Epoch 221/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5531 - accuracy: 0.7586\n",
            "Epoch 222/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5524 - accuracy: 0.7701\n",
            "Epoch 223/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5522 - accuracy: 0.7701\n",
            "Epoch 224/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5512 - accuracy: 0.7701\n",
            "Epoch 225/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5508 - accuracy: 0.7701\n",
            "Epoch 226/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5491 - accuracy: 0.7701\n",
            "Epoch 227/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5491 - accuracy: 0.7701\n",
            "Epoch 228/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5482 - accuracy: 0.7701\n",
            "Epoch 229/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5467 - accuracy: 0.7701\n",
            "Epoch 230/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.5466 - accuracy: 0.7701\n",
            "Epoch 231/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5458 - accuracy: 0.7701\n",
            "Epoch 232/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5469 - accuracy: 0.7586\n",
            "Epoch 233/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5437 - accuracy: 0.7701\n",
            "Epoch 234/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5442 - accuracy: 0.7701\n",
            "Epoch 235/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5421 - accuracy: 0.7701\n",
            "Epoch 236/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5417 - accuracy: 0.7701\n",
            "Epoch 237/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5411 - accuracy: 0.7701\n",
            "Epoch 238/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.5402 - accuracy: 0.7701\n",
            "Epoch 239/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5396 - accuracy: 0.7701\n",
            "Epoch 240/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5393 - accuracy: 0.7701\n",
            "Epoch 241/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5378 - accuracy: 0.7701\n",
            "Epoch 242/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5375 - accuracy: 0.7586\n",
            "Epoch 243/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5368 - accuracy: 0.7701\n",
            "Epoch 244/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5382 - accuracy: 0.7586\n",
            "Epoch 245/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5366 - accuracy: 0.7701\n",
            "Epoch 246/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5350 - accuracy: 0.7701\n",
            "Epoch 247/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5335 - accuracy: 0.7701\n",
            "Epoch 248/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5327 - accuracy: 0.7701\n",
            "Epoch 249/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5329 - accuracy: 0.7586\n",
            "Epoch 250/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5330 - accuracy: 0.7701\n",
            "Epoch 251/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5318 - accuracy: 0.7586\n",
            "Epoch 252/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5303 - accuracy: 0.7701\n",
            "Epoch 253/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5293 - accuracy: 0.7701\n",
            "Epoch 254/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5288 - accuracy: 0.7701\n",
            "Epoch 255/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5279 - accuracy: 0.7701\n",
            "Epoch 256/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5285 - accuracy: 0.7701\n",
            "Epoch 257/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5283 - accuracy: 0.7701\n",
            "Epoch 258/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.5272 - accuracy: 0.7701\n",
            "Epoch 259/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5257 - accuracy: 0.7701\n",
            "Epoch 260/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5251 - accuracy: 0.7701\n",
            "Epoch 261/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.5263 - accuracy: 0.7701\n",
            "Epoch 262/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.5232 - accuracy: 0.7701\n",
            "Epoch 263/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5236 - accuracy: 0.7701\n",
            "Epoch 264/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5226 - accuracy: 0.7701\n",
            "Epoch 265/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5217 - accuracy: 0.7586\n",
            "Epoch 266/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5218 - accuracy: 0.7701\n",
            "Epoch 267/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5197 - accuracy: 0.7586\n",
            "Epoch 268/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5199 - accuracy: 0.7586\n",
            "Epoch 269/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5190 - accuracy: 0.7701\n",
            "Epoch 270/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5188 - accuracy: 0.7701\n",
            "Epoch 271/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5173 - accuracy: 0.7701\n",
            "Epoch 272/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5176 - accuracy: 0.7586\n",
            "Epoch 273/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5159 - accuracy: 0.7586\n",
            "Epoch 274/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.5160 - accuracy: 0.7586\n",
            "Epoch 275/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.5152 - accuracy: 0.7586\n",
            "Epoch 276/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.5167 - accuracy: 0.7701\n",
            "Epoch 277/1000\n",
            "87/87 [==============================] - 0s 187us/step - loss: 0.5138 - accuracy: 0.7701\n",
            "Epoch 278/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5126 - accuracy: 0.7701\n",
            "Epoch 279/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5125 - accuracy: 0.7701\n",
            "Epoch 280/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.5121 - accuracy: 0.7701\n",
            "Epoch 281/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5110 - accuracy: 0.7701\n",
            "Epoch 282/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5115 - accuracy: 0.7701\n",
            "Epoch 283/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5104 - accuracy: 0.7701\n",
            "Epoch 284/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5091 - accuracy: 0.7701\n",
            "Epoch 285/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5092 - accuracy: 0.7701\n",
            "Epoch 286/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5086 - accuracy: 0.7701\n",
            "Epoch 287/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5087 - accuracy: 0.7701\n",
            "Epoch 288/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5079 - accuracy: 0.7701\n",
            "Epoch 289/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5056 - accuracy: 0.7701\n",
            "Epoch 290/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5080 - accuracy: 0.7701\n",
            "Epoch 291/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5042 - accuracy: 0.7701\n",
            "Epoch 292/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5039 - accuracy: 0.7701\n",
            "Epoch 293/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5045 - accuracy: 0.7701\n",
            "Epoch 294/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5036 - accuracy: 0.7701\n",
            "Epoch 295/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5024 - accuracy: 0.7701\n",
            "Epoch 296/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5013 - accuracy: 0.7701\n",
            "Epoch 297/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5012 - accuracy: 0.7701\n",
            "Epoch 298/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5003 - accuracy: 0.7701\n",
            "Epoch 299/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4994 - accuracy: 0.7701\n",
            "Epoch 300/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4992 - accuracy: 0.7701\n",
            "Epoch 301/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4986 - accuracy: 0.7701\n",
            "Epoch 302/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4992 - accuracy: 0.7701\n",
            "Epoch 303/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4976 - accuracy: 0.7701\n",
            "Epoch 304/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4971 - accuracy: 0.7816\n",
            "Epoch 305/1000\n",
            "87/87 [==============================] - 0s 143us/step - loss: 0.4961 - accuracy: 0.7816\n",
            "Epoch 306/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4965 - accuracy: 0.7816\n",
            "Epoch 307/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4950 - accuracy: 0.7816\n",
            "Epoch 308/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4942 - accuracy: 0.7816\n",
            "Epoch 309/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4939 - accuracy: 0.7816\n",
            "Epoch 310/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4941 - accuracy: 0.7816\n",
            "Epoch 311/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4932 - accuracy: 0.7816\n",
            "Epoch 312/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4916 - accuracy: 0.7816\n",
            "Epoch 313/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4912 - accuracy: 0.7816\n",
            "Epoch 314/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4922 - accuracy: 0.7816\n",
            "Epoch 315/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4907 - accuracy: 0.7931\n",
            "Epoch 316/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4896 - accuracy: 0.7816\n",
            "Epoch 317/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4889 - accuracy: 0.7816\n",
            "Epoch 318/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4888 - accuracy: 0.7816\n",
            "Epoch 319/1000\n",
            "87/87 [==============================] - 0s 65us/step - loss: 0.4878 - accuracy: 0.7931\n",
            "Epoch 320/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.4886 - accuracy: 0.7931\n",
            "Epoch 321/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.4865 - accuracy: 0.7816\n",
            "Epoch 322/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.4863 - accuracy: 0.7816\n",
            "Epoch 323/1000\n",
            "87/87 [==============================] - 0s 177us/step - loss: 0.4861 - accuracy: 0.7816\n",
            "Epoch 324/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4850 - accuracy: 0.7816\n",
            "Epoch 325/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4840 - accuracy: 0.7931\n",
            "Epoch 326/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4838 - accuracy: 0.7816\n",
            "Epoch 327/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4834 - accuracy: 0.7931\n",
            "Epoch 328/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.4828 - accuracy: 0.7931\n",
            "Epoch 329/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4826 - accuracy: 0.7931\n",
            "Epoch 330/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4832 - accuracy: 0.7931\n",
            "Epoch 331/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4811 - accuracy: 0.7931\n",
            "Epoch 332/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4804 - accuracy: 0.7931\n",
            "Epoch 333/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4796 - accuracy: 0.7931\n",
            "Epoch 334/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4788 - accuracy: 0.7931\n",
            "Epoch 335/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4787 - accuracy: 0.7931\n",
            "Epoch 336/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4792 - accuracy: 0.7931\n",
            "Epoch 337/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4770 - accuracy: 0.7931\n",
            "Epoch 338/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4770 - accuracy: 0.7931\n",
            "Epoch 339/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4767 - accuracy: 0.7931\n",
            "Epoch 340/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4755 - accuracy: 0.7931\n",
            "Epoch 341/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4754 - accuracy: 0.7931\n",
            "Epoch 342/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4747 - accuracy: 0.7931\n",
            "Epoch 343/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4770 - accuracy: 0.7931\n",
            "Epoch 344/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4731 - accuracy: 0.7931\n",
            "Epoch 345/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4733 - accuracy: 0.7931\n",
            "Epoch 346/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.4730 - accuracy: 0.7931\n",
            "Epoch 347/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4719 - accuracy: 0.7931\n",
            "Epoch 348/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4717 - accuracy: 0.7931\n",
            "Epoch 349/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.4705 - accuracy: 0.7931\n",
            "Epoch 350/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4704 - accuracy: 0.7931\n",
            "Epoch 351/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4697 - accuracy: 0.7931\n",
            "Epoch 352/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4694 - accuracy: 0.7931\n",
            "Epoch 353/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4693 - accuracy: 0.7931\n",
            "Epoch 354/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4696 - accuracy: 0.8046\n",
            "Epoch 355/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4679 - accuracy: 0.8046\n",
            "Epoch 356/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4670 - accuracy: 0.8046\n",
            "Epoch 357/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.4669 - accuracy: 0.8046\n",
            "Epoch 358/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4666 - accuracy: 0.8046\n",
            "Epoch 359/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4650 - accuracy: 0.8046\n",
            "Epoch 360/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4660 - accuracy: 0.8046\n",
            "Epoch 361/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4657 - accuracy: 0.8046\n",
            "Epoch 362/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4661 - accuracy: 0.8046\n",
            "Epoch 363/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4634 - accuracy: 0.8046\n",
            "Epoch 364/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4629 - accuracy: 0.8046\n",
            "Epoch 365/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4626 - accuracy: 0.8046\n",
            "Epoch 366/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4630 - accuracy: 0.8046\n",
            "Epoch 367/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4619 - accuracy: 0.8046\n",
            "Epoch 368/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4609 - accuracy: 0.8046\n",
            "Epoch 369/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4603 - accuracy: 0.8046\n",
            "Epoch 370/1000\n",
            "87/87 [==============================] - 0s 60us/step - loss: 0.4596 - accuracy: 0.8046\n",
            "Epoch 371/1000\n",
            "87/87 [==============================] - 0s 43us/step - loss: 0.4592 - accuracy: 0.8046\n",
            "Epoch 372/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.4583 - accuracy: 0.8046\n",
            "Epoch 373/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.4593 - accuracy: 0.8046\n",
            "Epoch 374/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4578 - accuracy: 0.8046\n",
            "Epoch 375/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4576 - accuracy: 0.8046\n",
            "Epoch 376/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4567 - accuracy: 0.8046\n",
            "Epoch 377/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4581 - accuracy: 0.8046\n",
            "Epoch 378/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.4579 - accuracy: 0.8046\n",
            "Epoch 379/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4547 - accuracy: 0.8046\n",
            "Epoch 380/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4555 - accuracy: 0.8046\n",
            "Epoch 381/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4553 - accuracy: 0.8046\n",
            "Epoch 382/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4542 - accuracy: 0.8046\n",
            "Epoch 383/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4540 - accuracy: 0.8046\n",
            "Epoch 384/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4523 - accuracy: 0.8046\n",
            "Epoch 385/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4518 - accuracy: 0.8046\n",
            "Epoch 386/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4518 - accuracy: 0.8046\n",
            "Epoch 387/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4510 - accuracy: 0.8046\n",
            "Epoch 388/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4502 - accuracy: 0.8046\n",
            "Epoch 389/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.4498 - accuracy: 0.8046\n",
            "Epoch 390/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.4495 - accuracy: 0.8046\n",
            "Epoch 391/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4487 - accuracy: 0.8046\n",
            "Epoch 392/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4511 - accuracy: 0.8046\n",
            "Epoch 393/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4497 - accuracy: 0.8046\n",
            "Epoch 394/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4484 - accuracy: 0.8046\n",
            "Epoch 395/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.4472 - accuracy: 0.8046\n",
            "Epoch 396/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4462 - accuracy: 0.8046\n",
            "Epoch 397/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4466 - accuracy: 0.8046\n",
            "Epoch 398/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4454 - accuracy: 0.8046\n",
            "Epoch 399/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4445 - accuracy: 0.8046\n",
            "Epoch 400/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4463 - accuracy: 0.8046\n",
            "Epoch 401/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4445 - accuracy: 0.8046\n",
            "Epoch 402/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4450 - accuracy: 0.8046\n",
            "Epoch 403/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4427 - accuracy: 0.8046\n",
            "Epoch 404/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4421 - accuracy: 0.8046\n",
            "Epoch 405/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4417 - accuracy: 0.8046\n",
            "Epoch 406/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4407 - accuracy: 0.8046\n",
            "Epoch 407/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4406 - accuracy: 0.8046\n",
            "Epoch 408/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4403 - accuracy: 0.8046\n",
            "Epoch 409/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4395 - accuracy: 0.8046\n",
            "Epoch 410/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4408 - accuracy: 0.7931\n",
            "Epoch 411/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4386 - accuracy: 0.7931\n",
            "Epoch 412/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4383 - accuracy: 0.7931\n",
            "Epoch 413/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4388 - accuracy: 0.8046\n",
            "Epoch 414/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4380 - accuracy: 0.7931\n",
            "Epoch 415/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4365 - accuracy: 0.7931\n",
            "Epoch 416/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4365 - accuracy: 0.7931\n",
            "Epoch 417/1000\n",
            "87/87 [==============================] - 0s 111us/step - loss: 0.4371 - accuracy: 0.7931\n",
            "Epoch 418/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4354 - accuracy: 0.7931\n",
            "Epoch 419/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4354 - accuracy: 0.7931\n",
            "Epoch 420/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4354 - accuracy: 0.7931\n",
            "Epoch 421/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4338 - accuracy: 0.7931\n",
            "Epoch 422/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4357 - accuracy: 0.7931\n",
            "Epoch 423/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.4328 - accuracy: 0.7931\n",
            "Epoch 424/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4320 - accuracy: 0.7931\n",
            "Epoch 425/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4320 - accuracy: 0.7931\n",
            "Epoch 426/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4314 - accuracy: 0.7931\n",
            "Epoch 427/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4310 - accuracy: 0.7931\n",
            "Epoch 428/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4310 - accuracy: 0.7931\n",
            "Epoch 429/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.4315 - accuracy: 0.7931\n",
            "Epoch 430/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4298 - accuracy: 0.7931\n",
            "Epoch 431/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4293 - accuracy: 0.7931\n",
            "Epoch 432/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4286 - accuracy: 0.7931\n",
            "Epoch 433/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4280 - accuracy: 0.7931\n",
            "Epoch 434/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.4274 - accuracy: 0.7931\n",
            "Epoch 435/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4302 - accuracy: 0.7931\n",
            "Epoch 436/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4261 - accuracy: 0.7931\n",
            "Epoch 437/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4260 - accuracy: 0.7931\n",
            "Epoch 438/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4270 - accuracy: 0.7931\n",
            "Epoch 439/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4259 - accuracy: 0.7931\n",
            "Epoch 440/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4250 - accuracy: 0.7931\n",
            "Epoch 441/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4251 - accuracy: 0.7931\n",
            "Epoch 442/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4257 - accuracy: 0.7931\n",
            "Epoch 443/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.4233 - accuracy: 0.7931\n",
            "Epoch 444/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.4240 - accuracy: 0.7931\n",
            "Epoch 445/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4226 - accuracy: 0.7931\n",
            "Epoch 446/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4228 - accuracy: 0.7931\n",
            "Epoch 447/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4221 - accuracy: 0.7931\n",
            "Epoch 448/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4206 - accuracy: 0.7931\n",
            "Epoch 449/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4221 - accuracy: 0.7931\n",
            "Epoch 450/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4197 - accuracy: 0.7931\n",
            "Epoch 451/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4214 - accuracy: 0.7931\n",
            "Epoch 452/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4191 - accuracy: 0.7931\n",
            "Epoch 453/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.4186 - accuracy: 0.7931\n",
            "Epoch 454/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4176 - accuracy: 0.7931\n",
            "Epoch 455/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4190 - accuracy: 0.8046\n",
            "Epoch 456/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4177 - accuracy: 0.7931\n",
            "Epoch 457/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4175 - accuracy: 0.7931\n",
            "Epoch 458/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4160 - accuracy: 0.7931\n",
            "Epoch 459/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4167 - accuracy: 0.7931\n",
            "Epoch 460/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4150 - accuracy: 0.7931\n",
            "Epoch 461/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4147 - accuracy: 0.7931\n",
            "Epoch 462/1000\n",
            "87/87 [==============================] - 0s 188us/step - loss: 0.4139 - accuracy: 0.7931\n",
            "Epoch 463/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4182 - accuracy: 0.7931\n",
            "Epoch 464/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4130 - accuracy: 0.7931\n",
            "Epoch 465/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4131 - accuracy: 0.8046\n",
            "Epoch 466/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.4126 - accuracy: 0.8046\n",
            "Epoch 467/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4127 - accuracy: 0.8046\n",
            "Epoch 468/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4108 - accuracy: 0.8046\n",
            "Epoch 469/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4123 - accuracy: 0.7931\n",
            "Epoch 470/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4094 - accuracy: 0.7931\n",
            "Epoch 471/1000\n",
            "87/87 [==============================] - 0s 115us/step - loss: 0.4100 - accuracy: 0.8046\n",
            "Epoch 472/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4107 - accuracy: 0.8046\n",
            "Epoch 473/1000\n",
            "87/87 [==============================] - 0s 105us/step - loss: 0.4089 - accuracy: 0.7931\n",
            "Epoch 474/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4125 - accuracy: 0.7931\n",
            "Epoch 475/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4075 - accuracy: 0.8046\n",
            "Epoch 476/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4069 - accuracy: 0.8046\n",
            "Epoch 477/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4071 - accuracy: 0.8046\n",
            "Epoch 478/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4064 - accuracy: 0.8046\n",
            "Epoch 479/1000\n",
            "87/87 [==============================] - 0s 105us/step - loss: 0.4068 - accuracy: 0.8046\n",
            "Epoch 480/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4053 - accuracy: 0.8046\n",
            "Epoch 481/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.4047 - accuracy: 0.8046\n",
            "Epoch 482/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.4057 - accuracy: 0.8046\n",
            "Epoch 483/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4040 - accuracy: 0.8046\n",
            "Epoch 484/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4038 - accuracy: 0.8046\n",
            "Epoch 485/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.4043 - accuracy: 0.8046\n",
            "Epoch 486/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4028 - accuracy: 0.8161\n",
            "Epoch 487/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4019 - accuracy: 0.8161\n",
            "Epoch 488/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4013 - accuracy: 0.8046\n",
            "Epoch 489/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.4018 - accuracy: 0.8046\n",
            "Epoch 490/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4010 - accuracy: 0.8046\n",
            "Epoch 491/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3999 - accuracy: 0.8046\n",
            "Epoch 492/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.4001 - accuracy: 0.8046\n",
            "Epoch 493/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.3990 - accuracy: 0.8046\n",
            "Epoch 494/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.3987 - accuracy: 0.8046\n",
            "Epoch 495/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.3982 - accuracy: 0.8046\n",
            "Epoch 496/1000\n",
            "87/87 [==============================] - 0s 63us/step - loss: 0.3983 - accuracy: 0.8046\n",
            "Epoch 497/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3983 - accuracy: 0.8046\n",
            "Epoch 498/1000\n",
            "87/87 [==============================] - 0s 116us/step - loss: 0.3972 - accuracy: 0.8161\n",
            "Epoch 499/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3966 - accuracy: 0.8046\n",
            "Epoch 500/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3964 - accuracy: 0.8046\n",
            "Epoch 501/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3959 - accuracy: 0.8046\n",
            "Epoch 502/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.3953 - accuracy: 0.8046\n",
            "Epoch 503/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3975 - accuracy: 0.8046\n",
            "Epoch 504/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.3988 - accuracy: 0.8046\n",
            "Epoch 505/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3936 - accuracy: 0.8046\n",
            "Epoch 506/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.3944 - accuracy: 0.8046\n",
            "Epoch 507/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.3929 - accuracy: 0.8276\n",
            "Epoch 508/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.3940 - accuracy: 0.8276\n",
            "Epoch 509/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3921 - accuracy: 0.8391\n",
            "Epoch 510/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3917 - accuracy: 0.8046\n",
            "Epoch 511/1000\n",
            "87/87 [==============================] - 0s 104us/step - loss: 0.3917 - accuracy: 0.8161\n",
            "Epoch 512/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3908 - accuracy: 0.8046\n",
            "Epoch 513/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3900 - accuracy: 0.8046\n",
            "Epoch 514/1000\n",
            "87/87 [==============================] - 0s 134us/step - loss: 0.3898 - accuracy: 0.8161\n",
            "Epoch 515/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3899 - accuracy: 0.8046\n",
            "Epoch 516/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3884 - accuracy: 0.8046\n",
            "Epoch 517/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3883 - accuracy: 0.8161\n",
            "Epoch 518/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3888 - accuracy: 0.8161\n",
            "Epoch 519/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3880 - accuracy: 0.8161\n",
            "Epoch 520/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3871 - accuracy: 0.8161\n",
            "Epoch 521/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3887 - accuracy: 0.8161\n",
            "Epoch 522/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3882 - accuracy: 0.8276\n",
            "Epoch 523/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.3876 - accuracy: 0.8276\n",
            "Epoch 524/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3851 - accuracy: 0.8391\n",
            "Epoch 525/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3866 - accuracy: 0.8391\n",
            "Epoch 526/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3844 - accuracy: 0.8276\n",
            "Epoch 527/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3848 - accuracy: 0.8161\n",
            "Epoch 528/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3844 - accuracy: 0.8276\n",
            "Epoch 529/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3835 - accuracy: 0.8276\n",
            "Epoch 530/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3836 - accuracy: 0.8276\n",
            "Epoch 531/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3830 - accuracy: 0.8276\n",
            "Epoch 532/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3818 - accuracy: 0.8276\n",
            "Epoch 533/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3825 - accuracy: 0.8161\n",
            "Epoch 534/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.3818 - accuracy: 0.8391\n",
            "Epoch 535/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3814 - accuracy: 0.8276\n",
            "Epoch 536/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3817 - accuracy: 0.8391\n",
            "Epoch 537/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3812 - accuracy: 0.8276\n",
            "Epoch 538/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3824 - accuracy: 0.8276\n",
            "Epoch 539/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3792 - accuracy: 0.8276\n",
            "Epoch 540/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3790 - accuracy: 0.8276\n",
            "Epoch 541/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3787 - accuracy: 0.8391\n",
            "Epoch 542/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3799 - accuracy: 0.8276\n",
            "Epoch 543/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3772 - accuracy: 0.8276\n",
            "Epoch 544/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3782 - accuracy: 0.8276\n",
            "Epoch 545/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3768 - accuracy: 0.8391\n",
            "Epoch 546/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3764 - accuracy: 0.8391\n",
            "Epoch 547/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3774 - accuracy: 0.8391\n",
            "Epoch 548/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3761 - accuracy: 0.8506\n",
            "Epoch 549/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3766 - accuracy: 0.8506\n",
            "Epoch 550/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3746 - accuracy: 0.8391\n",
            "Epoch 551/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.3765 - accuracy: 0.8391\n",
            "Epoch 552/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3736 - accuracy: 0.8391\n",
            "Epoch 553/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3735 - accuracy: 0.8506\n",
            "Epoch 554/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3750 - accuracy: 0.8391\n",
            "Epoch 555/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3722 - accuracy: 0.8506\n",
            "Epoch 556/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3731 - accuracy: 0.8506\n",
            "Epoch 557/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3722 - accuracy: 0.8506\n",
            "Epoch 558/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3727 - accuracy: 0.8506\n",
            "Epoch 559/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3716 - accuracy: 0.8506\n",
            "Epoch 560/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3705 - accuracy: 0.8506\n",
            "Epoch 561/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3700 - accuracy: 0.8506\n",
            "Epoch 562/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3701 - accuracy: 0.8506\n",
            "Epoch 563/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3689 - accuracy: 0.8506\n",
            "Epoch 564/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3699 - accuracy: 0.8506\n",
            "Epoch 565/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3685 - accuracy: 0.8506\n",
            "Epoch 566/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3682 - accuracy: 0.8506\n",
            "Epoch 567/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3686 - accuracy: 0.8506\n",
            "Epoch 568/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3696 - accuracy: 0.8391\n",
            "Epoch 569/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3676 - accuracy: 0.8506\n",
            "Epoch 570/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3666 - accuracy: 0.8506\n",
            "Epoch 571/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3666 - accuracy: 0.8621\n",
            "Epoch 572/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3653 - accuracy: 0.8621\n",
            "Epoch 573/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3659 - accuracy: 0.8621\n",
            "Epoch 574/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3645 - accuracy: 0.8506\n",
            "Epoch 575/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3644 - accuracy: 0.8506\n",
            "Epoch 576/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3647 - accuracy: 0.8506\n",
            "Epoch 577/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3644 - accuracy: 0.8621\n",
            "Epoch 578/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3644 - accuracy: 0.8621\n",
            "Epoch 579/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3632 - accuracy: 0.8506\n",
            "Epoch 580/1000\n",
            "87/87 [==============================] - 0s 58us/step - loss: 0.3625 - accuracy: 0.8506\n",
            "Epoch 581/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3646 - accuracy: 0.8276\n",
            "Epoch 582/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3619 - accuracy: 0.8506\n",
            "Epoch 583/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.3612 - accuracy: 0.8506\n",
            "Epoch 584/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3613 - accuracy: 0.8506\n",
            "Epoch 585/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3601 - accuracy: 0.8506\n",
            "Epoch 586/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3599 - accuracy: 0.8506\n",
            "Epoch 587/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3596 - accuracy: 0.8506\n",
            "Epoch 588/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3608 - accuracy: 0.8621\n",
            "Epoch 589/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3589 - accuracy: 0.8621\n",
            "Epoch 590/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3596 - accuracy: 0.8506\n",
            "Epoch 591/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3583 - accuracy: 0.8506\n",
            "Epoch 592/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3574 - accuracy: 0.8621\n",
            "Epoch 593/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3582 - accuracy: 0.8621\n",
            "Epoch 594/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3568 - accuracy: 0.8621\n",
            "Epoch 595/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3576 - accuracy: 0.8621\n",
            "Epoch 596/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3584 - accuracy: 0.8506\n",
            "Epoch 597/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3552 - accuracy: 0.8506\n",
            "Epoch 598/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3564 - accuracy: 0.8621\n",
            "Epoch 599/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3566 - accuracy: 0.8621\n",
            "Epoch 600/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3543 - accuracy: 0.8621\n",
            "Epoch 601/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3552 - accuracy: 0.8506\n",
            "Epoch 602/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3542 - accuracy: 0.8736\n",
            "Epoch 603/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3539 - accuracy: 0.8736\n",
            "Epoch 604/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3527 - accuracy: 0.8736\n",
            "Epoch 605/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3542 - accuracy: 0.8736\n",
            "Epoch 606/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3527 - accuracy: 0.8621\n",
            "Epoch 607/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3530 - accuracy: 0.8736\n",
            "Epoch 608/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3516 - accuracy: 0.8506\n",
            "Epoch 609/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3513 - accuracy: 0.8621\n",
            "Epoch 610/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3521 - accuracy: 0.8621\n",
            "Epoch 611/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3505 - accuracy: 0.8736\n",
            "Epoch 612/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3501 - accuracy: 0.8736\n",
            "Epoch 613/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3509 - accuracy: 0.8736\n",
            "Epoch 614/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3500 - accuracy: 0.8736\n",
            "Epoch 615/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3490 - accuracy: 0.8736\n",
            "Epoch 616/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3492 - accuracy: 0.8736\n",
            "Epoch 617/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3504 - accuracy: 0.8736\n",
            "Epoch 618/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3480 - accuracy: 0.8736\n",
            "Epoch 619/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3476 - accuracy: 0.8736\n",
            "Epoch 620/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3476 - accuracy: 0.8736\n",
            "Epoch 621/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3468 - accuracy: 0.8736\n",
            "Epoch 622/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3473 - accuracy: 0.8736\n",
            "Epoch 623/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3465 - accuracy: 0.8736\n",
            "Epoch 624/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3458 - accuracy: 0.8736\n",
            "Epoch 625/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3452 - accuracy: 0.8621\n",
            "Epoch 626/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3451 - accuracy: 0.8736\n",
            "Epoch 627/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3460 - accuracy: 0.8736\n",
            "Epoch 628/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3458 - accuracy: 0.8736\n",
            "Epoch 629/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3442 - accuracy: 0.8736\n",
            "Epoch 630/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3453 - accuracy: 0.8621\n",
            "Epoch 631/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3431 - accuracy: 0.8621\n",
            "Epoch 632/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3431 - accuracy: 0.8621\n",
            "Epoch 633/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3420 - accuracy: 0.8621\n",
            "Epoch 634/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3444 - accuracy: 0.8621\n",
            "Epoch 635/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3428 - accuracy: 0.8621\n",
            "Epoch 636/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3427 - accuracy: 0.8621\n",
            "Epoch 637/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3407 - accuracy: 0.8736\n",
            "Epoch 638/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3410 - accuracy: 0.8736\n",
            "Epoch 639/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3401 - accuracy: 0.8736\n",
            "Epoch 640/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3396 - accuracy: 0.8736\n",
            "Epoch 641/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3396 - accuracy: 0.8736\n",
            "Epoch 642/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3385 - accuracy: 0.8736\n",
            "Epoch 643/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3384 - accuracy: 0.8621\n",
            "Epoch 644/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3391 - accuracy: 0.8736\n",
            "Epoch 645/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3386 - accuracy: 0.8621\n",
            "Epoch 646/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3371 - accuracy: 0.8621\n",
            "Epoch 647/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3366 - accuracy: 0.8736\n",
            "Epoch 648/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3370 - accuracy: 0.8736\n",
            "Epoch 649/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3371 - accuracy: 0.8736\n",
            "Epoch 650/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3358 - accuracy: 0.8736\n",
            "Epoch 651/1000\n",
            "87/87 [==============================] - 0s 60us/step - loss: 0.3356 - accuracy: 0.8621\n",
            "Epoch 652/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3356 - accuracy: 0.8736\n",
            "Epoch 653/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3359 - accuracy: 0.8736\n",
            "Epoch 654/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3355 - accuracy: 0.8736\n",
            "Epoch 655/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3345 - accuracy: 0.8736\n",
            "Epoch 656/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3338 - accuracy: 0.8736\n",
            "Epoch 657/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3336 - accuracy: 0.8621\n",
            "Epoch 658/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3341 - accuracy: 0.8736\n",
            "Epoch 659/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3324 - accuracy: 0.8736\n",
            "Epoch 660/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3324 - accuracy: 0.8736\n",
            "Epoch 661/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3341 - accuracy: 0.8736\n",
            "Epoch 662/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3315 - accuracy: 0.8736\n",
            "Epoch 663/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3347 - accuracy: 0.8736\n",
            "Epoch 664/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3305 - accuracy: 0.8736\n",
            "Epoch 665/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.3308 - accuracy: 0.8736\n",
            "Epoch 666/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3305 - accuracy: 0.8736\n",
            "Epoch 667/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3308 - accuracy: 0.8621\n",
            "Epoch 668/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.3288 - accuracy: 0.8736\n",
            "Epoch 669/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3289 - accuracy: 0.8736\n",
            "Epoch 670/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.3302 - accuracy: 0.8736\n",
            "Epoch 671/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3276 - accuracy: 0.8851\n",
            "Epoch 672/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.3298 - accuracy: 0.8851\n",
            "Epoch 673/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3288 - accuracy: 0.8851\n",
            "Epoch 674/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3286 - accuracy: 0.8851\n",
            "Epoch 675/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3267 - accuracy: 0.8736\n",
            "Epoch 676/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3275 - accuracy: 0.8736\n",
            "Epoch 677/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3276 - accuracy: 0.8851\n",
            "Epoch 678/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3261 - accuracy: 0.8736\n",
            "Epoch 679/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3282 - accuracy: 0.8621\n",
            "Epoch 680/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3248 - accuracy: 0.8621\n",
            "Epoch 681/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3272 - accuracy: 0.8851\n",
            "Epoch 682/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3240 - accuracy: 0.8851\n",
            "Epoch 683/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.3246 - accuracy: 0.8851\n",
            "Epoch 684/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3236 - accuracy: 0.8851\n",
            "Epoch 685/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3234 - accuracy: 0.8851\n",
            "Epoch 686/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3233 - accuracy: 0.8851\n",
            "Epoch 687/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3223 - accuracy: 0.8851\n",
            "Epoch 688/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3244 - accuracy: 0.8736\n",
            "Epoch 689/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.3220 - accuracy: 0.8851\n",
            "Epoch 690/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3224 - accuracy: 0.8851\n",
            "Epoch 691/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3230 - accuracy: 0.8851\n",
            "Epoch 692/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3206 - accuracy: 0.8851\n",
            "Epoch 693/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3209 - accuracy: 0.8851\n",
            "Epoch 694/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3202 - accuracy: 0.8851\n",
            "Epoch 695/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3201 - accuracy: 0.8736\n",
            "Epoch 696/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3207 - accuracy: 0.8736\n",
            "Epoch 697/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.3210 - accuracy: 0.8851\n",
            "Epoch 698/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3195 - accuracy: 0.8851\n",
            "Epoch 699/1000\n",
            "87/87 [==============================] - 0s 168us/step - loss: 0.3183 - accuracy: 0.8851\n",
            "Epoch 700/1000\n",
            "87/87 [==============================] - 0s 59us/step - loss: 0.3184 - accuracy: 0.8851\n",
            "Epoch 701/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.3181 - accuracy: 0.8851\n",
            "Epoch 702/1000\n",
            "87/87 [==============================] - 0s 130us/step - loss: 0.3179 - accuracy: 0.8851\n",
            "Epoch 703/1000\n",
            "87/87 [==============================] - 0s 185us/step - loss: 0.3184 - accuracy: 0.8851\n",
            "Epoch 704/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.3181 - accuracy: 0.8851\n",
            "Epoch 705/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3174 - accuracy: 0.8851\n",
            "Epoch 706/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3177 - accuracy: 0.8851\n",
            "Epoch 707/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3161 - accuracy: 0.8851\n",
            "Epoch 708/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3179 - accuracy: 0.8851\n",
            "Epoch 709/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3163 - accuracy: 0.8851\n",
            "Epoch 710/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3146 - accuracy: 0.8851\n",
            "Epoch 711/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3179 - accuracy: 0.8851\n",
            "Epoch 712/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3163 - accuracy: 0.8851\n",
            "Epoch 713/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3145 - accuracy: 0.8851\n",
            "Epoch 714/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3136 - accuracy: 0.8851\n",
            "Epoch 715/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3137 - accuracy: 0.8851\n",
            "Epoch 716/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3130 - accuracy: 0.8851\n",
            "Epoch 717/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3131 - accuracy: 0.8851\n",
            "Epoch 718/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3152 - accuracy: 0.8851\n",
            "Epoch 719/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3118 - accuracy: 0.8851\n",
            "Epoch 720/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3117 - accuracy: 0.8851\n",
            "Epoch 721/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.3109 - accuracy: 0.8851\n",
            "Epoch 722/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3111 - accuracy: 0.8851\n",
            "Epoch 723/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3105 - accuracy: 0.8851\n",
            "Epoch 724/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3103 - accuracy: 0.8851\n",
            "Epoch 725/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3099 - accuracy: 0.8851\n",
            "Epoch 726/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3097 - accuracy: 0.8851\n",
            "Epoch 727/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3118 - accuracy: 0.8851\n",
            "Epoch 728/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3088 - accuracy: 0.8851\n",
            "Epoch 729/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3083 - accuracy: 0.8851\n",
            "Epoch 730/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3084 - accuracy: 0.8851\n",
            "Epoch 731/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3092 - accuracy: 0.8851\n",
            "Epoch 732/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3076 - accuracy: 0.8851\n",
            "Epoch 733/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3072 - accuracy: 0.8851\n",
            "Epoch 734/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.3077 - accuracy: 0.8851\n",
            "Epoch 735/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3062 - accuracy: 0.8966\n",
            "Epoch 736/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3065 - accuracy: 0.8851\n",
            "Epoch 737/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3066 - accuracy: 0.8851\n",
            "Epoch 738/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3062 - accuracy: 0.8966\n",
            "Epoch 739/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3053 - accuracy: 0.8966\n",
            "Epoch 740/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.3046 - accuracy: 0.8966\n",
            "Epoch 741/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3061 - accuracy: 0.8966\n",
            "Epoch 742/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3039 - accuracy: 0.8966\n",
            "Epoch 743/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.3040 - accuracy: 0.8851\n",
            "Epoch 744/1000\n",
            "87/87 [==============================] - 0s 127us/step - loss: 0.3036 - accuracy: 0.8966\n",
            "Epoch 745/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3028 - accuracy: 0.8966\n",
            "Epoch 746/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3029 - accuracy: 0.8966\n",
            "Epoch 747/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3018 - accuracy: 0.8966\n",
            "Epoch 748/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.3032 - accuracy: 0.8966\n",
            "Epoch 749/1000\n",
            "87/87 [==============================] - 0s 68us/step - loss: 0.3014 - accuracy: 0.8966\n",
            "Epoch 750/1000\n",
            "87/87 [==============================] - 0s 68us/step - loss: 0.3014 - accuracy: 0.8966\n",
            "Epoch 751/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3008 - accuracy: 0.8966\n",
            "Epoch 752/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.3007 - accuracy: 0.8966\n",
            "Epoch 753/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.3001 - accuracy: 0.8966\n",
            "Epoch 754/1000\n",
            "87/87 [==============================] - 0s 132us/step - loss: 0.2999 - accuracy: 0.8966\n",
            "Epoch 755/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2994 - accuracy: 0.8966\n",
            "Epoch 756/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2998 - accuracy: 0.8966\n",
            "Epoch 757/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2992 - accuracy: 0.8966\n",
            "Epoch 758/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2997 - accuracy: 0.8966\n",
            "Epoch 759/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2984 - accuracy: 0.8966\n",
            "Epoch 760/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2982 - accuracy: 0.8966\n",
            "Epoch 761/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2979 - accuracy: 0.8966\n",
            "Epoch 762/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2969 - accuracy: 0.8966\n",
            "Epoch 763/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2971 - accuracy: 0.8966\n",
            "Epoch 764/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2962 - accuracy: 0.8966\n",
            "Epoch 765/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2966 - accuracy: 0.8966\n",
            "Epoch 766/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2955 - accuracy: 0.8966\n",
            "Epoch 767/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2952 - accuracy: 0.8966\n",
            "Epoch 768/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2993 - accuracy: 0.8966\n",
            "Epoch 769/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2971 - accuracy: 0.8966\n",
            "Epoch 770/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2959 - accuracy: 0.8966\n",
            "Epoch 771/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2938 - accuracy: 0.8966\n",
            "Epoch 772/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2944 - accuracy: 0.8966\n",
            "Epoch 773/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2949 - accuracy: 0.8966\n",
            "Epoch 774/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2936 - accuracy: 0.8966\n",
            "Epoch 775/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2942 - accuracy: 0.8966\n",
            "Epoch 776/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2921 - accuracy: 0.8966\n",
            "Epoch 777/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2926 - accuracy: 0.8966\n",
            "Epoch 778/1000\n",
            "87/87 [==============================] - 0s 107us/step - loss: 0.2920 - accuracy: 0.8966\n",
            "Epoch 779/1000\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.2915 - accuracy: 0.8966\n",
            "Epoch 780/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2907 - accuracy: 0.8966\n",
            "Epoch 781/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2915 - accuracy: 0.8966\n",
            "Epoch 782/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2902 - accuracy: 0.8966\n",
            "Epoch 783/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2912 - accuracy: 0.8966\n",
            "Epoch 784/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2900 - accuracy: 0.8966\n",
            "Epoch 785/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2902 - accuracy: 0.8966\n",
            "Epoch 786/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2902 - accuracy: 0.8966\n",
            "Epoch 787/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2894 - accuracy: 0.8966\n",
            "Epoch 788/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2882 - accuracy: 0.8966\n",
            "Epoch 789/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2893 - accuracy: 0.8966\n",
            "Epoch 790/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2882 - accuracy: 0.8966\n",
            "Epoch 791/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2884 - accuracy: 0.8966\n",
            "Epoch 792/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2888 - accuracy: 0.8966\n",
            "Epoch 793/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2880 - accuracy: 0.8966\n",
            "Epoch 794/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2883 - accuracy: 0.8966\n",
            "Epoch 795/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2883 - accuracy: 0.9080\n",
            "Epoch 796/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2859 - accuracy: 0.8966\n",
            "Epoch 797/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2869 - accuracy: 0.8966\n",
            "Epoch 798/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2849 - accuracy: 0.8966\n",
            "Epoch 799/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2866 - accuracy: 0.8966\n",
            "Epoch 800/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2841 - accuracy: 0.8966\n",
            "Epoch 801/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.2856 - accuracy: 0.8966\n",
            "Epoch 802/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2851 - accuracy: 0.9080\n",
            "Epoch 803/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.2830 - accuracy: 0.9080\n",
            "Epoch 804/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2823 - accuracy: 0.9195\n",
            "Epoch 805/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2824 - accuracy: 0.9195\n",
            "Epoch 806/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2811 - accuracy: 0.9080\n",
            "Epoch 807/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2821 - accuracy: 0.9080\n",
            "Epoch 808/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2813 - accuracy: 0.9080\n",
            "Epoch 809/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2826 - accuracy: 0.9080\n",
            "Epoch 810/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2804 - accuracy: 0.9080\n",
            "Epoch 811/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.2817 - accuracy: 0.9080\n",
            "Epoch 812/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2799 - accuracy: 0.9080\n",
            "Epoch 813/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2803 - accuracy: 0.9195\n",
            "Epoch 814/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2789 - accuracy: 0.9195\n",
            "Epoch 815/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2796 - accuracy: 0.9195\n",
            "Epoch 816/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2793 - accuracy: 0.9195\n",
            "Epoch 817/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2779 - accuracy: 0.9080\n",
            "Epoch 818/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2783 - accuracy: 0.9080\n",
            "Epoch 819/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2770 - accuracy: 0.9195\n",
            "Epoch 820/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2768 - accuracy: 0.9195\n",
            "Epoch 821/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2773 - accuracy: 0.9195\n",
            "Epoch 822/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2761 - accuracy: 0.9195\n",
            "Epoch 823/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2776 - accuracy: 0.9080\n",
            "Epoch 824/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2747 - accuracy: 0.9080\n",
            "Epoch 825/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2763 - accuracy: 0.9080\n",
            "Epoch 826/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2754 - accuracy: 0.9080\n",
            "Epoch 827/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2748 - accuracy: 0.9080\n",
            "Epoch 828/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2740 - accuracy: 0.9195\n",
            "Epoch 829/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2734 - accuracy: 0.9195\n",
            "Epoch 830/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2736 - accuracy: 0.9310\n",
            "Epoch 831/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2725 - accuracy: 0.9195\n",
            "Epoch 832/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2751 - accuracy: 0.9195\n",
            "Epoch 833/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2716 - accuracy: 0.9310\n",
            "Epoch 834/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2723 - accuracy: 0.9195\n",
            "Epoch 835/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2715 - accuracy: 0.9195\n",
            "Epoch 836/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2721 - accuracy: 0.9195\n",
            "Epoch 837/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2740 - accuracy: 0.9195\n",
            "Epoch 838/1000\n",
            "87/87 [==============================] - 0s 125us/step - loss: 0.2706 - accuracy: 0.9080\n",
            "Epoch 839/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.2712 - accuracy: 0.9195\n",
            "Epoch 840/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2712 - accuracy: 0.9195\n",
            "Epoch 841/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.2695 - accuracy: 0.9195\n",
            "Epoch 842/1000\n",
            "87/87 [==============================] - 0s 178us/step - loss: 0.2695 - accuracy: 0.9195\n",
            "Epoch 843/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2697 - accuracy: 0.9195\n",
            "Epoch 844/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.2683 - accuracy: 0.9195\n",
            "Epoch 845/1000\n",
            "87/87 [==============================] - 0s 114us/step - loss: 0.2677 - accuracy: 0.9195\n",
            "Epoch 846/1000\n",
            "87/87 [==============================] - 0s 124us/step - loss: 0.2680 - accuracy: 0.9195\n",
            "Epoch 847/1000\n",
            "87/87 [==============================] - 0s 131us/step - loss: 0.2677 - accuracy: 0.9195\n",
            "Epoch 848/1000\n",
            "87/87 [==============================] - 0s 132us/step - loss: 0.2673 - accuracy: 0.9195\n",
            "Epoch 849/1000\n",
            "87/87 [==============================] - 0s 133us/step - loss: 0.2694 - accuracy: 0.9080\n",
            "Epoch 850/1000\n",
            "87/87 [==============================] - 0s 129us/step - loss: 0.2671 - accuracy: 0.9195\n",
            "Epoch 851/1000\n",
            "87/87 [==============================] - 0s 142us/step - loss: 0.2662 - accuracy: 0.9195\n",
            "Epoch 852/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.2680 - accuracy: 0.9310\n",
            "Epoch 853/1000\n",
            "87/87 [==============================] - 0s 117us/step - loss: 0.2666 - accuracy: 0.9310\n",
            "Epoch 854/1000\n",
            "87/87 [==============================] - 0s 127us/step - loss: 0.2653 - accuracy: 0.9195\n",
            "Epoch 855/1000\n",
            "87/87 [==============================] - 0s 122us/step - loss: 0.2647 - accuracy: 0.9195\n",
            "Epoch 856/1000\n",
            "87/87 [==============================] - 0s 113us/step - loss: 0.2665 - accuracy: 0.9310\n",
            "Epoch 857/1000\n",
            "87/87 [==============================] - 0s 109us/step - loss: 0.2634 - accuracy: 0.9195\n",
            "Epoch 858/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.2642 - accuracy: 0.9195\n",
            "Epoch 859/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2645 - accuracy: 0.9310\n",
            "Epoch 860/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.2629 - accuracy: 0.9195\n",
            "Epoch 861/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2651 - accuracy: 0.9195\n",
            "Epoch 862/1000\n",
            "87/87 [==============================] - 0s 114us/step - loss: 0.2626 - accuracy: 0.9310\n",
            "Epoch 863/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.2628 - accuracy: 0.9310\n",
            "Epoch 864/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.2632 - accuracy: 0.9310\n",
            "Epoch 865/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2630 - accuracy: 0.9195\n",
            "Epoch 866/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2651 - accuracy: 0.9310\n",
            "Epoch 867/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2618 - accuracy: 0.9310\n",
            "Epoch 868/1000\n",
            "87/87 [==============================] - 0s 109us/step - loss: 0.2621 - accuracy: 0.9195\n",
            "Epoch 869/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2603 - accuracy: 0.9310\n",
            "Epoch 870/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2605 - accuracy: 0.9310\n",
            "Epoch 871/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2603 - accuracy: 0.9310\n",
            "Epoch 872/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2591 - accuracy: 0.9310\n",
            "Epoch 873/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2597 - accuracy: 0.9310\n",
            "Epoch 874/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2615 - accuracy: 0.9195\n",
            "Epoch 875/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2608 - accuracy: 0.9310\n",
            "Epoch 876/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2579 - accuracy: 0.9195\n",
            "Epoch 877/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2576 - accuracy: 0.9310\n",
            "Epoch 878/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2579 - accuracy: 0.9310\n",
            "Epoch 879/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.2574 - accuracy: 0.9195\n",
            "Epoch 880/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2593 - accuracy: 0.9310\n",
            "Epoch 881/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2563 - accuracy: 0.9195\n",
            "Epoch 882/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2566 - accuracy: 0.9195\n",
            "Epoch 883/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2562 - accuracy: 0.9195\n",
            "Epoch 884/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2558 - accuracy: 0.9310\n",
            "Epoch 885/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2553 - accuracy: 0.9310\n",
            "Epoch 886/1000\n",
            "87/87 [==============================] - 0s 107us/step - loss: 0.2559 - accuracy: 0.9310\n",
            "Epoch 887/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2546 - accuracy: 0.9310\n",
            "Epoch 888/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2540 - accuracy: 0.9310\n",
            "Epoch 889/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2535 - accuracy: 0.9310\n",
            "Epoch 890/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2542 - accuracy: 0.9195\n",
            "Epoch 891/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2532 - accuracy: 0.9195\n",
            "Epoch 892/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2528 - accuracy: 0.9310\n",
            "Epoch 893/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2539 - accuracy: 0.9310\n",
            "Epoch 894/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2533 - accuracy: 0.9310\n",
            "Epoch 895/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2541 - accuracy: 0.9310\n",
            "Epoch 896/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2518 - accuracy: 0.9310\n",
            "Epoch 897/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2519 - accuracy: 0.9310\n",
            "Epoch 898/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.2525 - accuracy: 0.9425\n",
            "Epoch 899/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2505 - accuracy: 0.9425\n",
            "Epoch 900/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2519 - accuracy: 0.9310\n",
            "Epoch 901/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2500 - accuracy: 0.9310\n",
            "Epoch 902/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2504 - accuracy: 0.9425\n",
            "Epoch 903/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2507 - accuracy: 0.9425\n",
            "Epoch 904/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2508 - accuracy: 0.9310\n",
            "Epoch 905/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2515 - accuracy: 0.9310\n",
            "Epoch 906/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2490 - accuracy: 0.9540\n",
            "Epoch 907/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2495 - accuracy: 0.9540\n",
            "Epoch 908/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.2487 - accuracy: 0.9310\n",
            "Epoch 909/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2487 - accuracy: 0.9310\n",
            "Epoch 910/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2486 - accuracy: 0.9425\n",
            "Epoch 911/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2470 - accuracy: 0.9425\n",
            "Epoch 912/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2483 - accuracy: 0.9425\n",
            "Epoch 913/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2471 - accuracy: 0.9425\n",
            "Epoch 914/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2474 - accuracy: 0.9425\n",
            "Epoch 915/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2475 - accuracy: 0.9195\n",
            "Epoch 916/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2471 - accuracy: 0.9195\n",
            "Epoch 917/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.2466 - accuracy: 0.9425\n",
            "Epoch 918/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2450 - accuracy: 0.9425\n",
            "Epoch 919/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2473 - accuracy: 0.9425\n",
            "Epoch 920/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2461 - accuracy: 0.9195\n",
            "Epoch 921/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2442 - accuracy: 0.9310\n",
            "Epoch 922/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2453 - accuracy: 0.9425\n",
            "Epoch 923/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2438 - accuracy: 0.9425\n",
            "Epoch 924/1000\n",
            "87/87 [==============================] - 0s 67us/step - loss: 0.2450 - accuracy: 0.9425\n",
            "Epoch 925/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2439 - accuracy: 0.9310\n",
            "Epoch 926/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2440 - accuracy: 0.9425\n",
            "Epoch 927/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2427 - accuracy: 0.9425\n",
            "Epoch 928/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2431 - accuracy: 0.9425\n",
            "Epoch 929/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2436 - accuracy: 0.9425\n",
            "Epoch 930/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2423 - accuracy: 0.9540\n",
            "Epoch 931/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2430 - accuracy: 0.9310\n",
            "Epoch 932/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.2416 - accuracy: 0.9425\n",
            "Epoch 933/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2437 - accuracy: 0.9425\n",
            "Epoch 934/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2407 - accuracy: 0.9425\n",
            "Epoch 935/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2418 - accuracy: 0.9425\n",
            "Epoch 936/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2423 - accuracy: 0.9425\n",
            "Epoch 937/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.2420 - accuracy: 0.9425\n",
            "Epoch 938/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2398 - accuracy: 0.9425\n",
            "Epoch 939/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2404 - accuracy: 0.9425\n",
            "Epoch 940/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2397 - accuracy: 0.9425\n",
            "Epoch 941/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2385 - accuracy: 0.9425\n",
            "Epoch 942/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2389 - accuracy: 0.9425\n",
            "Epoch 943/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2384 - accuracy: 0.9425\n",
            "Epoch 944/1000\n",
            "87/87 [==============================] - 0s 57us/step - loss: 0.2382 - accuracy: 0.9425\n",
            "Epoch 945/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2385 - accuracy: 0.9425\n",
            "Epoch 946/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2369 - accuracy: 0.9425\n",
            "Epoch 947/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2364 - accuracy: 0.9425\n",
            "Epoch 948/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2369 - accuracy: 0.9425\n",
            "Epoch 949/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 950/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 951/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2359 - accuracy: 0.9425\n",
            "Epoch 952/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2360 - accuracy: 0.9425\n",
            "Epoch 953/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2369 - accuracy: 0.9540\n",
            "Epoch 954/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2360 - accuracy: 0.9425\n",
            "Epoch 955/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.2354 - accuracy: 0.9425\n",
            "Epoch 956/1000\n",
            "87/87 [==============================] - 0s 68us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 957/1000\n",
            "87/87 [==============================] - 0s 67us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 958/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2337 - accuracy: 0.9425\n",
            "Epoch 959/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.2335 - accuracy: 0.9425\n",
            "Epoch 960/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2341 - accuracy: 0.9425\n",
            "Epoch 961/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2330 - accuracy: 0.9425\n",
            "Epoch 962/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2339 - accuracy: 0.9425\n",
            "Epoch 963/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2326 - accuracy: 0.9425\n",
            "Epoch 964/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2335 - accuracy: 0.9425\n",
            "Epoch 965/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2323 - accuracy: 0.9425\n",
            "Epoch 966/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2325 - accuracy: 0.9425\n",
            "Epoch 967/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2312 - accuracy: 0.9540\n",
            "Epoch 968/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2313 - accuracy: 0.9540\n",
            "Epoch 969/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2306 - accuracy: 0.9540\n",
            "Epoch 970/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2308 - accuracy: 0.9425\n",
            "Epoch 971/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2299 - accuracy: 0.9425\n",
            "Epoch 972/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2313 - accuracy: 0.9425\n",
            "Epoch 973/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2297 - accuracy: 0.9425\n",
            "Epoch 974/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2307 - accuracy: 0.9425\n",
            "Epoch 975/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2292 - accuracy: 0.9425\n",
            "Epoch 976/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2292 - accuracy: 0.9425\n",
            "Epoch 977/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2287 - accuracy: 0.9425\n",
            "Epoch 978/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2287 - accuracy: 0.9425\n",
            "Epoch 979/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2283 - accuracy: 0.9425\n",
            "Epoch 980/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2277 - accuracy: 0.9425\n",
            "Epoch 981/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2298 - accuracy: 0.9425\n",
            "Epoch 982/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2282 - accuracy: 0.9425\n",
            "Epoch 983/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2311 - accuracy: 0.9425\n",
            "Epoch 984/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2265 - accuracy: 0.9425\n",
            "Epoch 985/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2263 - accuracy: 0.9425\n",
            "Epoch 986/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2265 - accuracy: 0.9425\n",
            "Epoch 987/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2255 - accuracy: 0.9425\n",
            "Epoch 988/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2253 - accuracy: 0.9425\n",
            "Epoch 989/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2262 - accuracy: 0.9425\n",
            "Epoch 990/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2267 - accuracy: 0.9425\n",
            "Epoch 991/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2254 - accuracy: 0.9540\n",
            "Epoch 992/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2260 - accuracy: 0.9425\n",
            "Epoch 993/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2254 - accuracy: 0.9425\n",
            "Epoch 994/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2258 - accuracy: 0.9425\n",
            "Epoch 995/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2242 - accuracy: 0.9425\n",
            "Epoch 996/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2239 - accuracy: 0.9425\n",
            "Epoch 997/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2241 - accuracy: 0.9425\n",
            "Epoch 998/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2232 - accuracy: 0.9425\n",
            "Epoch 999/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2238 - accuracy: 0.9425\n",
            "Epoch 1000/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2225 - accuracy: 0.9540\n",
            "44/44 [==============================] - 0s 606us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-5e636cf8b2b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 score = scorer._score(cached_call, estimator,\n\u001b[0;32m---> 87\u001b[0;31m                                       *args, **kwargs)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 212\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK7gz80tB10x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
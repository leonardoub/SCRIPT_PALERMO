{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_NO_kfold.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJuKtTGmCnf90W9IrOA1kM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Classification_NO_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i4O_AtKVzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb00g1tWTqw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "9cf0141b-69b3-4893-faec-51d1ce6578ad"
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "44293414-7a60-4666-f411-e4d749ea5fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = X_train_1.mean(axis=0)\n",
        "train_data_stand = X_train - mean\n",
        "std = X_train.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaqeWiaXXGy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand = X_val - mean\n",
        "val_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = X_test - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "222a9c5e-ef14-4aba-bf96-820a2bc420a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, y_train).transform(train_data_stand)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "616a1173-cbb4-4fa7-ac09-7e10729a539c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0UwBbCjf13g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = lda.transform(val_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5fsvoQwBqR",
        "colab_type": "text"
      },
      "source": [
        "#Z-score dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8_CwJZwBD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KW2c_RIXpWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = val_data_stand_lda - mean\n",
        "val_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3wGNwiWXvbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLYdHX-mYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAd2LU_dEO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('loss', patience=10, verbose=1, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_sSb3CnLM2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a7379ea-3dd5-4d02-aafe-e5bb8969e992"
      },
      "source": [
        "one_hot_val_labels.shape"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hc4rG203_u",
        "colab_type": "code",
        "outputId": "5c02b472-598b-4927-d038-6f7dfcc469fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 180\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=1)\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/180\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.7584 - acc: 0.9048 - val_loss: 1.5579 - val_acc: 0.2308\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.6654 - acc: 0.9905 - val_loss: 1.4746 - val_acc: 0.2308\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5914 - acc: 1.0000 - val_loss: 1.4319 - val_acc: 0.3077\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5283 - acc: 1.0000 - val_loss: 1.4250 - val_acc: 0.3846\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 965us/step - loss: 0.4720 - acc: 1.0000 - val_loss: 1.4396 - val_acc: 0.4615\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.4212 - acc: 1.0000 - val_loss: 1.4766 - val_acc: 0.4615\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 988us/step - loss: 0.3757 - acc: 1.0000 - val_loss: 1.5305 - val_acc: 0.4615\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3350 - acc: 1.0000 - val_loss: 1.5928 - val_acc: 0.4615\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 1000us/step - loss: 0.2990 - acc: 1.0000 - val_loss: 1.6625 - val_acc: 0.4615\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 983us/step - loss: 0.2673 - acc: 1.0000 - val_loss: 1.7363 - val_acc: 0.4615\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 919us/step - loss: 0.2395 - acc: 1.0000 - val_loss: 1.8116 - val_acc: 0.4615\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2151 - acc: 1.0000 - val_loss: 1.8885 - val_acc: 0.4615\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1939 - acc: 1.0000 - val_loss: 1.9641 - val_acc: 0.4615\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1755 - acc: 1.0000 - val_loss: 2.0395 - val_acc: 0.4615\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1593 - acc: 1.0000 - val_loss: 2.1132 - val_acc: 0.4615\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1452 - acc: 1.0000 - val_loss: 2.1852 - val_acc: 0.4615\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1329 - acc: 1.0000 - val_loss: 2.2551 - val_acc: 0.4615\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1220 - acc: 1.0000 - val_loss: 2.3235 - val_acc: 0.4615\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1125 - acc: 1.0000 - val_loss: 2.3888 - val_acc: 0.4615\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1040 - acc: 1.0000 - val_loss: 2.4533 - val_acc: 0.4615\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0965 - acc: 1.0000 - val_loss: 2.5154 - val_acc: 0.4615\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0898 - acc: 1.0000 - val_loss: 2.5741 - val_acc: 0.4615\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0838 - acc: 1.0000 - val_loss: 2.6303 - val_acc: 0.4615\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0785 - acc: 1.0000 - val_loss: 2.6852 - val_acc: 0.4615\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0736 - acc: 1.0000 - val_loss: 2.7374 - val_acc: 0.4615\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0693 - acc: 1.0000 - val_loss: 2.7881 - val_acc: 0.4615\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 987us/step - loss: 0.0653 - acc: 1.0000 - val_loss: 2.8371 - val_acc: 0.4615\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0617 - acc: 1.0000 - val_loss: 2.8842 - val_acc: 0.4615\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0585 - acc: 1.0000 - val_loss: 2.9293 - val_acc: 0.4615\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0555 - acc: 1.0000 - val_loss: 2.9742 - val_acc: 0.4615\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0527 - acc: 1.0000 - val_loss: 3.0172 - val_acc: 0.4615\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0502 - acc: 1.0000 - val_loss: 3.0590 - val_acc: 0.4615\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0479 - acc: 1.0000 - val_loss: 3.0996 - val_acc: 0.4615\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0457 - acc: 1.0000 - val_loss: 3.1389 - val_acc: 0.4615\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0437 - acc: 1.0000 - val_loss: 3.1771 - val_acc: 0.4615\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0419 - acc: 1.0000 - val_loss: 3.2137 - val_acc: 0.4615\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0401 - acc: 1.0000 - val_loss: 3.2499 - val_acc: 0.4615\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0385 - acc: 1.0000 - val_loss: 3.2849 - val_acc: 0.4615\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0371 - acc: 1.0000 - val_loss: 3.3188 - val_acc: 0.4615\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0357 - acc: 1.0000 - val_loss: 3.3522 - val_acc: 0.4615\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0343 - acc: 1.0000 - val_loss: 3.3848 - val_acc: 0.4615\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0331 - acc: 1.0000 - val_loss: 3.4164 - val_acc: 0.4615\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.0320 - acc: 1.0000 - val_loss: 3.4471 - val_acc: 0.4615\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0309 - acc: 1.0000 - val_loss: 3.4770 - val_acc: 0.4615\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0299 - acc: 1.0000 - val_loss: 3.5061 - val_acc: 0.4615\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0289 - acc: 1.0000 - val_loss: 3.5354 - val_acc: 0.4615\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0280 - acc: 1.0000 - val_loss: 3.5635 - val_acc: 0.4615\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0271 - acc: 1.0000 - val_loss: 3.5912 - val_acc: 0.4615\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0263 - acc: 1.0000 - val_loss: 3.6182 - val_acc: 0.4615\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0255 - acc: 1.0000 - val_loss: 3.6446 - val_acc: 0.4615\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0248 - acc: 1.0000 - val_loss: 3.6706 - val_acc: 0.4615\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0241 - acc: 1.0000 - val_loss: 3.6960 - val_acc: 0.4615\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0234 - acc: 1.0000 - val_loss: 3.7207 - val_acc: 0.4615\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0228 - acc: 1.0000 - val_loss: 3.7449 - val_acc: 0.4615\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0222 - acc: 1.0000 - val_loss: 3.7689 - val_acc: 0.4615\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0216 - acc: 1.0000 - val_loss: 3.7923 - val_acc: 0.4615\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.0211 - acc: 1.0000 - val_loss: 3.8152 - val_acc: 0.4615\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 970us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 3.8377 - val_acc: 0.4615\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0200 - acc: 1.0000 - val_loss: 3.8596 - val_acc: 0.4615\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0196 - acc: 1.0000 - val_loss: 3.8815 - val_acc: 0.4615\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0191 - acc: 1.0000 - val_loss: 3.9028 - val_acc: 0.4615\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 3.9237 - val_acc: 0.4615\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0182 - acc: 1.0000 - val_loss: 3.9441 - val_acc: 0.4615\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 964us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 3.9645 - val_acc: 0.4615\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 981us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 3.9842 - val_acc: 0.4615\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 4.0039 - val_acc: 0.4615\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 987us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 4.0229 - val_acc: 0.4615\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 4.0419 - val_acc: 0.4615\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0160 - acc: 1.0000 - val_loss: 4.0607 - val_acc: 0.4615\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0157 - acc: 1.0000 - val_loss: 4.0789 - val_acc: 0.4615\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 4.0969 - val_acc: 0.4615\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 985us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 4.1146 - val_acc: 0.4615\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0148 - acc: 1.0000 - val_loss: 4.1322 - val_acc: 0.4615\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 987us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 4.1492 - val_acc: 0.4615\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 958us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 4.1663 - val_acc: 0.4615\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 4.1831 - val_acc: 0.4615\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0137 - acc: 1.0000 - val_loss: 4.1996 - val_acc: 0.4615\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 4.2159 - val_acc: 0.4615\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 4.2318 - val_acc: 0.4615\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 981us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 4.2476 - val_acc: 0.4615\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 4.2632 - val_acc: 0.4615\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 4.2786 - val_acc: 0.4615\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0123 - acc: 1.0000 - val_loss: 4.2936 - val_acc: 0.4615\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0121 - acc: 1.0000 - val_loss: 4.3086 - val_acc: 0.4615\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0119 - acc: 1.0000 - val_loss: 4.3236 - val_acc: 0.4615\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 4.3382 - val_acc: 0.4615\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 4.3525 - val_acc: 0.4615\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 4.3665 - val_acc: 0.4615\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 4.3807 - val_acc: 0.4615\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 4.3947 - val_acc: 0.4615\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 4.4085 - val_acc: 0.4615\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 1.0000 - val_loss: 4.4217 - val_acc: 0.4615\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 4.4341 - val_acc: 0.4615\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0104 - acc: 1.0000 - val_loss: 4.4436 - val_acc: 0.4615\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0102 - acc: 1.0000 - val_loss: 4.4531 - val_acc: 0.4615\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 1.0000 - val_loss: 4.4625 - val_acc: 0.4615\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 4.4716 - val_acc: 0.4615\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 1.0000 - val_loss: 4.4808 - val_acc: 0.4615\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 4.4898 - val_acc: 0.4615\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 4.4987 - val_acc: 0.4615\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 1.0000 - val_loss: 4.5076 - val_acc: 0.4615\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 4.5163 - val_acc: 0.4615\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 985us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 4.5249 - val_acc: 0.4615\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 4.5335 - val_acc: 0.4615\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 4.5420 - val_acc: 0.4615\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 4.5503 - val_acc: 0.4615\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 968us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 4.5585 - val_acc: 0.4615\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 936us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 4.5668 - val_acc: 0.4615\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 975us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 4.5749 - val_acc: 0.4615\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0084 - acc: 1.0000 - val_loss: 4.5830 - val_acc: 0.4615\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0083 - acc: 1.0000 - val_loss: 4.5909 - val_acc: 0.4615\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 4.5989 - val_acc: 0.4615\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 4.6067 - val_acc: 0.4615\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 4.6144 - val_acc: 0.4615\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 999us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 4.6221 - val_acc: 0.4615\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 4.6297 - val_acc: 0.4615\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 4.6371 - val_acc: 0.4615\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 4.6447 - val_acc: 0.4615\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 4.6521 - val_acc: 0.4615\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 990us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 4.6594 - val_acc: 0.4615\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0074 - acc: 1.0000 - val_loss: 4.6667 - val_acc: 0.4615\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0073 - acc: 1.0000 - val_loss: 4.6739 - val_acc: 0.4615\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 4.6810 - val_acc: 0.4615\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 4.6881 - val_acc: 0.4615\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0071 - acc: 1.0000 - val_loss: 4.6951 - val_acc: 0.4615\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0070 - acc: 1.0000 - val_loss: 4.7020 - val_acc: 0.4615\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 4.7088 - val_acc: 0.4615\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 4.7158 - val_acc: 0.4615\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 4.7226 - val_acc: 0.4615\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 967us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 4.7292 - val_acc: 0.4615\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 4.7360 - val_acc: 0.4615\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 4.7426 - val_acc: 0.4615\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 4.7491 - val_acc: 0.4615\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 4.7556 - val_acc: 0.4615\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0064 - acc: 1.0000 - val_loss: 4.7619 - val_acc: 0.4615\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 4.7684 - val_acc: 0.4615\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 4.7748 - val_acc: 0.4615\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 999us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 4.7811 - val_acc: 0.4615\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 4.7874 - val_acc: 0.4615\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 946us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 4.7936 - val_acc: 0.4615\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 998us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 4.7998 - val_acc: 0.4615\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 998us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 4.8057 - val_acc: 0.4615\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 964us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 4.8119 - val_acc: 0.4615\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 4.8179 - val_acc: 0.4615\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 4.8237 - val_acc: 0.4615\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 996us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 4.8298 - val_acc: 0.4615\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 4.8356 - val_acc: 0.4615\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 4.8414 - val_acc: 0.4615\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 4.8472 - val_acc: 0.4615\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 4.8530 - val_acc: 0.4615\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 4.8586 - val_acc: 0.4615\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 4.8643 - val_acc: 0.4615\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 4.8699 - val_acc: 0.4615\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 4.8755 - val_acc: 0.4615\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 4.8811 - val_acc: 0.4615\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 966us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 4.8866 - val_acc: 0.4615\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 4.8919 - val_acc: 0.4615\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 980us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 4.8975 - val_acc: 0.4615\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 4.9028 - val_acc: 0.4615\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 964us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 4.9081 - val_acc: 0.4615\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 975us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 4.9135 - val_acc: 0.4615\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 4.9188 - val_acc: 0.4615\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 4.9241 - val_acc: 0.4615\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 4.9293 - val_acc: 0.4615\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 4.9345 - val_acc: 0.4615\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 4.9396 - val_acc: 0.4615\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 4.9447 - val_acc: 0.4615\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 4.9499 - val_acc: 0.4615\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 4.9549 - val_acc: 0.4615\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 4.9600 - val_acc: 0.4615\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 4.9649 - val_acc: 0.4615\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 4.9698 - val_acc: 0.4615\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 957us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 4.9748 - val_acc: 0.4615\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 4.9797 - val_acc: 0.4615\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 4.9846 - val_acc: 0.4615\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 4.9895 - val_acc: 0.4615\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 4.9942 - val_acc: 0.4615\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 935us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 4.9990 - val_acc: 0.4615\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 998us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 5.0038 - val_acc: 0.4615\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 962us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 5.0084 - val_acc: 0.4615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f38022da-6e44-4f98-bbd5-752c0ce7cf65",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd27db4aa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b9f9d14f-200c-4505-9d77-93565b416e50",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd27d68f908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZgU5bn+8e/NJriwCLgEVFCJyjYI\nI5C4r0GjEHdRY3Aj+gtqPIkJJxrj0STnJC5RE44naNwSFTlJVExcooQcNEZlQEBFBaIksogDIoto\nZPT5/VE1Y8/QM9PAdPcMfX+uq6+p5a2qp6p7+ul636q3FBGYmVnpalXsAMzMrLicCMzMSpwTgZlZ\niXMiMDMrcU4EZmYlzonAzKzEORGUAEmtJa2TtHtTli0mSXtLavJrnyUdJWlRxvgbkg7OpexmbOsO\nSd/b3OVLnaQLJP2lgfnPShpTuIharjbFDsA2Jmldxui2wL+AT9Lxr0fEfZuyvoj4BNi+qcuWgojY\npynWI+kC4OyIOCxj3Rc0xbrNtpQTQTMUETVfxOkvzgsi4un6yktqExFVhYjNrDH+PLY8rhpqgST9\nUNKDkh6QtBY4W9IXJD0v6X1JyyTdKqltWr6NpJDUKx3/TTr/cUlrJf1NUu9NLZvOP1bSfEmrJf1c\n0l/rOx3PMcavS1ooaZWkWzOWbS3pZ5JWSnoTGNHA8blS0qQ60yZIuikdvkDSa+n+/D39tV7fuhZL\nOiwd3lbSr9PYXgWG1Cl7laQ30/W+KmlkOn0A8Avg4LTabUXGsb0mY/mL0n1fKelhSbvmcmw25ThX\nxyPpaUnvSXpH0ncytvP99JiskVQh6XPZquEyq13S4zk93c57wFWS+kialm5jRXrcOmUsv0e6j5Xp\n/FsktU9j3i+j3K6S1kvqWt/+ZpQdoaQqb7WkWwBlzGswnpIXEX414xewCDiqzrQfAh8DJ5Ak8w7A\nAcAwkrO8PYH5wLi0fBsggF7p+G+AFUA50BZ4EPjNZpTdCVgLjErn/RuwARhTz77kEuMjQCegF/Be\n9b4D44BXgZ5AV2B68vHNup09gXXAdhnrfhcoT8dPSMsIOAL4EBiYzjsKWJSxrsXAYenwDcBfgC7A\nHsC8OmVPA3ZN35Mz0xh2TuddAPylTpy/Aa5Jh49JYxwEtAf+G/hzLsdmE49zJ2A5cBmwDdARGJrO\n+3dgDtAn3YdBwI7A3nWPNfBs9fuc7lsVcDHQmuTz+HngSKBd+jn5K3BDxv68kh7P7dLyB6bzJgI/\nytjOt4CH6tnPmmOabmMdcCLJZ/GKNKbqGOuNx69wImjuL+pPBH9uZLlvA/+bDmf7cv+fjLIjgVc2\no+x5wDMZ8wQso55EkGOMwzPm/x74djo8naSKrHrecXW/nOqs+3ngzHT4WOCNBsr+AfhGOtxQIvhn\n5nsB/L/MslnW+wrw5XS4sURwD/DjjHkdSdqFejZ2bDbxOH8VmFFPub9Xx1tnei6J4M1GYjilervA\nwcA7QOss5Q4E3gKUjs8GTqpnnZmJ4Dzg2Yx5rRr6LGbG41e4aqgFeztzRNK+kv6YnuqvAa4FujWw\n/DsZw+tpuIG4vrKfy4wjkv+wxfWtJMcYc9oW8I8G4gW4HxidDp+ZjlfHcbykF9JqgvdJfo03dKyq\n7dpQDJLGSJqTVm+8D+yb43oh2b+a9UXEGmAV0COjTE7vWSPHeTeSL/xsGprXmLqfx10kTZa0JI3h\n7joxLIrkwoRaIuKvJL/kD5LUH9gd+GMO26/7WfyUjM9iI/GUPCeClqvupZO/JPkFundEdASuJqOO\nNE+WkfxiBUCSqP3FVdeWxLiM5AukWmOXt04GjpLUg6Tq6v40xg7Ab4H/JKm26Qz8Kcc43qkvBkl7\nAreRVI90Tdf7esZ6G7vUdSlJdVP1+nYgqYJakkNcdTV0nN8G9qpnufrmfZDGtG3GtF3qlKm7fz8h\nudptQBrDmDox7CGpdT1x3AucTXL2Mjki/lVPuUy1Ph+SWpHx2WwknpLnRLD12AFYDXyQNrZ9vQDb\n/AMwWNIJktqQ1Dt3z1OMk4FvSuqRNhx+t6HCEfEOSfXF3STVQgvSWduQ1BNXAp9IOp6k7jjXGL4n\nqbOS+yzGZczbnuTLsJIkJ15IckZQbTnQM7PRto4HgPMlDZS0DUmieiYi6j3DakBDx3kKsLukcZK2\nkdRR0tB03h3ADyXtpcQgSTuSJMB3SC5KaC1pLBlJq4EYPgBWS9qNpHqq2t+AlcCPlTTAd5B0YMb8\nX5NU3ZxJkhRy8QdgkKRR6TG+nNqfxYbiKXlOBFuPbwFfI2m8/SVJo25eRcRy4HTgJpJ/7L2Al0h+\neTV1jLcBU4GXgRkkv+obcz9JnX9NtVBEvE/yJfEQSYPrKSRfIrn4Ackvz0XA42R8SUXEXODnwItp\nmX2AFzKWfQpYACyXlFnFU738EyRVOA+ly+8OnJVjXHXVe5wjYjVwNHAySXKaDxyazr4eeJjkOK8h\nabhtn1b5XQh8j+TCgb3r7Fs2PwCGkiSkKcDvMmKoAo4H9iM5O/gnyftQPX8Ryfv8r4h4Lpcdzvgs\nXp/GuHudGOuNxz5rkDHbYump/lLglIh4ptjxWMsl6V6SBuhrih1LKfANZbZFJI0guULnQ5LLDzeQ\n/Co22yxpe8soYECxYykVrhqyLXUQ8CZJ3fiXgBNzbNwz24ik/yS5l+HHEfHPYsdTKlw1ZGZW4nxG\nYGZW4lpcG0G3bt2iV69exQ7DzKxFmTlz5oqIyHp5d4tLBL169aKioqLYYZiZtSiS6r0b31VDZmYl\nzonAzKzEORGYmZU4JwIzsxLnRGBmVuLylggk3SnpXUmv1DNf6aPtFkqaK2lwvmIxM7P65fOM4G4a\neK4syVOj+qSvsSS9S5qZWYHl7T6CiJiu9AHo9RgF3Jt2cft82sf7rhGxLF8xfRYb3H03vPVWvrdk\nZtZ0TjgBDjig6ddbzBvKelD78XaL02kbJYL0QRhjAXbfvbEHUzXu9tvh61+vXvcWr87MrCA+97mt\nLxHkLCImkjwkg/Ly8i3qJW/hQrj8cjjqKHjySWjl5nIzK3HF/BpcQu3nv/Zk857Pukm+8x1o1w7u\nustJwMwMipsIpgDnpFcPDQdWF6J9YMECOOww6Nmz0aJmZiUhb1VDkh4ADgO6SVpM8szQtgAR8T/A\nY8BxwEJgPXBuvmLJtGIFDB9eiC2ZmbUM+bxqaHQj8wP4Rr62n32bsHIldOtWyK2amTVvJVVLvnYt\nbNgAXbsWOxIzs+ajpBLBihXJX58RmJl9xonAzKzElVQiWLky+etEYGb2mZJKBD4jMDPbWEkmAjcW\nm5l9puQSQevW0KlTsSMxM2s+SioRrFyZnA24awkzs8+U1FfiihVuHzAzq6vkEoHbB8zMaiu5ROAz\nAjOz2koqEbifITOzjZVMIojwGYGZWTYlkwjWrIGqKrcRmJnVVTKJwHcVm5llVzKJwP0MmZllVzKJ\nwGcEZmbZ5TURSBoh6Q1JCyWNzzJ/D0lTJc2V9BdJeXuSsPsZMjPLLm+JQFJrYAJwLNAXGC2pb51i\nNwD3RsRA4FrgP/MVj88IzMyyy+cZwVBgYUS8GREfA5OAUXXK9AX+nA5PyzK/yRx4IFx3nTucMzOr\nK5+JoAfwdsb44nRapjnASenwicAOkjaqvJE0VlKFpIrKysrNCmbYMLjqKpA2a3Ezs61WsRuLvw0c\nKukl4FBgCfBJ3UIRMTEiyiOivHv37oWO0cxsq9Ymj+teAuyWMd4znVYjIpaSnhFI2h44OSLez2NM\nZmZWRz7PCGYAfST1ltQOOAOYkllAUjdJ1TH8O3BnHuMxM7Ms8pYIIqIKGAc8CbwGTI6IVyVdK2lk\nWuww4A1J84GdgR/lKx4zM8tOEVHsGDZJeXl5VFRUFDsMM7MWRdLMiCjPNq/YjcVmZlZkTgRmZiXO\nicDMrMQ5EZiZlTgnAjOzEudEYGZW4pwIzMxKnBOBmVmJcyIwMytxTgRmZiXOicDMrMQ5EZiZlTgn\nAjOzEudEYGZW4pwIzMxKnBOBmVmJy2sikDRC0huSFkoan2X+7pKmSXpJ0lxJx+UzHjMz21jeEoGk\n1sAE4FigLzBaUt86xa4ieYTl/iTPNP7vfMVjZmbZ5fOMYCiwMCLejIiPgUnAqDplAuiYDncCluYx\nHjMzy6JNHtfdA3g7Y3wxMKxOmWuAP0m6BNgOOCqP8ZiZWRbFbiweDdwdET2B44BfS9ooJkljJVVI\nqqisrCx4kGZmW7N8JoIlwG4Z4z3TaZnOByYDRMTfgPZAt7orioiJEVEeEeXdu3fPU7hmZqUpn4lg\nBtBHUm9J7Ugag6fUKfNP4EgASfuRJAL/5DczK6C8JYKIqALGAU8Cr5FcHfSqpGsljUyLfQu4UNIc\n4AFgTEREvmIyM7ON5bOxmIh4DHiszrSrM4bnAQfmMwYzM2tYsRuLzcysyJwIzMxKnBOBmVmJcyIw\nMytxTgRmZiXOicDMrMQ5EZiZlTgnAjOzEudEYGZW4pwIzMxKnBOBmVmJcyIwMytxTgRmZiXOicDM\nrMQ5EZiZlTgnAjOzEudEYGZW4vKaCCSNkPSGpIWSxmeZ/zNJs9PXfEnv5zMeMzPbWN4eVSmpNTAB\nOBpYDMyQNCV9PCUAEXF5RvlLgP3zFY+ZmWWXzzOCocDCiHgzIj4GJgGjGig/muQB9mZmVkD5TAQ9\ngLczxhen0zYiaQ+gN/DneuaPlVQhqaKysrLJAzUzK2XNpbH4DOC3EfFJtpkRMTEiyiOivHv37gUO\nzcxs69ZoIpB0iaQum7HuJcBuGeM902nZnIGrhczMiiKXM4KdSRp6J6dXASnHdc8A+kjqLakdyZf9\nlLqFJO0LdAH+lmvQZmbWdBpNBBFxFdAH+BUwBlgg6ceS9mpkuSpgHPAk8BowOSJelXStpJEZRc8A\nJkVEbOY+mJnZFsjp8tGICEnvAO8AVSS/4H8r6amI+E4Dyz0GPFZn2tV1xq/Z1KDNzKzpNJoIJF0G\nnAOsAO4AroiIDZJaAQuAehOBmZk1f7mcEewInBQR/8icGBGfSjo+P2GZmVmh5NJY/DjwXvWIpI6S\nhgFExGv5CszMzAojl0RwG7AuY3xdOs3MzLYCuSQCZV7RExGfksc+iszMrLBySQRvSrpUUtv0dRnw\nZr4DMzOzwsglEVwEfJHkruDFwDBgbD6DMjOzwmm0iici3iW56cvMzLZCudxH0B44H+gHtK+eHhHn\n5TEuMzMrkFyqhn4N7AJ8Cfg/ks7j1uYzKDMzK5xcEsHeEfF94IOIuAf4Mkk7gZmZbQVySQQb0r/v\nS+oPdAJ2yl9IZmZWSLncDzAxfR7BVSTdSG8PfD+vUZmZWcE0mAjSjuXWRMQqYDqwZ0GiMjOzgmmw\naii9i9i9i5qZbcVyaSN4WtK3Je0macfqV94jMzOzgsiljeD09O83MqYFriYyM9sq5PKoyt5ZXjkl\ngfQZx29IWihpfD1lTpM0T9Krku7f1B0wM7Mtk8udxedkmx4R9zayXGtgAnA0SR9FMyRNiYh5GWX6\nAP8OHBgRqyT5slQzswLLpWrogIzh9sCRwCygwUQADAUWRsSbAJImAaOAeRllLgQmpFclVfdrZGZm\nBZRLp3OXZI5L6gxMymHdPYC3M8arey7N9Pl0nX8FWgPXRMQTdVckaSxpj6e77757Dps2M7Nc5XLV\nUF0fAL2baPttgD7AYcBo4PY00dQSERMjojwiyrt3795EmzYzM8itjeBRkquEIEkcfYHJOax7CbBb\nxnjPdFqmxcALEbEBeEvSfJLEMCOH9ZuZWRPIpY3ghozhKuAfEbE4h+VmAH0k9SZJAGcAZ9Yp8zDJ\nmcBdkrqRVBX56WdmZgWUSyL4J7AsIj4CkNRBUq+IWNTQQhFRJWkc8CRJ/f+dEfGqpGuBioiYks47\nRtI84BPgiohYuQX7Y2Zmm0gZz6XPXkCqAL4YER+n4+2Av0bEAQ0umCfl5eVRUVFRjE2bmbVYkmZG\nRHm2ebk0FrepTgIA6XC7pgrOzMyKK5dEUClpZPWIpFHAivyFZGZmhZRLG8FFwH2SfpGOLway3m1s\nZmYtTy43lP0dGC5p+3R8Xd6jMjOzgmm0akjSjyV1joh1EbFOUhdJPyxEcGZmln+5tBEcGxHvV4+k\n/QIdl7+QzMyskHJJBK0lbVM9IqkDsE0D5c3MrAXJpbH4PmCqpLsAAWOAe/IZlJmZFU4ujcU/kTQH\nOIqkz6EngT3yHZiZmRVGrr2PLidJAqcCRwCv5S0iMzMrqHrPCCR9nqRDuNEkN5A9SNIlxeEFis3M\nzAqgoaqh14FngOMjYiGApMsLEpWZmRVMQ1VDJwHLgGmSbpd0JEljsZmZbUXqTQQR8XBEnAHsC0wD\nvgnsJOk2SccUKkAzM8uvRhuLI+KDiLg/Ik4gecrYS8B38x6ZmZkVxCY9szgiVqXPDz4yXwGZmVlh\nbc7D683MbCuS10QgaYSkNyQtlDQ+y/wxkiolzU5fF+QzHjMz21guXUxsFkmtgQnA0STPMJghaUpE\nzKtT9MGIGJevOMzMrGH5PCMYCiyMiDfTx1tOAkblcXtmZrYZ8pkIegBvZ4wvTqfVdbKkuZJ+K2m3\nbCuSNFZShaSKysrKfMRqZlayit1Y/CjQKyIGAk9RT6+m6ZVK5RFR3r1794IGaGa2tctnIlgCZP7C\n75lOqxERKyPiX+noHcCQPMZjZmZZ5DMRzAD6SOotqR1wBjAls4CkXTNGR+JeTc3MCi5vVw1FRJWk\ncSTPL2gN3BkRr0q6FqiIiCnApZJGAlXAeyQPvTEzswJSRBQ7hk1SXl4eFRUVxQ7DzKxFkTQzIsqz\nzSt2Y7GZmRWZE4GZWYnLWxtBi1JVBf/4R7GjMDNrWLdu0KlTk6/WiQDg0kvhttuKHYWZWcNuuw0u\nuqjJV+tEADB/PuyzD1x5ZbEjMTOr37BheVmtEwHAu+9C377w1a8WOxIzs4JzYzEkiWCnnYodhZlZ\nUTgRfPopVFY6EZhZyXIiWLkySQZOBGZWopwI3n03+etEYGYlyonAicDMSpwTgROBmZU4JwInAjMr\ncU4E774LrVrBjjsWOxIzs6JwInj3XejePUkGZmYlyN9+vpnMzEpcXhOBpBGS3pC0UNL4BsqdLCkk\nZX1oQl45EZhZictbIpDUGpgAHAv0BUZL6pul3A7AZcAL+YqlQU4EZlbi8nlGMBRYGBFvRsTHwCRg\nVJZy1wE/AT7KYyz1cyIwsxKXz0TQA3g7Y3xxOq2GpMHAbhHxx4ZWJGmspApJFZWVlU0X4UcfwZo1\nTgRmVtKK1lgsqRVwE/CtxspGxMSIKI+I8u7duzddENVJxYnAzEpYPhPBEmC3jPGe6bRqOwD9gb9I\nWgQMB6YUtMHYN5OZmeU1EcwA+kjqLakdcAYwpXpmRKyOiG4R0SsiegHPAyMjoiKPMdXmRGBmlr9E\nEBFVwDjgSeA1YHJEvCrpWkkj87XdTeJEYGaW30dVRsRjwGN1pl1dT9nD8hlLVk4EZmYlfmfxu+9C\nhw6w3XbFjsTMrGicCHbaCaRiR2JmVjROBK4WMrMS50TgRGBmJc6JwInAzEpc6SaCCCcCMzNKORGs\nWQMff+xEYGYlr3QTge8hMDMDnAicCMys5DkROBGYWYnLaxcTzZoTgbVwGzZsYPHixXz0UXGe6WTN\nU/v27enZsydt27bNeRkngm7dihuH2WZavHgxO+ywA7169UK+O96AiGDlypUsXryY3r1757xcaVcN\ndekC7doVOxKzzfLRRx/RtWtXJwGrIYmuXbtu8lliaScCVwtZC+ckYHVtzmfCicDMrMQ5EZjZZlm5\nciWDBg1i0KBB7LLLLvTo0aNm/OOPP85pHeeeey5vvPFGg2UmTJjAfffd1xQhWz1Ku7H40EOLHYVZ\ni9W1a1dmz54NwDXXXMP222/Pt7/97VplIoKIoFWr7L8577rrrka3841vfGPLgy2wqqoq2rRpOV+v\neT0jkDRC0huSFkoan2X+RZJeljRb0rOS+uYznhpVVbBypc8IbKvxzW/CYYc17eub39y8WBYuXEjf\nvn0566yz6NevH8uWLWPs2LGUl5fTr18/rr322pqyBx10ELNnz6aqqorOnTszfvx4ysrK+MIXvsC7\n6ZV9V111FTfffHNN+fHjxzN06FD22WcfnnvuOQA++OADTj75ZPr27cspp5xCeXl5TZLK9IMf/IAD\nDjiA/v37c9FFFxERAMyfP58jjjiCsrIyBg8ezKJFiwD48Y9/zIABAygrK+PKK6+sFTPAO++8w957\n7w3AHXfcwVe+8hUOP/xwvvSlL7FmzRqOOOIIBg8ezMCBA/nDH/5QE8ddd93FwIEDKSsr49xzz2X1\n6tXsueeeVFVVAbBq1apa4/mWt0QgqTUwATgW6AuMzvJFf39EDIiIQcBPgZvyFU8tK1cmnc45EZjl\nxeuvv87ll1/OvHnz6NGjB//1X/9FRUUFc+bM4amnnmLevHkbLbN69WoOPfRQ5syZwxe+8AXuvPPO\nrOuOCF588UWuv/76mqTy85//nF122YV58+bx/e9/n5deeinrspdddhkzZszg5ZdfZvXq1TzxxBMA\njB49mssvv5w5c+bw3HPPsdNOO/Hoo4/y+OOP8+KLLzJnzhy+9a1vNbrfL730Er///e+ZOnUqHTp0\n4OGHH2bWrFk8/fTTXH755QDMmTOHn/zkJ/zlL39hzpw53HjjjXTq1IkDDzywJp4HHniAU089tWBn\nFfncylBgYUS8CSBpEjAKqPkERMSajPLbAZHHeD7jm8lsK5P+YG429tprL8rLy2vGH3jgAX71q19R\nVVXF0qVLmTdvHn371v5d2KFDB4499lgAhgwZwjPPPJN13SeddFJNmepf7s8++yzf/e53ASgrK6Nf\nv35Zl506dSrXX389H330EStWrGDIkCEMHz6cFStWcMIJJwDJDVkATz/9NOeddx4dOnQAYMcdd2x0\nv4855hi6dOkCJAlr/PjxPPvss7Rq1Yq3336bFStW8Oc//5nTTz+9Zn3Vfy+44AJuvfVWjj/+eO66\n6y5+/etfN7q9ppLPRNADeDtjfDEwrG4hSd8A/g1oBxyRbUWSxgJjAXbfffctj8yJwCyvtst4DviC\nBQu45ZZbePHFF+ncuTNnn3121uvc22Xc09O6det6q0W22WabRstks379esaNG8esWbPo0aMHV111\n1Wbdld2mTRs+/fRTgI2Wz9zve++9l9WrVzNr1izatGlDz549G9zeoYceyrhx45g2bRpt27Zl3333\n3eTYNlfRrxqKiAkRsRfwXeCqespMjIjyiCjv3r37lm/UicCsYNasWcMOO+xAx44dWbZsGU8++WST\nb+PAAw9k8uTJALz88stZq54+/PBDWrVqRbdu3Vi7di2/+93vAOjSpQvdu3fn0UcfBZIv9/Xr13P0\n0Udz55138uGHHwLw3nvvAdCrVy9mzpwJwG9/+9t6Y1q9ejU77bQTbdq04amnnmLJkiUAHHHEETz4\n4IM166v+C3D22Wdz1llnce65527R8dhU+UwES4DdMsZ7ptPqMwn4Sh7j+YwTgVnBDB48mL59+7Lv\nvvtyzjnncOCBBzb5Ni655BKWLFlC3759+Y//+A/69u1Lp06dapXp2rUrX/va1+jbty/HHnssw4Z9\nVkFx3333ceONNzJw4EAOOuggKisrOf744xkxYgTl5eUMGjSIn/3sZwBcccUV3HLLLQwePJhVq1bV\nG9NXv/pVnnvuOQYMGMCkSZPo06cPkFRdfec73+GQQw5h0KBBXHHFFTXLnHXWWaxevZrTTz+9KQ9P\n46ov72rqF0m105tAb5JqnzlAvzpl+mQMnwBUNLbeIUOGxBb73vci2rSJ+OSTLV+XWZHMmzev2CE0\nGxs2bIgPP/wwIiLmz58fvXr1ig0bNhQ5qk33wAMPxJgxY7Z4Pdk+Gw19v+atjSAiqiSNA54EWgN3\nRsSrkq5NA5oCjJN0FLABWAV8LV/x1LJ8OXTvDvVc22xmLcu6des48sgjqaqqIiL45S9/2aKu4we4\n+OKLefrpp2uuHCqkvB6piHgMeKzOtKszhi/L5/br5buKzbYqnTt3rqm3b6luu+22om27NH8SOxGY\nmdVwIjAzK3FOBGZmJa70EsEHHyQvJwIzM6AUE0FlZfLXicBsixx++OEb3Rx28803c/HFFze43Pbb\nbw/A0qVLOeWUU7KWOeyww6ioqGhwPTfffDPr16+vGT/uuON4//33cwnd6ii9ROCbycyaxOjRo5k0\naVKtaZMmTWL06NE5Lf+5z32uwTtzG1M3ETz22GN07tx5s9dXaBFR01VFsTkRmG0NitAP9SmnnMIf\n//jHmofQLFq0iKVLl3LwwQfXXNc/ePBgBgwYwCOPPLLR8osWLaJ///5A0v3DGWecwX777ceJJ55Y\n060DJNfXV3dh/YMf/ACAW2+9laVLl3L44Ydz+OGHA0nXDytWrADgpptuon///vTv37+mC+tFixax\n3377ceGFF9KvXz+OOeaYWtup9uijjzJs2DD2339/jjrqKJYvXw4k9yqce+65DBgwgIEDB9Z0UfHE\nE08wePBgysrKOPLII4Hk+Qw33HBDzTr79+/PokWLWLRoEfvssw/nnHMO/fv35+233866fwAzZszg\ni1/8ImVlZQwdOpS1a9dyyCGH1Ope+6CDDmLOnDkNvk+5aFl3XDQFJwKzJrHjjjsydOhQHn/8cUaN\nGsWkSZM47bTTkET79u156KGH6NixIytWrGD48OGMHDmy3ufp3nbbbWy77ba89tprzJ07l8GDB9fM\n+9GPfsSOO+7IJ598wpFHHsncuXO59NJLuemmm5g2bRrdunWrta6ZM2dy11138cILLxARDBs2jEMP\nPZQuXbqwYMECHnjgAW6//XZOO+00fve733H22WfXWv6ggw7i+eefRxJ33HEHP/3pT7nxxhu57rrr\n6NSpEy+//DKQPDOgsrKSCy+8kOnTp9O7d+9a/QbVZ8GCBdxzzz0MHz683v3bd999Of3003nwwQc5\n4IADWLNmDR06dOD888/n7rvv5uabb2b+/Pl89NFHlJWVbdL7lk3pJoKm6LzOrLkoUj/U1dVD1Yng\nV7/6FZBUe3zve99j+vTptMUXYBgAAAsISURBVGrViiVLlrB8+XJ22WWXrOuZPn06l156KQADBw5k\n4MCBNfMmT57MxIkTqaqqYtmyZcybN6/W/LqeffZZTjzxxJqeQE866SSeeeYZRo4cSe/evRk0aBBQ\nuxvrTIsXL+b0009n2bJlfPzxx/Tu3RtIuqXOrArr0qULjz76KIccckhNmVy6qt5jjz1qkkB9+yeJ\nXXfdlQMOOACAjh07AnDqqady3XXXcf3113PnnXcyZsyYRreXi9KsGtpuu+RlZltk1KhRTJ06lVmz\nZrF+/XqGDBkCJJ24VVZWMnPmTGbPns3OO++8WV0+v/XWW9xwww1MnTqVuXPn8uUvf3mz1lOtugtr\nqL8b60suuYRx48bx8ssv88tf/nKLu6qG2t1VZ3ZVvan7t+2223L00UfzyCOPMHnyZM4666xNji2b\n0kwErhYyaxLbb789hx9+OOedd16tRuLqLpjbtm3LtGnT+Mc//tHgeg455BDuv/9+AF555RXmzp0L\nJF1Yb7fddnTq1Inly5fz+OOP1yyzww47sHbt2o3WdfDBB/Pwww+zfv16PvjgAx566CEOPvjgnPdp\n9erV9OjRA4B77rmnZvrRRx/NhAkTasZXrVrF8OHDmT59Om+99RZQu6vqWbNmATBr1qya+XXVt3/7\n7LMPy5YtY8aMGQCsXbu2JmldcMEFXHrppRxwwAE1D8HZUqWTCO68E/r1g0cecbWQWRMaPXo0c+bM\nqZUIzjrrLCoqKhgwYAD33ntvow9Zufjii1m3bh377bcfV199dc2ZRVlZGfvvvz/77rsvZ555Zq0u\nrMeOHcuIESNqGourDR48mDFjxjB06FCGDRvGBRdcwP7775/z/lxzzTWceuqpDBkypFb7w1VXXcWq\nVavo378/ZWVlTJs2je7duzNx4kROOukkysrKarqPPvnkk3nvvffo168fv/jFL/j85z+fdVv17V+7\ndu148MEHueSSSygrK+Poo4+uOVMYMmQIHTt2bNJnFiiiME+HbCrl5eXR2PXFWT3yCPzmN8nwKadA\nofv7Nmtir732Gvvtt1+xw7ACW7p0KYcddhivv/46rerpQTnbZ0PSzIgoz1a+dBqLR41KXmZmLdS9\n997LlVdeyU033VRvEtgcpZMIzMxauHPOOYdzzjmnyddbOm0EZluhlla1a/m3OZ8JJwKzFqp9+/as\nXLnSycBqRAQrV66kffv2m7RcXquGJI0AbiF5VOUdEfFfdeb/G3ABUAVUAudFRMPXmZkZAD179mTx\n4sVUVnekaEbyA6Fnz56btEzeEoGk1sAE4GhgMTBD0pSImJdR7CWgPCLWS7oY+Cngy3nMctC2bdua\nO1rNtkQ+q4aGAgsj4s2I+BiYBNS6bCcipkVEdfeBzwOblsbMzGyL5TMR9ADezhhfnE6rz/nA49lm\nSBorqUJShU+DzcyaVrNoLJZ0NlAOXJ9tfkRMjIjyiCjv7ruCzcyaVD4bi5cAu2WM90yn1SLpKOBK\n4NCI+FdjK505c+YKSZvToNwNWLEZyxVaS4kTWk6sLSVOaDmxtpQ4oeXEmu8496hvRt66mJDUBpgP\nHEmSAGYAZ0bEqxll9gd+C4yIiAV5CeSzbVXUd3t1c9JS4oSWE2tLiRNaTqwtJU5oObEWM868VQ1F\nRBUwDngSeA2YHBGvSrpW0si02PXA9sD/SpotaUq+4jEzs+zyeh9BRDwGPFZn2tUZw0flc/tmZta4\nZtFYXCATix1AjlpKnNByYm0pcULLibWlxAktJ9aixdniuqE2M7OmVUpnBGZmloUTgZlZidvqE4Gk\nEZLekLRQ0vhix1NN0m6SpkmaJ+lVSZel06+RtCS9imq2pOOKHSuApEWSXk5jqkin7SjpKUkL0r9N\n8wDVLYtzn4xjN1vSGknfbA7HVdKdkt6V9ErGtKzHUIlb08/tXEmDm0Gs10t6PY3nIUmd0+m9JH2Y\ncWz/p8hx1vteS/r39Ji+IelLhYqzgVgfzIhzkaTZ6fTCHtOI2GpfJL2e/h3YE2gHzAH6FjuuNLZd\ngcHp8A4k91z0Ba4Bvl3s+LLEuwjoVmfaT4Hx6fB44CfFjjPL+/8OyY00RT+uwCHAYOCVxo4hcBxJ\nlysChgMvNINYjwHapMM/yYi1V2a5ZhBn1vc6/f+aA2wD9E6/G1oXM9Y6828Eri7GMd3azwga7fiu\nWCJiWUTMSofXktxr0VBfTM3RKOCedPge4CtFjCWbI4G/RzPp2jwipgPv1Zlc3zEcBdwbieeBzpJ2\nLUyk2WONiD9Fcn8QNJNOIus5pvUZBUyKiH9FxFvAQpLviIJoKFZJAk4DHihUPJm29kSwqR3fFYWk\nXsD+wAvppHHp6fedzaG6JRXAnyTNlDQ2nbZzRCxLh98Bdi5OaPU6g9r/WM3xuNZ3DJv7Z/c8ancS\n2VvSS5L+T9LBxQoqQ7b3ujkf04OB5VG7h4WCHdOtPRE0e5K2B34HfDMi1gC3AXsBg4BlJKeLzcFB\nETEYOBb4hqRDMmdGcj7bbK5FltQOGAn8bzqpuR7XGs3tGNZH0pUkD5O6L520DNg9IvYH/g24X1LH\nYsVHC3ivsxhN7R8tBT2mW3siyKnju2KR1JYkCdwXEb8HiIjlEfFJRHwK3E4BT10bEhFL0r/vAg+R\nxLW8uroi/ftu8SLcyLHArIhYDs33uFL/MWyWn11JY4DjgbPSxEVa1bIyHZ5JUvf++WLF2MB73VyP\naRvgJODB6mmFPqZbeyKYAfSR1Dv9hXgG0Cz6M0rrBH8FvBYRN2VMz6wHPhF4pe6yhSZpO0k7VA+T\nNBq+QnIsv5YW+xrwSHEizKrWL6zmeFxT9R3DKcA56dVDw4HVGVVIRaHk0bPfAUbGZw+UQlJ3JU8k\nRNKeQB/gzeJE2eB7PQU4Q9I2knqTxPlioePL4ijg9YhYXD2h4Me0UK3SxXqRXH0xnySjXlnseDLi\nOoikGmAuMDt9HQf8Gng5nT4F2LUZxLonydUWc4BXq48j0BWYCiwAngZ2LHasaVzbASuBThnTin5c\nSRLTMmADSf30+fUdQ5KrhSakn9uXSR7pWuxYF5LUsVd/Xv8nLXty+rmYDcwCTihynPW+1yRd3v8d\neAM4ttjHNJ1+N3BRnbIFPabuYsLMrMRt7VVDZmbWCCcCM7MS50RgZlbinAjMzEqcE4GZWYlzIjBL\nSfpEtXsubbLeatPeJJvLvQtmteT1mcVmLcyHETGo2EGYFZrPCMwakfYT/1Mlz2N4UdLe6fRekv6c\ndm42VdLu6fSd0/7656SvL6arai3pdiXPn/iTpA5p+UuVPJdirqRJRdpNK2FOBGaf6VCnauj0jHmr\nI2IA8Avg5nTaz4F7ImIgSQdst6bTbwX+LyLKSPqffzWd3geYEBH9gPdJ7h6F5DkE+6fruShfO2dW\nH99ZbJaStC4its8yfRFwRES8mXYU+E5EdJW0gqT7gg3p9GUR0U1SJdAzIv6VsY5ewFMR0Scd/y7Q\nNiJ+KOkJYB3wMPBwRKzL866a1eIzArPcRD3Dm+JfGcOf8Fkb3ZdJ+hUaDMxIe6M0KxgnArPcnJ7x\n92/p8HMkPdoCnAU8kw5PBS4GkNRaUqf6ViqpFbBbREwDvgt0AjY6KzHLJ//yMPtMh+qHh6eeiIjq\nS0i7SJpL8qt+dDrtEuAuSVcAlcC56fTLgImSzif55X8xSa+T2bQGfpMmCwG3RsT7TbZHZjlwG4FZ\nI9I2gvKIWFHsWMzywVVDZmYlzmcEZmYlzmcEZmYlzonAzKzEORGYmZU4JwIzsxLnRGBmVuL+P/Mo\nGaVhwIGhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dbf3be95-3d26-4190-ca35-2cdcb8463887",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=1, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/180\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.9294 - acc: 0.4952\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 921us/step - loss: 0.8590 - acc: 0.4762\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 889us/step - loss: 0.8044 - acc: 0.4762\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.7607 - acc: 0.4952\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.7243 - acc: 0.5143\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 961us/step - loss: 0.6932 - acc: 0.5524\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 952us/step - loss: 0.6658 - acc: 0.6476\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 920us/step - loss: 0.6414 - acc: 0.7524\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 990us/step - loss: 0.6194 - acc: 0.7905\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 951us/step - loss: 0.5993 - acc: 0.8571\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 962us/step - loss: 0.5806 - acc: 0.8667\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.5633 - acc: 0.8667\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5471 - acc: 0.8667\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.5319 - acc: 0.8667\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.5176 - acc: 0.8667\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 941us/step - loss: 0.5039 - acc: 0.8667\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.4910 - acc: 0.8667\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 885us/step - loss: 0.4786 - acc: 0.8762\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.4669 - acc: 0.8762\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.4556 - acc: 0.8762\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.4448 - acc: 0.8762\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.4343 - acc: 0.8762\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.4242 - acc: 0.8762\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 967us/step - loss: 0.4144 - acc: 0.8762\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.4049 - acc: 0.8857\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 916us/step - loss: 0.3958 - acc: 0.8857\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.3869 - acc: 0.8857\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 965us/step - loss: 0.3782 - acc: 0.8857\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3698 - acc: 0.8857\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.3616 - acc: 0.8857\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 919us/step - loss: 0.3535 - acc: 0.8857\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3457 - acc: 0.8857\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.3381 - acc: 0.8857\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.3306 - acc: 0.8857\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 959us/step - loss: 0.3233 - acc: 0.8857\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3160 - acc: 0.8857\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.3088 - acc: 0.8857\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3017 - acc: 0.8857\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2948 - acc: 0.8857\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2881 - acc: 0.8857\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 971us/step - loss: 0.2814 - acc: 0.8857\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.2750 - acc: 0.8857\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2687 - acc: 0.8857\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.2625 - acc: 0.8857\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.2566 - acc: 0.8857\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2507 - acc: 0.8857\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2451 - acc: 0.8857\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2396 - acc: 0.8857\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 975us/step - loss: 0.2342 - acc: 0.8857\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2290 - acc: 0.8857\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 992us/step - loss: 0.2239 - acc: 0.8857\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2190 - acc: 0.8857\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 933us/step - loss: 0.2143 - acc: 0.8857\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.2096 - acc: 0.8857\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2051 - acc: 0.8857\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 943us/step - loss: 0.2008 - acc: 0.8857\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.1965 - acc: 0.8857\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1925 - acc: 0.8857\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 929us/step - loss: 0.1885 - acc: 0.8857\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 904us/step - loss: 0.1846 - acc: 0.8857\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 925us/step - loss: 0.1808 - acc: 0.8857\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1772 - acc: 0.8857\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 937us/step - loss: 0.1737 - acc: 0.8857\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 980us/step - loss: 0.1703 - acc: 0.8857\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1669 - acc: 0.8857\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.1637 - acc: 0.8857\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 971us/step - loss: 0.1606 - acc: 0.8857\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.1575 - acc: 0.8857\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1546 - acc: 0.8857\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 975us/step - loss: 0.1517 - acc: 0.8857\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 957us/step - loss: 0.1489 - acc: 0.8857\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 981us/step - loss: 0.1462 - acc: 0.8857\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1436 - acc: 0.8857\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1410 - acc: 0.8857\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.1385 - acc: 0.8857\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.1361 - acc: 0.8857\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 970us/step - loss: 0.1338 - acc: 0.8857\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 958us/step - loss: 0.1314 - acc: 0.8857\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.1292 - acc: 0.8857\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.1270 - acc: 0.8857\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 957us/step - loss: 0.1249 - acc: 0.9429\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1228 - acc: 1.0000\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1208 - acc: 1.0000\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.1189 - acc: 1.0000\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.1170 - acc: 1.0000\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1151 - acc: 1.0000\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 970us/step - loss: 0.1133 - acc: 1.0000\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.1115 - acc: 1.0000\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 930us/step - loss: 0.1098 - acc: 1.0000\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 994us/step - loss: 0.1081 - acc: 1.0000\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1064 - acc: 1.0000\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 944us/step - loss: 0.1048 - acc: 1.0000\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.1032 - acc: 1.0000\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1017 - acc: 1.0000\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 945us/step - loss: 0.1002 - acc: 1.0000\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.0987 - acc: 1.0000\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 977us/step - loss: 0.0973 - acc: 1.0000\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 986us/step - loss: 0.0959 - acc: 1.0000\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 961us/step - loss: 0.0945 - acc: 1.0000\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 999us/step - loss: 0.0932 - acc: 1.0000\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0919 - acc: 1.0000\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0906 - acc: 1.0000\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 882us/step - loss: 0.0893 - acc: 1.0000\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 945us/step - loss: 0.0881 - acc: 1.0000\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0869 - acc: 1.0000\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0858 - acc: 1.0000\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.0846 - acc: 1.0000\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0835 - acc: 1.0000\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 935us/step - loss: 0.0824 - acc: 1.0000\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 887us/step - loss: 0.0813 - acc: 1.0000\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.0802 - acc: 1.0000\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0792 - acc: 1.0000\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.0782 - acc: 1.0000\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.0772 - acc: 1.0000\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.0762 - acc: 1.0000\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0753 - acc: 1.0000\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0744 - acc: 1.0000\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.0734 - acc: 1.0000\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.0726 - acc: 1.0000\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.0717 - acc: 1.0000\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0708 - acc: 1.0000\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.0700 - acc: 1.0000\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 964us/step - loss: 0.0691 - acc: 1.0000\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.0683 - acc: 1.0000\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 988us/step - loss: 0.0675 - acc: 1.0000\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 928us/step - loss: 0.0668 - acc: 1.0000\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.0660 - acc: 1.0000\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0652 - acc: 1.0000\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 961us/step - loss: 0.0645 - acc: 1.0000\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 936us/step - loss: 0.0638 - acc: 1.0000\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 936us/step - loss: 0.0631 - acc: 1.0000\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0624 - acc: 1.0000\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 905us/step - loss: 0.0617 - acc: 1.0000\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 941us/step - loss: 0.0610 - acc: 1.0000\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.0603 - acc: 1.0000\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.0597 - acc: 1.0000\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 960us/step - loss: 0.0591 - acc: 1.0000\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.0584 - acc: 1.0000\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 981us/step - loss: 0.0578 - acc: 1.0000\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.0572 - acc: 1.0000\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 951us/step - loss: 0.0566 - acc: 1.0000\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 912us/step - loss: 0.0560 - acc: 1.0000\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.0555 - acc: 1.0000\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0549 - acc: 1.0000\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0543 - acc: 1.0000\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0538 - acc: 1.0000\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0533 - acc: 1.0000\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 905us/step - loss: 0.0527 - acc: 1.0000\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 931us/step - loss: 0.0522 - acc: 1.0000\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.0517 - acc: 1.0000\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 945us/step - loss: 0.0512 - acc: 1.0000\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.0507 - acc: 1.0000\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.0502 - acc: 1.0000\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.0498 - acc: 1.0000\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.0493 - acc: 1.0000\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.0488 - acc: 1.0000\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.0484 - acc: 1.0000\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.0479 - acc: 1.0000\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 956us/step - loss: 0.0475 - acc: 1.0000\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.0471 - acc: 1.0000\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 971us/step - loss: 0.0466 - acc: 1.0000\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 976us/step - loss: 0.0462 - acc: 1.0000\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 950us/step - loss: 0.0458 - acc: 1.0000\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0454 - acc: 1.0000\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0450 - acc: 1.0000\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 921us/step - loss: 0.0446 - acc: 1.0000\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.0442 - acc: 1.0000\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 900us/step - loss: 0.0438 - acc: 1.0000\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 968us/step - loss: 0.0434 - acc: 1.0000\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 978us/step - loss: 0.0431 - acc: 1.0000\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 943us/step - loss: 0.0427 - acc: 1.0000\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 984us/step - loss: 0.0424 - acc: 1.0000\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0420 - acc: 1.0000\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 967us/step - loss: 0.0416 - acc: 1.0000\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 930us/step - loss: 0.0413 - acc: 1.0000\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 956us/step - loss: 0.0410 - acc: 1.0000\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.0406 - acc: 1.0000\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 968us/step - loss: 0.0403 - acc: 1.0000\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 822us/step - loss: 0.0400 - acc: 1.0000\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 869us/step - loss: 0.0396 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8ce4a441-e681-4e80-c687-ab180852425a",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b4a4f604-cf03-4f29-a20e-a8ff2d90b883",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    }
  ]
}